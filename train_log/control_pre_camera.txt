[2025-08-01 14:16:04,543] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:07,063] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:14,955] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:14,960] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:14,967] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:14,979] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:16,177] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:16,179] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:16,181] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:16,187] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:16,190] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:16,191] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:16,209] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:16,219] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:16,219] [INFO] [comm.py:707:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
[2025-08-01 14:16:43,103] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:44,544] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:50,280] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:50,294] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:50,294] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:50,294] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 14:16:51,536] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:51,537] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:51,546] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:51,546] [INFO] [comm.py:707:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-08-01 14:16:51,548] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:51,552] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:51,563] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 14:16:51,591] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 14:16:51,601] [INFO] [comm.py:676:init_distributed] cdb=None
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    The following models are loaded: ['wan_video_text_encoder'].
    The following models are loaded: ['wan_video_text_encoder'].
    The following models are loaded: ['wan_video_text_encoder'].
    The following models are loaded: ['wan_video_text_encoder'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
    model_name: wan_video_vae model_class: WanVideoVAE
    model_name: wan_video_vae model_class: WanVideoVAE
    model_name: wan_video_vae model_class: WanVideoVAE
    model_name: wan_video_vae model_class: WanVideoVAE
    The following models are loaded: ['wan_video_vae'].
    The following models are loaded: ['wan_video_vae'].
    The following models are loaded: ['wan_video_vae'].
    The following models are loaded: ['wan_video_vae'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    The following models are loaded: ['wan_video_image_encoder'].
    The following models are loaded: ['wan_video_image_encoder'].
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
    The following models are loaded: ['wan_video_image_encoder'].
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
    The following models are loaded: ['wan_video_image_encoder'].
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
Using 0 control blocks, for layers []
Module 'pose_encoder' (SimpleAdapter) has 151001088 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Using 0 control blocks, for layers []
Module 'pose_encoder' (SimpleAdapter) has 151001088 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Using 0 control blocks, for layers []
Module 'pose_encoder' (SimpleAdapter) has 151001088 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Using 0 control blocks, for layers []
Module 'pose_encoder' (SimpleAdapter) has 151001088 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 1717788992
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Model parameters: 632076801
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Trainable parameters: 1717788992
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 1717788992
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Model parameters: 632076801
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Trainable parameters: 1717788992
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 184 scenes for testing, total 9233 scenes.
Using 184 scenes for testing, total 9233 scenes.
No training state found at meta_state.pt.
No training state found at meta_state.pt.
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 1717788992
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Model parameters: 632076801
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Trainable parameters: 1717788992
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 184 scenes for testing, total 9233 scenes.
No training state found at meta_state.pt.
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 1717788992
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Model parameters: 632076801
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Trainable parameters: 1717788992
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 184 scenes for testing, total 9233 scenes.
No training state found at meta_state.pt.
Initial accelerator with gradient accumulation steps: 16
Using 4 processes for training.
Preparing model, optimizer, dataloader, and scheduler with accelerator.
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
Time to load cpu_adam op: 2.8435590267181396 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
[2025-08-01 14:17:37,552] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.8538401126861572 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-08-01 14:17:37,561] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.949471950531006 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-08-01 14:17:37,656] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.2, git-hash=unknown, git-branch=unknown
[2025-08-01 14:17:37,656] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.9509847164154053 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-08-01 14:17:37,658] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
[2025-08-01 14:17:40,701] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=4
	 self.mp_world_size=1
	 self.seq_dp_world_size=4
	 self.sequence_parallel_size=1
***********************************************
[2025-08-01 14:17:47,571] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-01 14:17:47,574] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-08-01 14:17:47,574] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-01 14:17:47,695] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-08-01 14:17:47,695] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-08-01 14:17:47,695] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-08-01 14:17:47,695] [INFO] [stage_1_and_2.py:172:__init__] Reduce bucket size 500000000
[2025-08-01 14:17:47,695] [INFO] [stage_1_and_2.py:173:__init__] Allgather bucket size 500000000
[2025-08-01 14:17:47,695] [INFO] [stage_1_and_2.py:174:__init__] CPU Offload: True
[2025-08-01 14:17:47,695] [INFO] [stage_1_and_2.py:175:__init__] Round robin gradient partitioning: False
[2025-08-01 14:17:52,501] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-01 14:17:52,501] [INFO] [utils.py:782:see_memory_usage] MA 15.4 GB         Max_MA 15.4 GB         CA 15.85 GB         Max_CA 16 GB 
[2025-08-01 14:17:52,502] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 126.44 GB, percent = 12.5%
[2025-08-01 14:17:53,263] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-01 14:17:53,263] [INFO] [utils.py:782:see_memory_usage] MA 15.4 GB         Max_MA 15.4 GB         CA 15.85 GB         Max_CA 16 GB 
[2025-08-01 14:17:53,264] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 129.37 GB, percent = 12.8%
[2025-08-01 14:17:53,264] [INFO] [stage_1_and_2.py:599:__init__] optimizer state initialized
[2025-08-01 14:17:53,422] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-01 14:17:53,422] [INFO] [utils.py:782:see_memory_usage] MA 15.4 GB         Max_MA 15.4 GB         CA 15.85 GB         Max_CA 16 GB 
[2025-08-01 14:17:53,423] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 129.97 GB, percent = 12.9%
[2025-08-01 14:17:53,429] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-08-01 14:17:53,429] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-08-01 14:17:53,429] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-08-01 14:17:53,429] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-08-01 14:17:53,433] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-08-01 14:17:53,433] [INFO] [config.py:954:print] DeepSpeedEngine configuration:
[2025-08-01 14:17:53,433] [INFO] [config.py:958:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-08-01 14:17:53,433] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-01 14:17:53,433] [INFO] [config.py:958:print]   amp_enabled .................. False
[2025-08-01 14:17:53,433] [INFO] [config.py:958:print]   amp_params ................... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f34741299d0>
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   communication_data_type ...... None
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   dataloader_drop_last ......... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   disable_allgather ............ False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   dump_state ................... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   elasticity_enabled ........... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   global_rank .................. 0
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   grad_accum_dtype ............. None
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 16
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   gradient_clipping ............ 1.0
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   graph_harvesting ............. False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   load_universal_checkpoint .... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   memory_breakdown ............. False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   mics_shard_size .............. -1
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   optimizer_name ............... None
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   optimizer_params ............. None
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   pld_enabled .................. False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   pld_params ................... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   prescale_gradients ........... False
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   scheduler_name ............... None
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   scheduler_params ............. None
[2025-08-01 14:17:53,434] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   sparse_attention ............. None
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   steps_per_print .............. inf
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   train_batch_size ............. 64
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   use_node_local_storage ....... False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   weight_quantization_config ... None
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   world_size ................... 4
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  True
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   zero_enabled ................. True
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-01 14:17:53,435] [INFO] [config.py:958:print]   zero_optimization_stage ...... 2
[2025-08-01 14:17:53,435] [INFO] [config.py:944:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 16, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Model param dtype : torch.bfloat16
Starting training from scratch.
Validate every 100 steps.
Starting validation with model at epoch 0, global step 0
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 0: 0.0992678627371788
Loss at step 1: 0.02380348928272724
Loss at step 2: 0.07073969393968582
Loss at step 3: 0.39862796664237976
Loss at step 4: 0.047618214040994644
Loss at step 5: 0.0749126598238945
Loss at step 6: 0.314906507730484
Loss at step 7: 0.06283626705408096
Loss at step 8: 0.09330649673938751
Loss at step 9: 0.1767573207616806
Loss at step 10: 0.4309263825416565
Loss at step 11: 0.010002780705690384
Loss at step 12: 0.33615681529045105
Loss at step 13: 0.057491984218358994
Loss at step 14: 0.09446802735328674
Loss at step 15: 0.024398140609264374
Loss at step 16: 0.2176094502210617
Loss at step 17: 0.13095702230930328
Loss at step 18: 0.02169698476791382
Loss at step 19: 0.1639293134212494
Loss at step 20: 0.06980065256357193
Loss at step 21: 0.07841655611991882
Loss at step 22: 0.06795905530452728
Loss at step 23: 0.004246753174811602
Loss at step 24: 0.3987734913825989
Loss at step 25: 0.21525762975215912
Loss at step 26: 0.2620166540145874
Loss at step 27: 0.35866641998291016
Loss at step 28: 0.1311395913362503
Loss at step 29: 0.15491127967834473
Loss at step 30: 0.18492093682289124
Loss at step 31: 0.4694187045097351
Loss at step 32: 0.21758799254894257
Loss at step 33: 0.03370188549160957
Loss at step 34: 0.31818822026252747
Loss at step 35: 0.0919555276632309
Loss at step 36: 0.1980844885110855
Loss at step 37: 0.08353523164987564
Loss at step 38: 0.21651120483875275
Loss at step 39: 0.0823574811220169
Loss at step 40: 0.033404119312763214
Loss at step 41: 0.14527904987335205
Loss at step 42: 0.130264550447464
Loss at step 43: 0.06492898613214493
Loss at step 44: 0.05084048584103584
Loss at step 45: 0.08114220947027206
Loss at step 46: 0.5584423542022705
Loss at step 47: 0.09939055889844894
Loss at step 48: 0.005841528531163931
Loss at step 49: 0.5758548974990845
Loss at step 50: 0.3305080235004425
Loss at step 51: 0.10596532374620438
Loss at step 52: 0.021931448951363564
Loss at step 53: 0.15216214954853058
Loss at step 54: 0.42298924922943115
Loss at step 55: 0.08840860426425934
Loss at step 56: 0.07335834205150604
Loss at step 57: 0.15557534992694855
Loss at step 58: 0.05444375425577164
Loss at step 59: 0.010615057311952114
Loss at step 60: 0.08384758979082108
Loss at step 61: 0.03088044375181198
Loss at step 62: 0.29833993315696716
Loss at step 63: 0.054277028888463974
Loss at step 64: 0.12193363904953003
Loss at step 65: 0.420263409614563
Loss at step 66: 0.27839064598083496
Loss at step 67: 0.03148895874619484
Loss at step 68: 0.027968429028987885
Loss at step 69: 0.10657726228237152
Loss at step 70: 0.11491401493549347
Loss at step 71: 0.2159576565027237
Loss at step 72: 0.03383752331137657
Loss at step 73: 0.14898639917373657
Loss at step 74: 0.10909775644540787
Loss at step 75: 0.23515450954437256
Loss at step 76: 0.07194968312978745
Loss at step 77: 0.05129654332995415
Loss at step 78: 0.058509744703769684
Loss at step 79: 0.4660780727863312
Loss at step 80: 0.2114478498697281
Loss at step 81: 0.19948406517505646
Loss at step 82: 0.3929273188114166
Loss at step 83: 0.4993458688259125
Loss at step 84: 0.03541543334722519
Loss at step 85: 0.11002349853515625
Loss at step 86: 0.07197907567024231
Loss at step 87: 0.046179283410310745
Loss at step 88: 0.12289434671401978
Loss at step 89: 0.27393603324890137
Loss at step 90: 0.17608509957790375
Loss at step 91: 0.44484156370162964
Loss at step 92: 0.14353656768798828
Loss at step 93: 0.2932516932487488
Loss at step 94: 0.16051162779331207
Loss at step 95: 0.11475910991430283
Loss at step 96: 0.2347712367773056
Loss at step 97: 0.6221429109573364
Loss at step 98: 0.0
Loss at step 99: 0.06198063865303993
Saving training state...
[2025-08-01 18:29:02,682] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 18:29:08,001] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 100
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 100: 0.022399745881557465
Loss at step 101: 0.15433308482170105
Loss at step 102: 0.05201500281691551
Loss at step 103: 0.023834172636270523
Loss at step 104: 0.04508195444941521
Loss at step 105: 0.056924041360616684
Loss at step 106: 0.06395163387060165
Loss at step 107: 0.42178285121917725
Loss at step 108: 0.09239126741886139
Loss at step 109: 0.10201968997716904
Loss at step 110: 0.16444045305252075
Loss at step 111: 0.36599138379096985
Loss at step 112: 0.34099897742271423
Loss at step 113: 0.06734556704759598
Loss at step 114: 0.1713559925556183
Loss at step 115: 0.22427362203598022
Loss at step 116: 0.04247844219207764
Loss at step 117: 0.15597721934318542
Loss at step 118: 0.03151264041662216
Loss at step 119: 0.24461041390895844
Loss at step 120: 0.0879444107413292
Loss at step 121: 0.1113632470369339
Loss at step 122: 0.0
Loss at step 123: 0.11781196296215057
Loss at step 124: 0.4346698224544525
Loss at step 125: 0.1506315916776657
Loss at step 126: 0.37953680753707886
Loss at step 127: 0.235203355550766
Loss at step 128: 0.02547054924070835
Loss at step 129: 0.21578390896320343
Loss at step 130: 0.47535815834999084
Loss at step 131: 0.06827334314584732
Loss at step 132: 0.023147301748394966
Loss at step 133: 0.0926593765616417
Loss at step 134: 0.3905300498008728
Loss at step 135: 0.036178529262542725
Loss at step 136: 0.10679309070110321
Loss at step 137: 0.030520517379045486
Loss at step 138: 0.011747159995138645
Loss at step 139: 0.5314172506332397
Loss at step 140: 0.35269254446029663
Loss at step 141: 0.038983844220638275
Loss at step 142: 0.07171805948019028
Loss at step 143: 0.32079049944877625
Loss at step 144: 0.1807345747947693
Loss at step 145: 0.1408490240573883
Loss at step 146: 0.14174355566501617
Loss at step 147: 0.08457186073064804
Loss at step 148: 0.2417939305305481
Loss at step 149: 0.20007061958312988
Loss at step 150: 0.3467795252799988
Loss at step 151: 0.0778752937912941
Loss at step 152: 0.471329003572464
Loss at step 153: 0.4645099341869354
Loss at step 154: 0.25756382942199707
Loss at step 155: 0.351545125246048
Loss at step 156: 0.3954904079437256
Loss at step 157: 0.07192671298980713
Loss at step 158: 0.06618602573871613
Loss at step 159: 0.43593600392341614
Loss at step 160: 0.03872847184538841
Loss at step 161: 0.41886836290359497
Loss at step 162: 0.46015703678131104
Loss at step 163: 0.13737007975578308
Loss at step 164: 0.26280587911605835
Loss at step 165: 0.06361354887485504
Loss at step 166: 0.14042480289936066
Loss at step 167: 0.34799623489379883
Loss at step 168: 0.01867370679974556
Loss at step 169: 0.005604682490229607
Loss at step 170: 0.20460525155067444
Loss at step 171: 0.4622288644313812
Loss at step 172: 0.36140236258506775
Loss at step 173: 0.11382950842380524
Loss at step 174: 0.029794400557875633
Loss at step 175: 0.06212374567985535
Loss at step 176: 0.14269697666168213
Loss at step 177: 0.1428784877061844
Loss at step 178: 0.08803074806928635
Loss at step 179: 0.1504434198141098
Loss at step 180: 0.02688918448984623
Loss at step 181: 0.09264209866523743
Loss at step 182: 0.10803306847810745
Loss at step 183: 0.008610276505351067
Loss at step 184: 0.1080467626452446
Loss at step 185: 0.1146770492196083
Loss at step 186: 0.12267694622278214
Loss at step 187: 0.021386394277215004
Loss at step 188: 0.0956953763961792
Loss at step 189: 0.03498466685414314
Loss at step 190: 0.4210636019706726
Loss at step 191: 0.01871650479733944
Loss at step 192: 0.1679840087890625
Loss at step 193: 0.004215958062559366
Loss at step 194: 0.06290952116250992
Loss at step 195: 0.17347127199172974
Loss at step 196: 0.4640881419181824
Loss at step 197: 0.06895802915096283
Loss at step 198: 0.15408827364444733
Loss at step 199: 0.15279455482959747
Saving training state...
[2025-08-01 22:40:24,550] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 22:40:29,499] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 200: 0.08491126447916031
Loss at step 201: 0.08356112241744995
Loss at step 202: 0.17308412492275238
Loss at step 203: 0.08587425947189331
Loss at step 204: 0.134485125541687
Loss at step 205: 0.18508334457874298
Loss at step 206: 0.4074040949344635
Loss at step 207: 0.17780059576034546
Loss at step 208: 0.5306345224380493
Loss at step 209: 0.3073030114173889
Loss at step 210: 0.07794500142335892
Loss at step 211: 0.12028107047080994
Loss at step 212: 0.09615296870470047
Loss at step 213: 0.1602083146572113
Loss at step 214: 0.30822649598121643
Loss at step 215: 0.04540098085999489
Loss at step 216: 0.40349340438842773
Loss at step 217: 0.025258421897888184
Loss at step 218: 0.05576065182685852
Loss at step 219: 0.2724454402923584
Loss at step 220: 0.19100095331668854
Loss at step 221: 0.3318503201007843
Loss at step 222: 0.4020090699195862
Loss at step 223: 0.04925401508808136
Loss at step 224: 0.13403570652008057
Loss at step 225: 0.39473482966423035
Loss at step 226: 0.14661574363708496
Loss at step 227: 0.04686924070119858
Loss at step 228: 0.19015267491340637
Loss at step 229: 0.22420340776443481
Loss at step 230: 0.08366066962480545
Loss at step 231: 0.07717742770910263
Loss at step 232: 0.10443495959043503
Loss at step 233: 0.01796618103981018
Loss at step 234: 0.2844652831554413
Loss at step 235: 0.34336820244789124
Loss at step 236: 0.05645189434289932
Loss at step 237: 0.2152673900127411
Loss at step 238: 0.174482524394989
Loss at step 239: 0.4075130522251129
Loss at step 240: 0.05404163524508476
Loss at step 241: 0.1412026584148407
Loss at step 242: 0.15602630376815796
Loss at step 243: 0.012055867351591587
Loss at step 244: 0.04150334745645523
Loss at step 245: 0.16085316240787506
Loss at step 246: 0.5837692022323608
Loss at step 247: 0.05468744412064552
Loss at step 248: 0.17709100246429443
Loss at step 249: 0.17256709933280945
Loss at step 250: 0.21097324788570404
Loss at step 251: 0.20372244715690613
Loss at step 252: 0.16600483655929565
Loss at step 253: 0.08933480829000473
Loss at step 254: 0.15983979403972626
Loss at step 255: 0.11163899302482605
Loss at step 256: 0.21253569424152374
Loss at step 257: 0.20841138064861298
Loss at step 258: 0.23323924839496613
Loss at step 259: 0.4454405903816223
Loss at step 260: 0.18065966665744781
Loss at step 261: 0.024407677352428436
Loss at step 262: 0.07528702169656754
Loss at step 263: 0.0381661131978035
Loss at step 264: 0.18850506842136383
Loss at step 265: 0.6283450126647949
Loss at step 266: 0.07440383732318878
Loss at step 267: 0.05890873074531555
Loss at step 268: 0.05024499073624611
Loss at step 269: 0.12056965380907059
Loss at step 270: 0.001498152269050479
Loss at step 271: 0.4054742753505707
Loss at step 272: 0.08893715590238571
Loss at step 273: 0.10149310529232025
Loss at step 274: 0.05174001306295395
Loss at step 275: 0.25278037786483765
Loss at step 276: 0.05514141917228699
Loss at step 277: 0.1160622090101242
Loss at step 278: 0.37645235657691956
Loss at step 279: 0.0
Loss at step 280: 0.047818638384342194
Loss at step 281: 0.03969966992735863
Loss at step 282: 0.3418237268924713
Loss at step 283: 0.06417252123355865
Loss at step 284: 0.037962187081575394
Loss at step 285: 0.12292104214429855
Loss at step 286: 0.4753390848636627
Loss at step 287: 0.08679212629795074
Loss at step 288: 0.49181807041168213
Loss at step 289: 0.4863665699958801
Loss at step 290: 0.6085221767425537
Loss at step 291: 0.009373721666634083
Loss at step 292: 0.027981571853160858
Loss at step 293: 0.32426875829696655
Loss at step 294: 0.40016108751296997
Loss at step 295: 0.09932666271924973
Loss at step 296: 0.09603910893201828
Loss at step 297: 0.12390854954719543
Loss at step 298: 0.22714075446128845
Loss at step 299: 0.1614004522562027
Saving training state...
[2025-08-02 02:48:16,376] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 02:48:21,350] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 300
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 300: 0.11712270975112915
Loss at step 301: 0.22537219524383545
Loss at step 302: 0.47094687819480896
Loss at step 303: 0.4974510669708252
Loss at step 304: 0.05738218128681183
Loss at step 305: 0.010273358784615993
Loss at step 306: 0.06277010589838028
Loss at step 307: 0.18419791758060455
Loss at step 308: 0.3421326279640198
Loss at step 309: 0.21145126223564148
Loss at step 310: 0.1582556664943695
Loss at step 311: 0.008051996119320393
Loss at step 312: 0.4032618999481201
Loss at step 313: 0.19253478944301605
Loss at step 314: 0.11482392251491547
Loss at step 315: 0.20634020864963531
Loss at step 316: 0.012148587964475155
Loss at step 317: 0.10190309584140778
Loss at step 318: 0.24255821108818054
Loss at step 319: 0.24510958790779114
Loss at step 320: 0.22713132202625275
Loss at step 321: 0.15284255146980286
Loss at step 322: 0.19337837398052216
Loss at step 323: 0.11393807828426361
Loss at step 324: 0.409344881772995
Loss at step 325: 0.08471480011940002
Loss at step 326: 0.04406679421663284
Loss at step 327: 0.07746490836143494
Loss at step 328: 0.031351685523986816
Loss at step 329: 0.2522425949573517
Loss at step 330: 0.05213398113846779
Loss at step 331: 0.23748449981212616
Loss at step 332: 0.14347858726978302
Loss at step 333: 0.3705914318561554
Loss at step 334: 0.34089741110801697
Loss at step 335: 0.12345170229673386
Loss at step 336: 0.19990307092666626
Loss at step 337: 0.01665697619318962
Loss at step 338: 0.3881591558456421
Loss at step 339: 0.0751422867178917
Loss at step 340: 0.3017398416996002
Loss at step 341: 0.4596656858921051
Loss at step 342: 0.1355496346950531
Loss at step 343: 0.12388124316930771
Loss at step 344: 0.34659892320632935
Loss at step 345: 0.1771560162305832
Loss at step 346: 0.3260786533355713
Loss at step 347: 0.0
Loss at step 348: 0.08781828731298447
Loss at step 349: 0.09590676426887512
Loss at step 350: 0.45868414640426636
Loss at step 351: 0.1629071682691574
Loss at step 352: 0.14263056218624115
Loss at step 353: 0.3018898367881775
Loss at step 354: 0.0932803750038147
Loss at step 355: 0.33752864599227905
Loss at step 356: 0.05082837864756584
Loss at step 357: 0.016942491754889488
Loss at step 358: 0.3669886589050293
Loss at step 359: 0.052071429789066315
Loss at step 360: 0.14444194734096527
Loss at step 361: 0.016816023737192154
Loss at step 362: 0.06473829597234726
Loss at step 363: 0.15699143707752228
Loss at step 364: 0.18081428110599518
Loss at step 365: 0.08502203971147537
Loss at step 366: 0.357707679271698
Loss at step 367: 0.3159489333629608
Loss at step 368: 0.0
Loss at step 369: 0.03619145229458809
Loss at step 370: 0.16468580067157745
Loss at step 371: 0.07003174722194672
Loss at step 372: 0.27195271849632263
Loss at step 373: 0.02091909572482109
Loss at step 374: 0.1556898057460785
Loss at step 375: 0.320270836353302
Loss at step 376: 0.0637226551771164
Loss at step 377: 0.1325555145740509
Loss at step 378: 0.008393870666623116
Loss at step 379: 0.14580467343330383
Loss at step 380: 0.1767524629831314
Loss at step 381: 0.10885612666606903
Loss at step 382: 0.016404813155531883
Loss at step 383: 0.24430495500564575
Loss at step 384: 0.354883074760437
Loss at step 385: 0.4699458181858063
Loss at step 386: 0.14558421075344086
Loss at step 387: 0.07831286638975143
Loss at step 388: 0.1745176613330841
Loss at step 389: 0.03115226700901985
Loss at step 390: 0.01654651202261448
Loss at step 391: 0.394835501909256
Loss at step 392: 0.20103046298027039
Loss at step 393: 0.14402396976947784
Loss at step 394: 0.17365993559360504
Loss at step 395: 0.20034442842006683
Loss at step 396: 0.06233040243387222
Loss at step 397: 0.1683632731437683
Loss at step 398: 0.02104230783879757
Loss at step 399: 0.30001935362815857
Saving training state...
[2025-08-02 06:55:33,522] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 06:55:38,609] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 400: 0.05339517444372177
Loss at step 401: 0.07616300880908966
Loss at step 402: 0.3562942445278168
Loss at step 403: 0.05936549976468086
Loss at step 404: 0.24255849421024323
Loss at step 405: 0.11223064363002777
Loss at step 406: 0.441608190536499
Loss at step 407: 0.17219647765159607
Loss at step 408: 0.42682313919067383
Loss at step 409: 0.09134302288293839
Loss at step 410: 0.13528145849704742
Loss at step 411: 0.10708623379468918
Loss at step 412: 0.09813232719898224
Loss at step 413: 0.020499922335147858
Loss at step 414: 0.16057805716991425
Loss at step 415: 0.017548860982060432
Loss at step 416: 0.005321851931512356
Loss at step 417: 0.04389937222003937
Loss at step 418: 0.08293543756008148
Loss at step 419: 0.10392473638057709
Loss at step 420: 0.07457119971513748
Loss at step 421: 0.10073316097259521
Loss at step 422: 0.04853597283363342
Loss at step 423: 0.27825695276260376
Loss at step 424: 0.1689688116312027
Loss at step 425: 0.019001541659235954
Loss at step 426: 0.4076555371284485
Loss at step 427: 0.21410441398620605
Loss at step 428: 0.19071567058563232
Loss at step 429: 0.45045483112335205
Loss at step 430: 0.024398786947131157
Loss at step 431: 0.18794317543506622
Loss at step 432: 0.2628914713859558
Loss at step 433: 0.3771163523197174
Loss at step 434: 0.0846458226442337
Loss at step 435: 0.32305973768234253
Loss at step 436: 0.05765537917613983
Loss at step 437: 0.3774772882461548
Loss at step 438: 0.07181880623102188
Loss at step 439: 0.3542124927043915
Loss at step 440: 0.04773589223623276
Loss at step 441: 0.20356550812721252
Loss at step 442: 0.124757781624794
Loss at step 443: 0.39569664001464844
Loss at step 444: 0.11054598540067673
Loss at step 445: 0.25036564469337463
Loss at step 446: 0.34199008345603943
Loss at step 447: 0.27722784876823425
Loss at step 448: 0.09702932089567184
Loss at step 449: 0.04041450843214989
Loss at step 450: 0.13291722536087036
Loss at step 451: 0.02268342673778534
Loss at step 452: 0.16803589463233948
Loss at step 453: 0.3631623387336731
Loss at step 454: 0.1942109316587448
Loss at step 455: 0.2223411202430725
Loss at step 456: 0.12200722843408585
Loss at step 457: 0.38160160183906555
Loss at step 458: 0.023763014003634453
Loss at step 459: 0.31820887327194214
Loss at step 460: 0.022153854370117188
Loss at step 461: 0.250506728887558
Loss at step 462: 0.005738831125199795
Loss at step 463: 0.3189539611339569
Loss at step 464: 0.01099737361073494
Loss at step 465: 0.20221811532974243
Loss at step 466: 0.5523146986961365
Loss at step 467: 0.502782940864563
Loss at step 468: 0.15766280889511108
Loss at step 469: 0.40214216709136963
Loss at step 470: 0.3435695469379425
Loss at step 471: 0.0655624121427536
Loss at step 472: 0.24516434967517853
Loss at step 473: 0.6152143478393555
Loss at step 474: 0.3338092565536499
Loss at step 475: 0.14435164630413055
Loss at step 476: 0.6223915219306946
Loss at step 477: 0.39345574378967285
Loss at step 478: 0.0
Loss at step 479: 0.03981516510248184
Loss at step 480: 0.19238391518592834
Loss at step 481: 0.02310149557888508
Loss at step 482: 0.1192440390586853
Loss at step 483: 0.3293883800506592
Loss at step 484: 0.11966326832771301
Loss at step 485: 0.03290411829948425
Loss at step 486: 0.25644445419311523
Loss at step 487: 0.140614852309227
Loss at step 488: 0.330174058675766
Loss at step 489: 0.3079482316970825
Loss at step 490: 0.10554338246583939
Loss at step 491: 0.11377765238285065
Loss at step 492: 0.040345460176467896
Loss at step 493: 0.11376482993364334
Loss at step 494: 0.03836959972977638
Loss at step 495: 0.015618165023624897
Loss at step 496: 0.26120272278785706
Loss at step 497: 0.1390070617198944
Loss at step 498: 0.03462006151676178
Loss at step 499: 0.010341132059693336
Saving training state...
[2025-08-02 10:57:21,088] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 10:57:26,265] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 500
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 500: 0.12049978971481323
Loss at step 501: 0.30807217955589294
Loss at step 502: 0.0051186238415539265
Loss at step 503: 0.08892825245857239
Loss at step 504: 0.07252773642539978
Loss at step 505: 0.13495656847953796
Loss at step 506: 0.1370394378900528
Loss at step 507: 0.08995506167411804
Loss at step 508: 0.11313005536794662
Loss at step 509: 0.0709666833281517
Loss at step 510: 0.22650061547756195
Loss at step 511: 0.04526722803711891
Loss at step 512: 0.021414153277873993
Loss at step 513: 0.05910529941320419
Loss at step 514: 0.05261251702904701
Loss at step 515: 0.4692455232143402
Loss at step 516: 0.046716175973415375
Loss at step 517: 0.2703360915184021
Loss at step 518: 0.5112427473068237
Loss at step 519: 0.36762145161628723
Loss at step 520: 0.02496269717812538
Loss at step 521: 0.2745257019996643
Loss at step 522: 0.20375993847846985
Loss at step 523: 0.09466516226530075
Loss at step 524: 0.014379733242094517
Loss at step 525: 0.11396277695894241
Loss at step 526: 0.30283573269844055
Loss at step 527: 0.13248048722743988
Loss at step 528: 0.44805029034614563
Loss at step 529: 0.18398956954479218
Loss at step 530: 0.041378624737262726
Loss at step 531: 0.21950668096542358
Loss at step 532: 0.09841930866241455
Loss at step 533: 0.17911940813064575
Loss at step 534: 0.17920252680778503
Loss at step 535: 0.046180687844753265
Loss at step 536: 0.1391565203666687
Loss at step 537: 0.11511256545782089
Loss at step 538: 0.33765584230422974
Loss at step 539: 0.42007794976234436
Loss at step 540: 0.01571795344352722
Loss at step 541: 0.04414907097816467
Loss at step 542: 0.35875722765922546
Loss at step 543: 0.045455776154994965
Loss at step 544: 0.08029741793870926
Loss at step 545: 0.03200537711381912
Loss at step 546: 0.5143415927886963
Loss at step 547: 0.09116431325674057
Loss at step 548: 0.17734751105308533
Loss at step 549: 0.24933113157749176
Loss at step 550: 0.018117761239409447
Loss at step 551: 0.06683358550071716
Loss at step 552: 0.3866088092327118
Loss at step 553: 0.08966568112373352
Loss at step 554: 0.07165291905403137
Loss at step 555: 0.04887225106358528
Loss at step 556: 0.17522409558296204
Loss at step 557: 0.1930343508720398
Loss at step 558: 0.07402174919843674
Loss at step 559: 0.01971493847668171
Loss at step 560: 0.028212931007146835
Loss at step 561: 0.15699553489685059
Loss at step 562: 0.011387372389435768
Loss at step 563: 0.30352914333343506
Loss at step 564: 0.0359133742749691
Loss at step 565: 0.02978311851620674
Loss at step 566: 0.2998714745044708
Loss at step 567: 0.11137145757675171
Loss at step 568: 0.007564172148704529
Loss at step 569: 0.09463377296924591
Loss at step 570: 0.08590997755527496
Loss at step 571: 0.32017695903778076
Loss at step 572: 0.05602074787020683
Loss at step 573: 0.28211647272109985
Loss at step 574: 0.10822199285030365
Loss at step 575: 0.6072548627853394
Loss at step 576: 0.20291323959827423
Loss at step 577: 0.27601274847984314
Loss at step 578: 0.0785251259803772
Loss at step 579: 0.022794457152485847
Loss at step 580: 0.16722612082958221
Loss at step 581: 0.10563144087791443
Loss at step 582: 0.0053769866935908794
Loss at step 583: 0.0793866291642189
Loss at step 584: 0.11850430071353912
Loss at step 585: 0.08640075474977493
Loss at step 586: 0.16651764512062073
Loss at step 587: 0.07460793852806091
Loss at step 588: 0.04128022864460945
Loss at step 589: 0.1660839319229126
Loss at step 590: 0.45510777831077576
Loss at step 591: 0.10025330632925034
Loss at step 592: 0.09673236310482025
Loss at step 593: 0.09044278413057327
Loss at step 594: 0.16950257122516632
Loss at step 595: 0.21264174580574036
Loss at step 596: 0.06716528534889221
Loss at step 597: 0.21421806514263153
Loss at step 598: 0.2938207983970642
Loss at step 599: 0.04556666687130928
Saving training state...
[2025-08-02 15:01:06,585] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 15:01:11,487] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 600: 0.2521305978298187
Loss at step 601: 0.15124408900737762
Loss at step 602: 0.05551876127719879
Loss at step 603: 0.3029481768608093
Loss at step 604: 0.1203564703464508
Loss at step 605: 0.3390738368034363
Loss at step 606: 0.279188871383667
Loss at step 607: 0.051591526716947556
Loss at step 608: 0.16273154318332672
Loss at step 609: 0.17028771340847015
Loss at step 610: 0.07481839507818222
Loss at step 611: 0.012845093384385109
Loss at step 612: 0.12772798538208008
Loss at step 613: 0.04572729393839836
Loss at step 614: 0.04071235656738281
Loss at step 615: 0.10781150311231613
Loss at step 616: 0.3051965832710266
Loss at step 617: 0.01654299907386303
Loss at step 618: 0.12359703332185745
Loss at step 619: 0.07393616437911987
Loss at step 620: 0.3235965371131897
Loss at step 621: 0.016674520447850227
Loss at step 622: 0.21795439720153809
Loss at step 623: 0.05472175031900406
Loss at step 624: 0.31679069995880127
Loss at step 625: 0.05495506525039673
Loss at step 626: 0.5559133887290955
Loss at step 627: 0.24931509792804718
Loss at step 628: 0.3348903954029083
Loss at step 629: 0.09899222105741501
Loss at step 630: 0.08947792649269104
Loss at step 631: 0.09330948442220688
Loss at step 632: 0.22525624930858612
Loss at step 633: 0.26733294129371643
Loss at step 634: 0.04787071421742439
Loss at step 635: 0.15788482129573822
Loss at step 636: 0.05787544697523117
Loss at step 637: 0.10842721909284592
Loss at step 638: 0.021949920803308487
Loss at step 639: 0.08431950956583023
Loss at step 640: 0.13631713390350342
Loss at step 641: 0.10161514580249786
Loss at step 642: 0.2110474407672882
Loss at step 643: 0.4359332025051117
Loss at step 644: 0.061280809342861176
Loss at step 645: 0.058850303292274475
Loss at step 646: 0.059702418744564056
Loss at step 647: 0.22224141657352448
Loss at step 648: 0.03220576047897339
Loss at step 649: 0.3539201617240906
Loss at step 650: 0.05982520803809166
Loss at step 651: 0.2723872661590576
Loss at step 652: 0.13402631878852844
Loss at step 653: 0.07537944614887238
Loss at step 654: 0.2693284749984741
Loss at step 655: 0.48496225476264954
Loss at step 656: 0.0
Loss at step 657: 0.22599218785762787
Loss at step 658: 0.1278948336839676
Loss at step 659: 0.2928794026374817
Loss at step 660: 0.04557337239384651
Loss at step 661: 0.2842499017715454
Loss at step 662: 0.13076376914978027
Loss at step 663: 0.052455686032772064
Loss at step 664: 0.040627896785736084
Loss at step 665: 0.018765917047858238
Loss at step 666: 0.21861088275909424
Loss at step 667: 0.0700482502579689
Loss at step 668: 0.14451585710048676
Loss at step 669: 0.030606800690293312
Loss at step 670: 0.06827229261398315
Loss at step 671: 0.0013890278059989214
Loss at step 672: 0.0309047419577837
Loss at step 673: 0.1099369004368782
Loss at step 674: 0.3975411057472229
Loss at step 675: 0.28682494163513184
Loss at step 676: 0.20879028737545013
Loss at step 677: 0.35320931673049927
Loss at step 678: 0.06058591604232788
Loss at step 679: 0.3634183704853058
Loss at step 680: 0.42220717668533325
Loss at step 681: 0.017976058647036552
Loss at step 682: 0.2708379924297333
Loss at step 683: 0.24415913224220276
Loss at step 684: 0.41730326414108276
Loss at step 685: 0.33776968717575073
Loss at step 686: 0.1991809606552124
Loss at step 687: 0.12302108108997345
Loss at step 688: 0.07892317324876785
Loss at step 689: 0.4614465534687042
Loss at step 690: 0.17709669470787048
Loss at step 691: 0.12619909644126892
Loss at step 692: 0.2953157424926758
Loss at step 693: 0.10799312591552734
Loss at step 694: 0.32674452662467957
Loss at step 695: 0.048585642129182816
Loss at step 696: 0.14368629455566406
Loss at step 697: 0.04927249252796173
Loss at step 698: 0.18327537178993225
Loss at step 699: 0.10548848658800125
Saving training state...
[2025-08-02 19:08:43,496] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 19:08:48,254] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 700
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 700: 0.13574330508708954
Loss at step 701: 0.1882123202085495
Loss at step 702: 0.32940974831581116
Loss at step 703: 0.31072941422462463
Loss at step 704: 0.11614679545164108
Loss at step 705: 0.1842307448387146
Loss at step 706: 0.09946069121360779
Loss at step 707: 0.10975688695907593
Loss at step 708: 0.41910943388938904
Loss at step 709: 0.1443691849708557
Loss at step 710: 0.42220190167427063
Loss at step 711: 0.13146182894706726
Loss at step 712: 0.492234468460083
Loss at step 713: 0.10972549021244049
Loss at step 714: 0.05591472238302231
Loss at step 715: 0.05298477038741112
Loss at step 716: 0.2615218162536621
Loss at step 717: 0.0810832604765892
Loss at step 718: 0.045544594526290894
Loss at step 719: 0.2994646430015564
Loss at step 720: 0.21340301632881165
Loss at step 721: 0.26644256711006165
Loss at step 722: 0.13865616917610168
Loss at step 723: 0.19183509051799774
Loss at step 724: 0.06415966898202896
Loss at step 725: 0.36976587772369385
Loss at step 726: 0.18328359723091125
Loss at step 727: 0.12963631749153137
Loss at step 728: 0.07267817854881287
Loss at step 729: 0.0511302649974823
Loss at step 730: 0.46897611021995544
Loss at step 731: 0.022341232746839523
Loss at step 732: 0.029043026268482208
Loss at step 733: 0.11365395784378052
Loss at step 734: 0.1419307142496109
Loss at step 735: 0.07669216394424438
Loss at step 736: 0.276466429233551
Loss at step 737: 0.1966814249753952
Loss at step 738: 0.2787612974643707
Loss at step 739: 0.07087957113981247
Loss at step 740: 0.1410151720046997
Loss at step 741: 0.16478168964385986
Loss at step 742: 0.6549245715141296
Loss at step 743: 0.07997182011604309
Loss at step 744: 0.1918676644563675
Loss at step 745: 0.10422594100236893
Loss at step 746: 0.04886103793978691
Loss at step 747: 0.04924310743808746
Loss at step 748: 0.5608376860618591
Loss at step 749: 0.12700946629047394
Loss at step 750: 0.14214999973773956
Loss at step 751: 0.44262099266052246
Loss at step 752: 0.29057466983795166
Loss at step 753: 0.10840627551078796
Loss at step 754: 0.03085392899811268
Loss at step 755: 0.2359139621257782
Loss at step 756: 0.4614707827568054
Loss at step 757: 0.03262808173894882
Loss at step 758: 0.17860819399356842
Loss at step 759: 0.16118963062763214
Loss at step 760: 0.10685938596725464
Loss at step 761: 0.4726235866546631
Loss at step 762: 0.12392053008079529
Loss at step 763: 0.1529293805360794
Loss at step 764: 0.10219692438840866
Loss at step 765: 0.36580777168273926
Loss at step 766: 0.1594555526971817
Loss at step 767: 0.12457892298698425
Loss at step 768: 0.04950888454914093
Loss at step 769: 0.33098793029785156
Loss at step 770: 0.30223390460014343
Loss at step 771: 0.03658491000533104
Loss at step 772: 0.3427271842956543
Loss at step 773: 0.01834990829229355
Loss at step 774: 0.04197486490011215
Loss at step 775: 0.30617907643318176
Loss at step 776: 0.0632152408361435
Loss at step 777: 0.09445227682590485
Loss at step 778: 0.10529107600450516
Loss at step 779: 0.21340107917785645
Loss at step 780: 0.049558382481336594
Loss at step 781: 0.19522960484027863
Loss at step 782: 0.044000279158353806
Loss at step 783: 0.048808272927999496
Loss at step 784: 0.03116004168987274
Loss at step 785: 0.06851191818714142
Loss at step 786: 0.06428033113479614
Loss at step 787: 0.043602969497442245
Loss at step 788: 0.25791850686073303
Loss at step 789: 0.07327523082494736
Loss at step 790: 0.02511182427406311
Loss at step 791: 0.10055137425661087
Loss at step 792: 0.23074758052825928
Loss at step 793: 0.358465313911438
Loss at step 794: 0.12282349914312363
Loss at step 795: 0.06117941066622734
Loss at step 796: 0.02894281968474388
Loss at step 797: 0.3128887414932251
Loss at step 798: 0.0750616192817688
Loss at step 799: 0.04752739891409874
Saving training state...
[2025-08-02 23:12:46,351] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 23:12:51,267] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 800: 0.06825423985719681
Loss at step 801: 0.37801557779312134
Loss at step 802: 0.2963765859603882
Loss at step 803: 0.059480659663677216
Loss at step 804: 0.08188904821872711
Loss at step 805: 0.0348978117108345
Loss at step 806: 0.17734192311763763
Loss at step 807: 0.14353609085083008
Loss at step 808: 0.20549669861793518
Loss at step 809: 0.2443234622478485
Loss at step 810: 0.04134051501750946
Loss at step 811: 0.23950175940990448
Loss at step 812: 0.02303769253194332
Loss at step 813: 0.144876167178154
Loss at step 814: 0.053221043199300766
Loss at step 815: 0.35996443033218384
Loss at step 816: 0.40241286158561707
Loss at step 817: 0.1021314337849617
Loss at step 818: 0.060427162796258926
Loss at step 819: 0.3089273273944855
Loss at step 820: 0.14838695526123047
Loss at step 821: 0.11031694710254669
Loss at step 822: 0.037035875022411346
Loss at step 823: 0.30789920687675476
Loss at step 824: 0.06472241133451462
Loss at step 825: 0.020742055028676987
Loss at step 826: 0.03763280808925629
Loss at step 827: 0.1986294537782669
Loss at step 828: 0.10411985963582993
Loss at step 829: 0.062273286283016205
Loss at step 830: 0.07158098369836807
Loss at step 831: 0.3704948425292969
Loss at step 832: 0.15810459852218628
Loss at step 833: 0.08424172550439835
Loss at step 834: 0.14835548400878906
Loss at step 835: 0.07274002581834793
Loss at step 836: 0.04964614659547806
Loss at step 837: 0.24727442860603333
Loss at step 838: 0.6766993403434753
Loss at step 839: 0.2696395516395569
Loss at step 840: 0.032810937613248825
Loss at step 841: 0.23503516614437103
Loss at step 842: 0.22058869898319244
Loss at step 843: 0.29349610209465027
Loss at step 844: 0.07796285301446915
Loss at step 845: 0.0269243773072958
Loss at step 846: 0.19988267123699188
Loss at step 847: 0.053446754813194275
Loss at step 848: 0.04566806182265282
Loss at step 849: 0.32563135027885437
Loss at step 850: 0.26675236225128174
Loss at step 851: 0.12406839430332184
Loss at step 852: 0.20907336473464966
Loss at step 853: 0.014660755172371864
Loss at step 854: 0.17401596903800964
Loss at step 855: 0.10833366215229034
Loss at step 856: 0.21433225274085999
Loss at step 857: 0.13994023203849792
Loss at step 858: 0.10001874715089798
Loss at step 859: 0.3240519165992737
Loss at step 860: 0.1345035433769226
Loss at step 861: 0.13204340636730194
Loss at step 862: 0.10989217460155487
Loss at step 863: 0.0
Loss at step 864: 0.4708700180053711
Loss at step 865: 0.09246723353862762
Loss at step 866: 0.07651808857917786
Loss at step 867: 0.03307701274752617
Loss at step 868: 0.11947206407785416
Loss at step 869: 0.19599908590316772
Loss at step 870: 0.27075499296188354
Loss at step 871: 0.03673674911260605
Loss at step 872: 0.32394692301750183
Loss at step 873: 0.11944124102592468
Loss at step 874: 0.06318136304616928
Loss at step 875: 0.20180818438529968
Loss at step 876: 0.49834930896759033
Loss at step 877: 0.09463214129209518
Loss at step 878: 0.11729670315980911
Loss at step 879: 0.04060790315270424
Loss at step 880: 0.07633403688669205
Loss at step 881: 0.04731186851859093
Loss at step 882: 0.3177085816860199
Loss at step 883: 0.14287184178829193
Loss at step 884: 0.13241495192050934
Loss at step 885: 0.17494258284568787
Loss at step 886: 0.13407911360263824
Loss at step 887: 0.47617995738983154
Loss at step 888: 0.5487260818481445
Loss at step 889: 0.010738811455667019
Loss at step 890: 0.35037729144096375
Loss at step 891: 0.2808493971824646
Loss at step 892: 0.1443857103586197
Loss at step 893: 0.15692287683486938
Loss at step 894: 0.12654457986354828
Loss at step 895: 0.015554206445813179
Loss at step 896: 0.15665622055530548
Loss at step 897: 0.3371179401874542
Loss at step 898: 0.03689896687865257
Loss at step 899: 0.0480402335524559
Saving training state...
[2025-08-03 03:17:22,327] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 03:17:27,418] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 900
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 900: 0.0021257835905998945
Loss at step 901: 0.010645137168467045
Loss at step 902: 0.21048703789710999
Loss at step 903: 0.10537460446357727
Loss at step 904: 0.2924380898475647
Loss at step 905: 0.04446253180503845
Loss at step 906: 0.18069683015346527
Loss at step 907: 0.38318952918052673
Loss at step 908: 0.07314803451299667
Loss at step 909: 0.18546810746192932
Loss at step 910: 0.3826313614845276
Loss at step 911: 0.25831326842308044
Loss at step 912: 0.040226612240076065
Loss at step 913: 0.08479529619216919
Loss at step 914: 0.09723371267318726
Loss at step 915: 0.048168886452913284
Loss at step 916: 0.0982384979724884
Loss at step 917: 0.08428415656089783
Loss at step 918: 0.049726348370313644
Loss at step 919: 0.06681565195322037
Loss at step 920: 0.022914757952094078
Loss at step 921: 0.09007826447486877
Loss at step 922: 0.4981726408004761
Loss at step 923: 0.4785682260990143
Loss at step 924: 0.0906653180718422
Loss at step 925: 0.024969492107629776
Loss at step 926: 0.13527701795101166
Loss at step 927: 0.046452272683382034
Loss at step 928: 0.2496684342622757
Loss at step 929: 0.05887872353196144
Loss at step 930: 0.2645120322704315
Loss at step 931: 0.40259358286857605
Loss at step 932: 0.3737342357635498
Loss at step 933: 0.2106553167104721
Loss at step 934: 0.05375257134437561
Loss at step 935: 0.45562365651130676
Loss at step 936: 0.12285756319761276
Loss at step 937: 0.03219009190797806
Loss at step 938: 0.032143380492925644
Loss at step 939: 0.22510896623134613
Loss at step 940: 0.15609194338321686
Loss at step 941: 0.4227907061576843
Loss at step 942: 0.1283794492483139
Loss at step 943: 0.04111529141664505
Loss at step 944: 0.02724597230553627
Loss at step 945: 0.09167429804801941
Loss at step 946: 0.29997164011001587
Loss at step 947: 0.16325663030147552
Loss at step 948: 0.13030527532100677
Loss at step 949: 0.052109163254499435
Loss at step 950: 0.016442399471998215
Loss at step 951: 0.16163893043994904
Loss at step 952: 0.2768261134624481
Loss at step 953: 0.23169076442718506
Loss at step 954: 0.4158914387226105
Loss at step 955: 0.030385276302695274
Loss at step 956: 0.14986908435821533
Loss at step 957: 0.0408359095454216
Loss at step 958: 0.03679730370640755
Loss at step 959: 0.08650586009025574
Loss at step 960: 0.17113350331783295
Loss at step 961: 0.08085645735263824
Loss at step 962: 0.42150115966796875
Loss at step 963: 0.03741622716188431
Loss at step 964: 0.13855122029781342
Loss at step 965: 0.2439955174922943
Loss at step 966: 0.4174441993236542
Loss at step 967: 0.5039389729499817
Loss at step 968: 0.13662753999233246
Loss at step 969: 0.13293983042240143
Loss at step 970: 0.1838199347257614
Loss at step 971: 0.10022609680891037
Loss at step 972: 0.2091546207666397
Loss at step 973: 0.06582799553871155
Loss at step 974: 0.15651501715183258
Loss at step 975: 0.33481934666633606
Loss at step 976: 0.06059364229440689
Loss at step 977: 0.03789928928017616
Loss at step 978: 0.03953005000948906
Loss at step 979: 0.09089202433824539
Loss at step 980: 0.03242223337292671
Loss at step 981: 0.11592137813568115
Loss at step 982: 0.028398286551237106
Loss at step 983: 0.37856853008270264
Loss at step 984: 0.19498352706432343
Loss at step 985: 0.14317511022090912
Loss at step 986: 0.06859073787927628
Loss at step 987: 0.03869716823101044
Loss at step 988: 0.042241938412189484
Loss at step 989: 0.0
Loss at step 990: 0.017628885805606842
Loss at step 991: 0.20835000276565552
Loss at step 992: 0.09100276976823807
Loss at step 993: 0.43580755591392517
Loss at step 994: 0.17462798953056335
Loss at step 995: 0.21921443939208984
Loss at step 996: 0.18986472487449646
Loss at step 997: 0.08880265802145004
Loss at step 998: 0.1428229808807373
Loss at step 999: 0.2479780912399292
Saving training state...
[2025-08-03 07:18:57,195] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 07:19:02,212] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1000: 0.07197345048189163
Loss at step 1001: 0.04405057057738304
Loss at step 1002: 0.0706578940153122
Loss at step 1003: 0.1571238785982132
Loss at step 1004: 0.36725735664367676
Loss at step 1005: 0.06715817749500275
Loss at step 1006: 0.3360961675643921
Loss at step 1007: 0.03813796862959862
Loss at step 1008: 0.40838268399238586
Loss at step 1009: 0.056532811373472214
Loss at step 1010: 0.1504458785057068
Loss at step 1011: 0.2930773198604584
Loss at step 1012: 0.47081366181373596
Loss at step 1013: 0.1290515959262848
Loss at step 1014: 0.21282468736171722
Loss at step 1015: 0.17359763383865356
Loss at step 1016: 0.01950167492032051
Loss at step 1017: 0.049866706132888794
Loss at step 1018: 0.014915489591658115
Loss at step 1019: 0.0399203822016716
Loss at step 1020: 0.07673216611146927
Loss at step 1021: 0.09641101956367493
Loss at step 1022: 0.09346765279769897
Loss at step 1023: 0.2853899896144867
Loss at step 1024: 0.39637741446495056
Loss at step 1025: 0.2011280059814453
Loss at step 1026: 0.04823838546872139
Loss at step 1027: 0.09810777008533478
Loss at step 1028: 0.1957753449678421
Loss at step 1029: 0.23171691596508026
Loss at step 1030: 0.03202279657125473
Loss at step 1031: 0.028088079765439034
Loss at step 1032: 0.015574543736875057
Loss at step 1033: 0.37832796573638916
Loss at step 1034: 0.10517731308937073
Loss at step 1035: 0.2163681834936142
Loss at step 1036: 0.08468057960271835
Loss at step 1037: 0.05924796313047409
Loss at step 1038: 0.12466248869895935
Loss at step 1039: 0.06118239089846611
Loss at step 1040: 0.029133334755897522
Loss at step 1041: 0.49222221970558167
Loss at step 1042: 0.3075050413608551
Loss at step 1043: 0.26526713371276855
Loss at step 1044: 0.014081350527703762
Loss at step 1045: 0.025980697944760323
Loss at step 1046: 0.054351285099983215
Loss at step 1047: 0.0044274646788835526
Loss at step 1048: 0.16773416101932526
Loss at step 1049: 0.2034727782011032
Loss at step 1050: 0.0062727779150009155
Loss at step 1051: 0.2525523602962494
Loss at step 1052: 0.0173233263194561
Loss at step 1053: 0.07715167850255966
Loss at step 1054: 0.1003548875451088
Loss at step 1055: 0.1839527040719986
Loss at step 1056: 0.0450749434530735
Loss at step 1057: 0.06256972998380661
Loss at step 1058: 0.04191901907324791
Loss at step 1059: 0.020398210734128952
Loss at step 1060: 0.005697308573871851
Loss at step 1061: 0.38927093148231506
Loss at step 1062: 0.03999512642621994
Loss at step 1063: 0.33589982986450195
Loss at step 1064: 0.052790988236665726
Loss at step 1065: 0.04536179453134537
Loss at step 1066: 0.04496429115533829
Loss at step 1067: 0.020977307111024857
Loss at step 1068: 0.09115973114967346
Loss at step 1069: 0.4693962335586548
Loss at step 1070: 0.19555337727069855
Loss at step 1071: 0.060205940157175064
Loss at step 1072: 0.1076129823923111
Loss at step 1073: 0.19027602672576904
Loss at step 1074: 0.1107746884226799
Loss at step 1075: 0.2697116434574127
Loss at step 1076: 0.46439307928085327
Loss at step 1077: 0.21688221395015717
Loss at step 1078: 0.20513829588890076
Loss at step 1079: 0.064700186252594
Loss at step 1080: 0.173353374004364
Loss at step 1081: 0.42316415905952454
Loss at step 1082: 0.15171416103839874
Loss at step 1083: 0.1343165934085846
Loss at step 1084: 0.1734699010848999
Loss at step 1085: 0.2391282171010971
Loss at step 1086: 0.5853657126426697
Loss at step 1087: 2.704584836959839
Loss at step 1088: 1.5722651481628418
Loss at step 1089: 4.609668731689453
Loss at step 1090: 2.2974305152893066
Loss at step 1091: 2.9604227542877197
Loss at step 1092: 0.5223565697669983
Loss at step 1093: 0.7800109386444092
Loss at step 1094: 3.093496799468994
Loss at step 1095: 0.11320079118013382
Loss at step 1096: 2.033756732940674
Loss at step 1097: 0.0
Loss at step 1098: 1.221577525138855
Loss at step 1099: 0.9346742033958435
Saving training state...
[2025-08-03 11:23:13,889] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 11:23:18,798] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1100
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1100: 2.8071892261505127
Loss at step 1101: 0.7520071864128113
Loss at step 1102: 2.0884902477264404
Loss at step 1103: 1.2363795042037964
Loss at step 1104: 0.5942686796188354
Loss at step 1105: 3.035144805908203
Loss at step 1106: 1.6574645042419434
Loss at step 1107: 1.6185146570205688
Loss at step 1108: 1.3962558507919312
Loss at step 1109: 2.9915051460266113
Loss at step 1110: 1.3577691316604614
Loss at step 1111: 1.5142649412155151
Loss at step 1112: 1.3326867818832397
Loss at step 1113: 2.6544530391693115
Loss at step 1114: 2.654930830001831
Loss at step 1115: 2.863227605819702
Loss at step 1116: 0.23044231534004211
Loss at step 1117: 1.9033212661743164
Loss at step 1118: 0.10728774964809418
Loss at step 1119: 2.311523675918579
Loss at step 1120: 2.983474016189575
Loss at step 1121: 0.13847404718399048
Loss at step 1122: 2.5756335258483887
Loss at step 1123: 2.3772010803222656
Loss at step 1124: 0.069852314889431
Loss at step 1125: 1.0430024862289429
Loss at step 1126: 2.8397510051727295
Loss at step 1127: 0.3499792218208313
Loss at step 1128: 0.31992778182029724
Loss at step 1129: 0.7081520557403564
Loss at step 1130: 3.0915913581848145
Loss at step 1131: 2.2757389545440674
Loss at step 1132: 1.1507490873336792
Loss at step 1133: 2.2467141151428223
Loss at step 1134: 0.9720203280448914
Loss at step 1135: 2.7129387855529785
Loss at step 1136: 1.5689208507537842
Loss at step 1137: 0.6543464660644531
Loss at step 1138: 0.4144802689552307
Loss at step 1139: 1.1799132823944092
Loss at step 1140: 0.20456376671791077
Loss at step 1141: 1.2412745952606201
Loss at step 1142: 0.9371275901794434
Loss at step 1143: 1.023389220237732
Loss at step 1144: 0.03365589678287506
Loss at step 1145: 1.6664854288101196
Loss at step 1146: 0.9072691798210144
Loss at step 1147: 0.3779437243938446
Loss at step 1148: 0.0710429698228836
Loss at step 1149: 0.5744690299034119
Loss at step 1150: 2.3875153064727783
Loss at step 1151: 0.7640377283096313
Loss at step 1152: 2.682070016860962
Loss at step 1153: 0.795532763004303
Loss at step 1154: 1.5384172201156616
Loss at step 1155: 2.4539122581481934
Loss at step 1156: 0.03731604665517807
Loss at step 1157: 2.346637725830078
Loss at step 1158: 0.5925940871238708
Loss at step 1159: 2.513213634490967
Loss at step 1160: 2.1694016456604004
Loss at step 1161: 0.8959473967552185
Loss at step 1162: 0.1505444198846817
Loss at step 1163: 0.6427465677261353
Loss at step 1164: 0.06746697425842285
Loss at step 1165: 2.263251781463623
Loss at step 1166: 2.8077259063720703
Loss at step 1167: 2.82346510887146
Loss at step 1168: 2.2562296390533447
Loss at step 1169: 0.11280588060617447
Loss at step 1170: 1.046903133392334
Loss at step 1171: 1.7522692680358887
Loss at step 1172: 2.1923136711120605
Loss at step 1173: 0.4735005497932434
Loss at step 1174: 0.09981942176818848
Loss at step 1175: 2.5714190006256104
Loss at step 1176: 1.9878100156784058
Loss at step 1177: 0.6581534147262573
Loss at step 1178: 0.7331845164299011
Loss at step 1179: 0.43964439630508423
Loss at step 1180: 2.3727407455444336
Loss at step 1181: 2.478438377380371
Loss at step 1182: 0.5192066431045532
Loss at step 1183: 0.553521454334259
Loss at step 1184: 2.770068883895874
Loss at step 1185: 2.1360256671905518
Loss at step 1186: 1.182108759880066
Loss at step 1187: 2.4271419048309326
Loss at step 1188: 0.06645801663398743
Loss at step 1189: 1.4056719541549683
Loss at step 1190: 3.1040141582489014
Loss at step 1191: 2.449140787124634
Loss at step 1192: 2.7479348182678223
Loss at step 1193: 2.1709980964660645
Loss at step 1194: 0.4560300409793854
Loss at step 1195: 2.903729200363159
Loss at step 1196: 0.24841324985027313
Loss at step 1197: 2.35513973236084
Loss at step 1198: 2.900383949279785
Loss at step 1199: 0.9557932615280151
Saving training state...
[2025-08-03 15:26:21,852] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 15:26:26,969] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1200: 0.18970012664794922
Loss at step 1201: 0.8075576424598694
Loss at step 1202: 0.5599968433380127
Loss at step 1203: 1.2527316808700562
Loss at step 1204: 0.8770032525062561
Loss at step 1205: 2.9085693359375
Loss at step 1206: 2.4095213413238525
Loss at step 1207: 2.431306838989258
Loss at step 1208: 2.2769463062286377
Loss at step 1209: 1.3107250928878784
Loss at step 1210: 0.2650490403175354
Loss at step 1211: 3.251967668533325
Loss at step 1212: 2.191298246383667
Loss at step 1213: 0.6598770618438721
Loss at step 1214: 1.3051954507827759
Loss at step 1215: 1.0227060317993164
Loss at step 1216: 3.0756442546844482
Loss at step 1217: 2.4952478408813477
Loss at step 1218: 0.32911473512649536
Loss at step 1219: 0.6083729267120361
Loss at step 1220: 0.7295169234275818
Loss at step 1221: 0.48543524742126465
Loss at step 1222: 2.5749738216400146
Loss at step 1223: 0.1663532555103302
Loss at step 1224: 0.5064917206764221
Loss at step 1225: 0.13770848512649536
Loss at step 1226: 3.0865232944488525
Loss at step 1227: 2.036634683609009
Loss at step 1228: 2.213059663772583
Loss at step 1229: 0.13039204478263855
Loss at step 1230: 0.894252359867096
Loss at step 1231: 1.981711745262146
Loss at step 1232: 0.8311273455619812
Loss at step 1233: 1.568911075592041
Loss at step 1234: 1.9510210752487183
Loss at step 1235: 0.64898282289505
Loss at step 1236: 0.03167371451854706
Loss at step 1237: 0.363152414560318
Loss at step 1238: 0.5485333800315857
Loss at step 1239: 0.8631148934364319
Loss at step 1240: 0.9388637542724609
Loss at step 1241: 0.7744566798210144
Loss at step 1242: 0.6162927746772766
Loss at step 1243: 0.6671136021614075
Loss at step 1244: 2.4882121086120605
Loss at step 1245: 0.7926576137542725
Loss at step 1246: 0.1736304610967636
Loss at step 1247: 2.179748058319092
Loss at step 1248: 2.397554397583008
Loss at step 1249: 0.7858182787895203
Loss at step 1250: 1.959864854812622
Loss at step 1251: 0.16506969928741455
Loss at step 1252: 0.6563287377357483
Loss at step 1253: 0.6868994235992432
Loss at step 1254: 1.193149209022522
Loss at step 1255: 1.0520797967910767
Loss at step 1256: 0.22090572118759155
Loss at step 1257: 0.06353960186243057
Loss at step 1258: 0.030836325138807297
Loss at step 1259: 2.123135805130005
Loss at step 1260: 2.455591917037964
Loss at step 1261: 1.7918809652328491
Loss at step 1262: 2.1360535621643066
Loss at step 1263: 1.3367794752120972
Loss at step 1264: 0.15360583364963531
Loss at step 1265: 1.452857255935669
Loss at step 1266: 1.0216329097747803
Loss at step 1267: 1.9850378036499023
Loss at step 1268: 0.4277738332748413
Loss at step 1269: 0.0
Loss at step 1270: 2.704498052597046
Loss at step 1271: 2.9003899097442627
Loss at step 1272: 2.056525230407715
Loss at step 1273: 2.838024139404297
Loss at step 1274: 0.6167957186698914
Loss at step 1275: 2.9412169456481934
Loss at step 1276: 0.08354538679122925
Loss at step 1277: 2.0600314140319824
Loss at step 1278: 0.19087442755699158
Loss at step 1279: 2.3824143409729004
Loss at step 1280: 1.5951390266418457
Loss at step 1281: 0.071591317653656
Loss at step 1282: 1.2675217390060425
Loss at step 1283: 2.935293197631836
Loss at step 1284: 2.797119140625
Loss at step 1285: 0.10617262125015259
Loss at step 1286: 0.24114130437374115
Loss at step 1287: 0.8434839248657227
Loss at step 1288: 0.4678952693939209
Loss at step 1289: 3.442337989807129
Loss at step 1290: 1.6237443685531616
Loss at step 1291: 0.40157490968704224
Loss at step 1292: 2.651869535446167
Loss at step 1293: 0.21251410245895386
Loss at step 1294: 2.2789201736450195
Loss at step 1295: 2.9874043464660645
Loss at step 1296: 2.1549277305603027
Loss at step 1297: 0.7396225929260254
Loss at step 1298: 0.62138432264328
Loss at step 1299: 0.27303946018218994
Saving training state...
[2025-08-03 19:29:34,662] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 19:29:39,685] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-13-55-1e-4/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1300
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1300: 0.9503937363624573
Loss at step 1301: 1.6767933368682861
Loss at step 1302: 1.298852801322937
Loss at step 1303: 1.4091708660125732
Loss at step 1304: 2.6184568405151367
Loss at step 1305: 0.3615587651729584
Loss at step 1306: 1.676514983177185
Loss at step 1307: 2.48268461227417
Loss at step 1308: 2.4778635501861572
Loss at step 1309: 2.6280019283294678
Loss at step 1310: 0.6306062340736389
Loss at step 1311: 2.791879892349243
Loss at step 1312: 0.21416442096233368
Loss at step 1313: 2.6698906421661377
Loss at step 1314: 1.7650147676467896
Loss at step 1315: 3.0120484828948975
Loss at step 1316: 1.7537999153137207
Loss at step 1317: 0.7277455925941467
Loss at step 1318: 3.0431368350982666
Loss at step 1319: 0.9811813235282898
Loss at step 1320: 2.73464035987854
Loss at step 1321: 1.1995264291763306
Loss at step 1322: 2.9012415409088135
Loss at step 1323: 0.37835195660591125
Loss at step 1324: 0.8885032534599304
Loss at step 1325: 1.1671382188796997
Loss at step 1326: 0.5888136029243469
Loss at step 1327: 0.5897528529167175
Loss at step 1328: 1.7350690364837646
Loss at step 1329: 2.062225341796875
Loss at step 1330: 0.8308463096618652
Loss at step 1331: 0.6744298338890076
Loss at step 1332: 0.5433714389801025
Loss at step 1333: 1.9766839742660522
Loss at step 1334: 0.4534054696559906
Loss at step 1335: 0.2491687387228012
Loss at step 1336: 0.6086981296539307
Loss at step 1337: 0.03966968506574631
Loss at step 1338: 1.9566549062728882
Loss at step 1339: 0.08862625807523727
Loss at step 1340: 2.778930187225342
Loss at step 1341: 2.492255687713623
Loss at step 1342: 0.3411828577518463
Loss at step 1343: 0.9586009383201599
Loss at step 1344: 1.1707230806350708
Loss at step 1345: 0.6352506875991821
Loss at step 1346: 0.9142503142356873
Loss at step 1347: 0.6753633618354797
Loss at step 1348: 2.8071470260620117
Loss at step 1349: 0.03294610604643822
Loss at step 1350: 0.4171844720840454
Loss at step 1351: 2.5237224102020264
Loss at step 1352: 0.7492998242378235
Loss at step 1353: 2.1729302406311035
Loss at step 1354: 2.5051071643829346
Loss at step 1355: 0.40385448932647705
Loss at step 1356: 0.8299182057380676
Loss at step 1357: 1.149639368057251
Loss at step 1358: 1.7298057079315186
Loss at step 1359: 2.1166000366210938
Loss at step 1360: 1.7447746992111206
Loss at step 1361: 2.733675241470337
Loss at step 1362: 0.6909897327423096
Loss at step 1363: 1.552354097366333
Loss at step 1364: 0.31264492869377136
Loss at step 1365: 0.25748410820961
Loss at step 1366: 1.171162724494934
Loss at step 1367: 0.8427733778953552
Loss at step 1368: 0.38275542855262756
Loss at step 1369: 0.8669993281364441
Loss at step 1370: 0.18468093872070312
Loss at step 1371: 1.2731677293777466
Loss at step 1372: 2.043638229370117
Loss at step 1373: 0.06377311050891876
Loss at step 1374: 2.1003386974334717
Loss at step 1375: 0.38823798298835754
Loss at step 1376: 0.7460435032844543
Loss at step 1377: 0.14293576776981354
Loss at step 1378: 0.5372629761695862
Loss at step 1379: 0.6702356934547424
Loss at step 1380: 1.8451272249221802
Loss at step 1381: 0.23022796213626862
Loss at step 1382: 0.7900340557098389
Loss at step 1383: 0.3160686194896698
Loss at step 1384: 1.3582910299301147
Loss at step 1385: 2.602818489074707
Loss at step 1386: 0.695597231388092
Loss at step 1387: 2.408498764038086
Loss at step 1388: 2.7005906105041504
Loss at step 1389: 2.6350064277648926
Loss at step 1390: 1.0731433629989624
Loss at step 1391: 0.8309226036071777
