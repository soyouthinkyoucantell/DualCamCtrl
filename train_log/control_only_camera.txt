[2025-08-01 12:00:20,251] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 12:00:21,799] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 12:00:28,255] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 12:00:28,260] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 12:00:28,261] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 12:00:28,267] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-01 12:00:29,506] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 12:00:29,516] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 12:00:29,522] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 12:00:29,531] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 12:00:29,532] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 12:00:29,537] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-01 12:00:29,541] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-08-01 12:00:29,541] [INFO] [comm.py:707:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-08-01 12:00:29,547] [INFO] [comm.py:676:init_distributed] cdb=None
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_t5_umt5-xxl-enc-bf16.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, models_t5_umt5-xxl-enc-bf16.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, Wan2.1_VAE.pth) is redirected to (Wan-AI/Wan2.1-T2V-1.3B, Wan2.1_VAE.pth). You can use `redirect_common_files=False` to disable file redirection.
To avoid repeatedly downloading model files, (PAI/Wan2.1-Fun-V1.1-1.3B-InP, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth) is redirected to (Wan-AI/Wan2.1-I2V-14B-480P, models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth). You can use `redirect_common_files=False` to disable file redirection.
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    model_name: wan_video_dit model_class: WanModel
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
Loading models from: ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors
    model_name: wan_video_dit model_class: WanModel
hash_state_dict_keys(state_dict): 6d6ccde6845b95ad9114ab993d917893 from civitai
        This model is initialized with extra kwargs: {'has_image_input': True, 'patch_size': [1, 2, 2], 'in_dim': 36, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
    The following models are loaded: ['wan_video_dit'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    model_name: wan_video_text_encoder model_class: WanTextEncoder
    The following models are loaded: ['wan_video_text_encoder'].
    The following models are loaded: ['wan_video_text_encoder'].
    The following models are loaded: ['wan_video_text_encoder'].
    The following models are loaded: ['wan_video_text_encoder'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
Loading models from: ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
    model_name: wan_video_vae model_class: WanVideoVAE
    model_name: wan_video_vae model_class: WanVideoVAE
    model_name: wan_video_vae model_class: WanVideoVAE
    model_name: wan_video_vae model_class: WanVideoVAE
    The following models are loaded: ['wan_video_vae'].
    The following models are loaded: ['wan_video_vae'].
    The following models are loaded: ['wan_video_vae'].
    The following models are loaded: ['wan_video_vae'].
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-I2V-14B-480P
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
Loading models from: ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    model_name: wan_video_image_encoder model_class: WanImageEncoder
    The following models are loaded: ['wan_video_image_encoder'].
    The following models are loaded: ['wan_video_image_encoder'].
    The following models are loaded: ['wan_video_image_encoder'].
    The following models are loaded: ['wan_video_image_encoder'].
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
Using wan_video_text_encoder from ./models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ./models/PAI/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors.
Init CameraPoseEncoder with kwargs: {'downscale_factor': 8, 'channels': 1536, 'model_embed': 1536, 'nums_rb': 3, 'cin': 1536, 'ksize': 1, 'sk': True, 'use_conv': False, 'temporal_attention_nhead': 8, 'attention_block_types': ['Temporal_Self'], 'temporal_position_encoding': False, 'temporal_position_encoding_max_len': 16, 'rescale_output_factor': 1.0}
Init CameraPoseEncoder with kwargs: {'downscale_factor': 8, 'channels': 1536, 'model_embed': 1536, 'nums_rb': 3, 'cin': 1536, 'ksize': 1, 'sk': True, 'use_conv': False, 'temporal_attention_nhead': 8, 'attention_block_types': ['Temporal_Self'], 'temporal_position_encoding': False, 'temporal_position_encoding_max_len': 16, 'rescale_output_factor': 1.0}
Init CameraPoseEncoder with kwargs: {'downscale_factor': 8, 'channels': 1536, 'model_embed': 1536, 'nums_rb': 3, 'cin': 1536, 'ksize': 1, 'sk': True, 'use_conv': False, 'temporal_attention_nhead': 8, 'attention_block_types': ['Temporal_Self'], 'temporal_position_encoding': False, 'temporal_position_encoding_max_len': 16, 'rescale_output_factor': 1.0}
Init CameraPoseEncoder with kwargs: {'downscale_factor': 8, 'channels': 1536, 'model_embed': 1536, 'nums_rb': 3, 'cin': 1536, 'ksize': 1, 'sk': True, 'use_conv': False, 'temporal_attention_nhead': 8, 'attention_block_types': ['Temporal_Self'], 'temporal_position_encoding': False, 'temporal_position_encoding_max_len': 16, 'rescale_output_factor': 1.0}
Using 0 control blocks, for layers []
Module 'pose_encoder' (CameraPoseEncoder) has 702118656 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Using 0 control blocks, for layers []
Module 'pose_encoder' (CameraPoseEncoder) has 702118656 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Using 0 control blocks, for layers []
Module 'pose_encoder' (CameraPoseEncoder) has 702118656 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Using 0 control blocks, for layers []
Module 'pose_encoder' (CameraPoseEncoder) has 702118656 parameters.
Module 'control_blocks' (ModuleList) has 0 parameters.
Module 'control_zero_inits' (ModuleList) has 0 parameters.
Module 'blocks' (ModuleList) has 1534917120 parameters.
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
Controlnet dtype torch.bfloat16
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
Finished copying weights from main branch to control blocks.
Controlnet dtype torch.bfloat16
Finished copying weights from main branch to control blocks.
After alternation, blocks params:1566374400
Total param of the model : 2298004544
After alternation, blocks params:1566374400
Total param of the model : 2298004544
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
After alternation, blocks params:1566374400
After alternation, blocks params:1566374400
Total param of the model : 2298004544
Total param of the model : 2298004544
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
controlnet dtype torch.bfloat16
controlnet pose_encoder dtype torch.bfloat16
control blocks dtype torch.bfloat16
Using wan_video_vae from ./models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
Using wan_video_image_encoder from ./models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth.
No wan_video_motion_controller models available.
No wan_video_vace models available.
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Downloading Model from https://www.modelscope.cn to directory: /hpc2hdd/home/hongfeizhang/hongfei_workspace/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 2298004544
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Model parameters: 632076801
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Training only the camera model.
Trainable parameters: 733575936
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 184 scenes for testing, total 9233 scenes.
No training state found at meta_state.pt.
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 2298004544
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Model parameters: 632076801
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Training only the camera model.
Trainable parameters: 733575936
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 184 scenes for testing, total 9233 scenes.
No training state found at meta_state.pt.
Freezing model text_encoder.
Model parameters: 5680910336
Unfreezing model dit.
Model parameters: 2298004544
Freezing model vae.
Model parameters: 126892531
Freezing model image_encoder.
Freezing model text_encoder.
Model parameters: 632076801
Model parameters: 5680910336
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Training only the camera model.
Unfreezing model dit.
Model parameters: 2298004544
Trainable parameters: 733575936
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Freezing model vae.
Model parameters: 126892531
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Freezing model image_encoder.
Model parameters: 632076801
Using 184 scenes for testing, total 9233 scenes.
Training Module initialized with the following configurations:
Using gradient checkpointing: True
Using gradient checkpointing offload: False
Model data type : torch.bfloat16
Training only the camera model.
No training state found at meta_state.pt.
Trainable parameters: 733575936
Frame range: 5 to 41
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 9048 scenes for training, total 9233 scenes.
Frame range: 17 to 81
Info: no_extra_frame is set to True, extra_images and extra_image_frame_index will be None.
Using 184 scenes for testing, total 9233 scenes.
No training state found at meta_state.pt.
Initial accelerator with gradient accumulation steps: 2
Using 4 processes for training.
Preparing model, optimizer, dataloader, and scheduler with accelerator.
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.2 does not match the version torch was compiled with 12.6 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.547741174697876 seconds
Time to load cpu_adam op: 2.5478579998016357 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-08-01 12:01:19,472] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.2, git-hash=unknown, git-branch=unknown
[2025-08-01 12:01:19,472] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
[2025-08-01 12:01:19,472] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.6080825328826904 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-08-01 12:01:19,475] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.6258597373962402 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-08-01 12:01:19,550] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
[2025-08-01 12:01:22,879] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=4
	 self.mp_world_size=1
	 self.seq_dp_world_size=4
	 self.sequence_parallel_size=1
***********************************************
[2025-08-01 12:01:24,231] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-01 12:01:24,235] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-08-01 12:01:24,235] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-01 12:01:24,326] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-08-01 12:01:24,326] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-08-01 12:01:24,326] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-08-01 12:01:24,326] [INFO] [stage_1_and_2.py:172:__init__] Reduce bucket size 500000000
[2025-08-01 12:01:24,327] [INFO] [stage_1_and_2.py:173:__init__] Allgather bucket size 500000000
[2025-08-01 12:01:24,327] [INFO] [stage_1_and_2.py:174:__init__] CPU Offload: True
[2025-08-01 12:01:24,327] [INFO] [stage_1_and_2.py:175:__init__] Round robin gradient partitioning: False
[2025-08-01 12:01:27,142] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-01 12:01:27,143] [INFO] [utils.py:782:see_memory_usage] MA 16.32 GB         Max_MA 16.32 GB         CA 17.02 GB         Max_CA 17 GB 
[2025-08-01 12:01:27,143] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 112.84 GB, percent = 11.2%
[2025-08-01 12:01:27,522] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-01 12:01:27,523] [INFO] [utils.py:782:see_memory_usage] MA 16.32 GB         Max_MA 16.32 GB         CA 17.02 GB         Max_CA 17 GB 
[2025-08-01 12:01:27,523] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 113.56 GB, percent = 11.3%
[2025-08-01 12:01:27,523] [INFO] [stage_1_and_2.py:599:__init__] optimizer state initialized
[2025-08-01 12:01:27,688] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-01 12:01:27,688] [INFO] [utils.py:782:see_memory_usage] MA 16.32 GB         Max_MA 16.32 GB         CA 17.02 GB         Max_CA 17 GB 
[2025-08-01 12:01:27,688] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 113.56 GB, percent = 11.3%
[2025-08-01 12:01:27,697] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-08-01 12:01:27,697] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-08-01 12:01:27,697] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-08-01 12:01:27,697] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[9.999999999999999e-06], mom=[(0.9, 0.999)]
[2025-08-01 12:01:27,702] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-08-01 12:01:27,702] [INFO] [config.py:954:print] DeepSpeedEngine configuration:
[2025-08-01 12:01:27,702] [INFO] [config.py:958:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-08-01 12:01:27,702] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-01 12:01:27,702] [INFO] [config.py:958:print]   amp_enabled .................. False
[2025-08-01 12:01:27,702] [INFO] [config.py:958:print]   amp_params ................... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7233d08650>
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   communication_data_type ...... None
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   dataloader_drop_last ......... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   disable_allgather ............ False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   dump_state ................... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   elasticity_enabled ........... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   global_rank .................. 0
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   grad_accum_dtype ............. None
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 2
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   gradient_clipping ............ 1.0
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   graph_harvesting ............. False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   load_universal_checkpoint .... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   memory_breakdown ............. False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   mics_shard_size .............. -1
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   optimizer_name ............... None
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   optimizer_params ............. None
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-01 12:01:27,703] [INFO] [config.py:958:print]   pld_enabled .................. False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   pld_params ................... False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   prescale_gradients ........... False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   scheduler_name ............... None
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   scheduler_params ............. None
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   sparse_attention ............. None
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   steps_per_print .............. inf
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   train_batch_size ............. 8
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   use_node_local_storage ....... False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   weight_quantization_config ... None
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   world_size ................... 4
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  True
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   zero_enabled ................. True
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-01 12:01:27,704] [INFO] [config.py:958:print]   zero_optimization_stage ...... 2
[2025-08-01 12:01:27,704] [INFO] [config.py:944:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 2, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Model param dtype : torch.bfloat16
Starting training from scratch.
Validate every 200 steps.
Starting validation with model at epoch 0, global step 0
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 0: 0.3722420036792755
Loss at step 1: 0.14179079234600067
Loss at step 2: 0.10215754806995392
Loss at step 3: 0.15830256044864655
Loss at step 4: 0.11618229001760483
Loss at step 5: 0.03299110382795334
Loss at step 6: 0.45207640528678894
Loss at step 7: 0.015114882960915565
Loss at step 8: 0.21106457710266113
Loss at step 9: 0.22291678190231323
Loss at step 10: 0.06272159516811371
Loss at step 11: 0.1643444448709488
Loss at step 12: 0.2112996131181717
Loss at step 13: 0.047571565955877304
Loss at step 14: 0.11839582026004791
Loss at step 15: 0.2954305112361908
Loss at step 16: 0.19611142575740814
Loss at step 17: 0.11569581180810928
Loss at step 18: 0.04160824045538902
Loss at step 19: 0.054757725447416306
Loss at step 20: 0.10508187115192413
Loss at step 21: 0.1350359469652176
Loss at step 22: 0.1547032594680786
Loss at step 23: 0.1797347068786621
Loss at step 24: 0.09832813590765
Loss at step 25: 0.0663154274225235
Loss at step 26: 0.08409442007541656
Loss at step 27: 0.10153891891241074
Loss at step 28: 0.25489428639411926
Loss at step 29: 0.305759072303772
Loss at step 30: 0.11772453039884567
Loss at step 31: 0.033008597791194916
Loss at step 32: 0.0621807798743248
Loss at step 33: 0.11467950791120529
Loss at step 34: 0.044176194816827774
Loss at step 35: 0.14205974340438843
Loss at step 36: 0.015066773630678654
Loss at step 37: 0.3431931138038635
Loss at step 38: 0.10181626677513123
Loss at step 39: 0.11209934204816818
Loss at step 40: 0.13906224071979523
Loss at step 41: 0.08347070217132568
Loss at step 42: 0.15005840361118317
Loss at step 43: 0.11523646861314774
Loss at step 44: 0.015344424173235893
Loss at step 45: 0.21069742739200592
Loss at step 46: 0.04787372425198555
Loss at step 47: 0.06402873247861862
Loss at step 48: 0.04505562037229538
Loss at step 49: 0.016534501686692238
Loss at step 50: 0.13784171640872955
Loss at step 51: 0.05401691049337387
Loss at step 52: 0.008023031055927277
Loss at step 53: 0.00942519772797823
Loss at step 54: 0.15491510927677155
Loss at step 55: 0.12335517257452011
Loss at step 56: 0.23568470776081085
Loss at step 57: 0.07998337596654892
Loss at step 58: 0.08892039954662323
Loss at step 59: 0.21970170736312866
Loss at step 60: 0.14349840581417084
Loss at step 61: 0.042702581733465195
Loss at step 62: 0.027671005576848984
Loss at step 63: 0.3159221410751343
Loss at step 64: 0.057318463921546936
Loss at step 65: 0.2346668690443039
Loss at step 66: 0.09390727430582047
Loss at step 67: 0.04606965184211731
Loss at step 68: 0.21220292150974274
Loss at step 69: 0.05304335802793503
Loss at step 70: 0.04577060043811798
Loss at step 71: 0.05951112508773804
Loss at step 72: 0.18279683589935303
Loss at step 73: 0.1654379516839981
Loss at step 74: 0.053324662148952484
Loss at step 75: 0.07970280945301056
Loss at step 76: 0.1726958155632019
Loss at step 77: 0.3278396427631378
Loss at step 78: 0.1468925178050995
Loss at step 79: 0.04552619159221649
Loss at step 80: 0.09791067242622375
Loss at step 81: 0.06218909099698067
Loss at step 82: 0.025866935029625893
Loss at step 83: 0.3802020847797394
Loss at step 84: 0.1916230469942093
Loss at step 85: 0.03462722525000572
Loss at step 86: 0.02473023533821106
Loss at step 87: 0.11352811753749847
Loss at step 88: 0.29839619994163513
Loss at step 89: 0.04214368015527725
Loss at step 90: 0.4751664102077484
Loss at step 91: 0.12191782146692276
Loss at step 92: 0.2420375943183899
Loss at step 93: 0.03811237961053848
Loss at step 94: 0.028790276497602463
Loss at step 95: 0.08847025036811829
Loss at step 96: 0.13796772062778473
Loss at step 97: 0.16007289290428162
Loss at step 98: 0.05104609951376915
Loss at step 99: 0.10527408123016357
Loss at step 100: 0.5574466586112976
Loss at step 101: 0.018360495567321777
Loss at step 102: 0.4704829454421997
Loss at step 103: 0.3802112638950348
Loss at step 104: 0.037280213087797165
Loss at step 105: 0.1290566325187683
Loss at step 106: 0.147031769156456
Loss at step 107: 0.43078237771987915
Loss at step 108: 0.17085930705070496
Loss at step 109: 0.15395881235599518
Loss at step 110: 0.20199237763881683
Loss at step 111: 0.06834978610277176
Loss at step 112: 0.11939357966184616
Loss at step 113: 0.10774879902601242
Loss at step 114: 0.13669036328792572
Loss at step 115: 0.4024146497249603
Loss at step 116: 0.013108598999679089
Loss at step 117: 0.0947193130850792
Loss at step 118: 0.1495283842086792
Loss at step 119: 0.021041305735707283
Loss at step 120: 0.14294211566448212
Loss at step 121: 0.48862984776496887
Loss at step 122: 0.16818425059318542
Loss at step 123: 0.13343149423599243
Loss at step 124: 0.13095101714134216
Loss at step 125: 0.08040834963321686
Loss at step 126: 0.018147747963666916
Loss at step 127: 0.06552287936210632
Loss at step 128: 0.15303370356559753
Loss at step 129: 0.0754462480545044
Loss at step 130: 0.0121221411973238
Loss at step 131: 0.011773846112191677
Loss at step 132: 0.04180885851383209
Loss at step 133: 0.20953944325447083
Loss at step 134: 0.07631383091211319
Loss at step 135: 0.28298258781433105
Loss at step 136: 0.18111863732337952
Loss at step 137: 0.13919302821159363
Loss at step 138: 0.21094778180122375
Loss at step 139: 0.2973324656486511
Loss at step 140: 0.04748686030507088
Loss at step 141: 0.11677492409944534
Loss at step 142: 0.0554419606924057
Loss at step 143: 0.04515775665640831
Loss at step 144: 0.038176652044057846
Loss at step 145: 0.29784366488456726
Loss at step 146: 0.3172371983528137
Loss at step 147: 0.03910282254219055
Loss at step 148: 0.10210248082876205
Loss at step 149: 0.1867786943912506
Loss at step 150: 0.03494759649038315
Loss at step 151: 0.2271793782711029
Loss at step 152: 0.31279486417770386
Loss at step 153: 0.13380689918994904
Loss at step 154: 0.06271500885486603
Loss at step 155: 0.19951270520687103
Loss at step 156: 0.22726808488368988
Loss at step 157: 0.06092645972967148
Loss at step 158: 0.121949203312397
Loss at step 159: 0.048158399760723114
Loss at step 160: 0.10829910635948181
Loss at step 161: 0.050279319286346436
Loss at step 162: 0.14049650728702545
Loss at step 163: 0.07665099948644638
Loss at step 164: 0.11298057436943054
Loss at step 165: 0.07870161533355713
Loss at step 166: 0.041214924305677414
Loss at step 167: 0.11025926470756531
Loss at step 168: 0.21192854642868042
Loss at step 169: 0.07391694188117981
Loss at step 170: 0.195731058716774
Loss at step 171: 0.14068454504013062
Loss at step 172: 0.16341812908649445
Loss at step 173: 0.2535761892795563
Loss at step 174: 0.34759390354156494
Loss at step 175: 0.008795276284217834
Loss at step 176: 0.22070367634296417
Loss at step 177: 0.06867817044258118
Loss at step 178: 0.056625548750162125
Loss at step 179: 0.0
Loss at step 180: 0.14818662405014038
Loss at step 181: 0.07208993285894394
Loss at step 182: 0.026964936405420303
Loss at step 183: 0.011794543825089931
Loss at step 184: 0.026479413732886314
Loss at step 185: 0.045443177223205566
Loss at step 186: 0.043777357786893845
Loss at step 187: 0.13329651951789856
Loss at step 188: 0.009265831671655178
Loss at step 189: 0.08887013792991638
Loss at step 190: 0.16135716438293457
Loss at step 191: 0.04191799834370613
Loss at step 192: 0.11349485069513321
Loss at step 193: 0.14306846261024475
Loss at step 194: 0.2277367115020752
Loss at step 195: 0.15899181365966797
Loss at step 196: 0.12906986474990845
Loss at step 197: 0.09491486102342606
Loss at step 198: 0.1627124845981598
Loss at step 199: 0.09412211179733276
Saving training state...
[2025-08-01 13:09:48,472] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 13:09:58,088] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 200: 0.31488198041915894
Loss at step 201: 0.010583972558379173
Loss at step 202: 0.05374433472752571
Loss at step 203: 0.05752988159656525
Loss at step 204: 0.10519725829362869
Loss at step 205: 0.2693028450012207
Loss at step 206: 0.05681396648287773
Loss at step 207: 0.09705500304698944
Loss at step 208: 0.18406115472316742
Loss at step 209: 0.16701170802116394
Loss at step 210: 0.12661251425743103
Loss at step 211: 0.09091412276029587
Loss at step 212: 0.06715948134660721
Loss at step 213: 0.15587477385997772
Loss at step 214: 0.2731676995754242
Loss at step 215: 0.038222502917051315
Loss at step 216: 0.018665563315153122
Loss at step 217: 0.09007427096366882
Loss at step 218: 0.04872552677989006
Loss at step 219: 0.11544094979763031
Loss at step 220: 0.11620334535837173
Loss at step 221: 0.2347716987133026
Loss at step 222: 0.16236512362957
Loss at step 223: 0.013805847615003586
Loss at step 224: 0.009225389920175076
Loss at step 225: 0.1686791181564331
Loss at step 226: 0.082195945084095
Loss at step 227: 0.08023218810558319
Loss at step 228: 0.22178922593593597
Loss at step 229: 0.2664612829685211
Loss at step 230: 0.023656141012907028
Loss at step 231: 0.021401211619377136
Loss at step 232: 0.3322683870792389
Loss at step 233: 0.12233121693134308
Loss at step 234: 0.020709160715341568
Loss at step 235: 0.18041777610778809
Loss at step 236: 0.030864398926496506
Loss at step 237: 0.0
Loss at step 238: 0.17815382778644562
Loss at step 239: 0.019770028069615364
Loss at step 240: 0.1534879505634308
Loss at step 241: 0.03309124708175659
Loss at step 242: 0.13201485574245453
Loss at step 243: 0.05174774304032326
Loss at step 244: 0.2291605919599533
Loss at step 245: 0.09006202965974808
Loss at step 246: 0.06501272320747375
Loss at step 247: 0.3222810924053192
Loss at step 248: 0.09719903767108917
Loss at step 249: 0.07073338329792023
Loss at step 250: 0.18422959744930267
Loss at step 251: 0.04055048152804375
Loss at step 252: 0.06769749522209167
Loss at step 253: 0.19891640543937683
Loss at step 254: 0.02862986922264099
Loss at step 255: 0.05404171720147133
Loss at step 256: 0.16453047096729279
Loss at step 257: 0.07470788061618805
Loss at step 258: 0.11620397120714188
Loss at step 259: 0.307871550321579
Loss at step 260: 0.038000818341970444
Loss at step 261: 0.39420610666275024
Loss at step 262: 0.10177236050367355
Loss at step 263: 0.08660172671079636
Loss at step 264: 0.03448869660496712
Loss at step 265: 0.11017375439405441
Loss at step 266: 0.6020441055297852
Loss at step 267: 0.04947319254279137
Loss at step 268: 0.014456899836659431
Loss at step 269: 0.07090051472187042
Loss at step 270: 0.18847663700580597
Loss at step 271: 0.15134531259536743
Loss at step 272: 0.10798829793930054
Loss at step 273: 0.29351699352264404
Loss at step 274: 0.13963504135608673
Loss at step 275: 0.02637198567390442
Loss at step 276: 0.19208630919456482
Loss at step 277: 0.09094208478927612
Loss at step 278: 0.06338878720998764
Loss at step 279: 0.09536486864089966
Loss at step 280: 0.05210045725107193
Loss at step 281: 0.44986891746520996
Loss at step 282: 0.06118689104914665
Loss at step 283: 0.04227572679519653
Loss at step 284: 0.08911449462175369
Loss at step 285: 0.06408267468214035
Loss at step 286: 0.10921768844127655
Loss at step 287: 0.0583132803440094
Loss at step 288: 0.053555529564619064
Loss at step 289: 0.15863999724388123
Loss at step 290: 0.19669629633426666
Loss at step 291: 0.13513268530368805
Loss at step 292: 0.03576983883976936
Loss at step 293: 0.33826154470443726
Loss at step 294: 0.017323201522231102
Loss at step 295: 0.008849082514643669
Loss at step 296: 0.05215480178594589
Loss at step 297: 0.04803133383393288
Loss at step 298: 0.015827368944883347
Loss at step 299: 0.10544277727603912
Loss at step 300: 0.015426065772771835
Loss at step 301: 0.22563372552394867
Loss at step 302: 0.09491366147994995
Loss at step 303: 0.32771939039230347
Loss at step 304: 0.21435752511024475
Loss at step 305: 0.10328038036823273
Loss at step 306: 0.10967518389225006
Loss at step 307: 0.215287446975708
Loss at step 308: 0.10333490371704102
Loss at step 309: 0.04541186988353729
Loss at step 310: 0.016689307987689972
Loss at step 311: 0.1977500170469284
Loss at step 312: 0.12096218764781952
Loss at step 313: 0.04719807580113411
Loss at step 314: 0.5414071679115295
Loss at step 315: 0.04397011548280716
Loss at step 316: 0.10245926678180695
Loss at step 317: 0.135337695479393
Loss at step 318: 0.1152048408985138
Loss at step 319: 0.061029672622680664
Loss at step 320: 0.006389116868376732
Loss at step 321: 0.39673393964767456
Loss at step 322: 0.015827583149075508
Loss at step 323: 0.27708956599235535
Loss at step 324: 0.15594008564949036
Loss at step 325: 0.2047060877084732
Loss at step 326: 0.12500807642936707
Loss at step 327: 0.16232876479625702
Loss at step 328: 0.1587013155221939
Loss at step 329: 0.11429332196712494
Loss at step 330: 0.20100466907024384
Loss at step 331: 0.20839357376098633
Loss at step 332: 0.20348960161209106
Loss at step 333: 0.20994247496128082
Loss at step 334: 0.06614191085100174
Loss at step 335: 0.16577671468257904
Loss at step 336: 0.10866647958755493
Loss at step 337: 0.3073941767215729
Loss at step 338: 0.05276016891002655
Loss at step 339: 0.022223766893148422
Loss at step 340: 0.08130195736885071
Loss at step 341: 0.21005375683307648
Loss at step 342: 0.018312418833374977
Loss at step 343: 0.23044009506702423
Loss at step 344: 0.2797359526157379
Loss at step 345: 0.016131572425365448
Loss at step 346: 0.1017325222492218
Loss at step 347: 0.04932306706905365
Loss at step 348: 0.4187541604042053
Loss at step 349: 0.04317111521959305
Loss at step 350: 0.33750155568122864
Loss at step 351: 0.07597566395998001
Loss at step 352: 0.2659069895744324
Loss at step 353: 0.2474113553762436
Loss at step 354: 0.04184707626700401
Loss at step 355: 0.016266178339719772
Loss at step 356: 0.0993853509426117
Loss at step 357: 0.03601524606347084
Loss at step 358: 0.14465714991092682
Loss at step 359: 0.04204993695020676
Loss at step 360: 0.31186988949775696
Loss at step 361: 0.16019214689731598
Loss at step 362: 0.07839526981115341
Loss at step 363: 0.16825330257415771
Loss at step 364: 0.04146682098507881
Loss at step 365: 0.10443799197673798
Loss at step 366: 0.022944146767258644
Loss at step 367: 0.0910809263586998
Loss at step 368: 0.0479806549847126
Loss at step 369: 0.016476422548294067
Loss at step 370: 0.06344594061374664
Loss at step 371: 0.19558614492416382
Loss at step 372: 0.14387232065200806
Loss at step 373: 0.04466382786631584
Loss at step 374: 0.018262306228280067
Loss at step 375: 0.1453215330839157
Loss at step 376: 0.19611622393131256
Loss at step 377: 0.03767983987927437
Loss at step 378: 0.03338412195444107
Loss at step 379: 0.13819578289985657
Loss at step 380: 0.5766714215278625
Loss at step 381: 0.16328544914722443
Loss at step 382: 0.01103848498314619
Loss at step 383: 0.06155828759074211
Loss at step 384: 0.14461497962474823
Loss at step 385: 0.2638584077358246
Loss at step 386: 0.03402028605341911
Loss at step 387: 0.08567695319652557
Loss at step 388: 0.15979990363121033
Loss at step 389: 0.08571215718984604
Loss at step 390: 0.10188348591327667
Loss at step 391: 0.3926701545715332
Loss at step 392: 0.17255757749080658
Loss at step 393: 0.18503768742084503
Loss at step 394: 0.09790533781051636
Loss at step 395: 0.2598680853843689
Loss at step 396: 0.10181854665279388
Loss at step 397: 0.11302365362644196
Loss at step 398: 0.2059856355190277
Loss at step 399: 0.03688904270529747
Saving training state...
[2025-08-01 14:20:45,527] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 14:20:54,781] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 400: 0.23735342919826508
Loss at step 401: 0.2990274727344513
Loss at step 402: 0.051155801862478256
Loss at step 403: 0.06204099953174591
Loss at step 404: 0.38602760434150696
Loss at step 405: 0.06711546331644058
Loss at step 406: 0.022280408069491386
Loss at step 407: 0.22204259037971497
Loss at step 408: 0.049592968076467514
Loss at step 409: 0.10563571751117706
Loss at step 410: 0.0
Loss at step 411: 0.05994437634944916
Loss at step 412: 0.33758535981178284
Loss at step 413: 0.056575555354356766
Loss at step 414: 0.24056006968021393
Loss at step 415: 0.07805313169956207
Loss at step 416: 0.1280086487531662
Loss at step 417: 0.32643207907676697
Loss at step 418: 0.18669170141220093
Loss at step 419: 0.06834323704242706
Loss at step 420: 0.31595301628112793
Loss at step 421: 0.16095609962940216
Loss at step 422: 0.08834877610206604
Loss at step 423: 0.07883425056934357
Loss at step 424: 0.11621890962123871
Loss at step 425: 0.05462005361914635
Loss at step 426: 0.08973785489797592
Loss at step 427: 0.032233405858278275
Loss at step 428: 0.14129428565502167
Loss at step 429: 0.052913811057806015
Loss at step 430: 0.05578884854912758
Loss at step 431: 0.0666862204670906
Loss at step 432: 0.018488578498363495
Loss at step 433: 0.19965343177318573
Loss at step 434: 0.1271979808807373
Loss at step 435: 0.03863011300563812
Loss at step 436: 0.07845279574394226
Loss at step 437: 0.18711428344249725
Loss at step 438: 0.16417080163955688
Loss at step 439: 0.3766351044178009
Loss at step 440: 0.2768598794937134
Loss at step 441: 0.016785919666290283
Loss at step 442: 0.24858449399471283
Loss at step 443: 0.026766186580061913
Loss at step 444: 0.12973490357398987
Loss at step 445: 0.33684441447257996
Loss at step 446: 0.17166650295257568
Loss at step 447: 0.04187478870153427
Loss at step 448: 0.15079399943351746
Loss at step 449: 0.10635970532894135
Loss at step 450: 0.013325709849596024
Loss at step 451: 0.2052755504846573
Loss at step 452: 0.18800956010818481
Loss at step 453: 0.19936898350715637
Loss at step 454: 0.1733122318983078
Loss at step 455: 0.191358745098114
Loss at step 456: 0.02347390539944172
Loss at step 457: 0.2514227628707886
Loss at step 458: 0.13514137268066406
Loss at step 459: 0.4664526581764221
Loss at step 460: 0.21507690846920013
Loss at step 461: 0.3684214651584625
Loss at step 462: 0.4737517833709717
Loss at step 463: 0.4131157398223877
Loss at step 464: 0.06531471014022827
Loss at step 465: 0.007616757415235043
Loss at step 466: 0.28429123759269714
Loss at step 467: 0.01375411543995142
Loss at step 468: 0.009271793067455292
Loss at step 469: 0.04799223318696022
Loss at step 470: 0.09745557606220245
Loss at step 471: 0.04263995587825775
Loss at step 472: 0.5353201031684875
Loss at step 473: 0.14532755315303802
Loss at step 474: 0.1778741478919983
Loss at step 475: 0.05334017053246498
Loss at step 476: 0.039282314479351044
Loss at step 477: 0.15413877367973328
Loss at step 478: 0.09469451755285263
Loss at step 479: 0.1543259471654892
Loss at step 480: 0.04046262428164482
Loss at step 481: 0.35419169068336487
Loss at step 482: 0.24369800090789795
Loss at step 483: 0.018359115347266197
Loss at step 484: 0.17734074592590332
Loss at step 485: 0.18530058860778809
Loss at step 486: 0.16861633956432343
Loss at step 487: 0.18921774625778198
Loss at step 488: 0.09801024943590164
Loss at step 489: 0.15228843688964844
Loss at step 490: 0.3272993564605713
Loss at step 491: 0.03788153454661369
Loss at step 492: 0.16960616409778595
Loss at step 493: 0.16329431533813477
Loss at step 494: 0.20773616433143616
Loss at step 495: 0.045815058052539825
Loss at step 496: 0.053394418209791183
Loss at step 497: 0.06934930384159088
Loss at step 498: 0.1881900429725647
Loss at step 499: 0.11297985166311264
Loss at step 500: 0.3006970286369324
Loss at step 501: 0.5112257599830627
Loss at step 502: 0.09275741875171661
Loss at step 503: 0.12463250756263733
Loss at step 504: 0.043243005871772766
Loss at step 505: 0.33540377020835876
Loss at step 506: 0.2684309184551239
Loss at step 507: 0.18966509401798248
Loss at step 508: 0.22659333050251007
Loss at step 509: 0.15501873195171356
Loss at step 510: 0.10168185085058212
Loss at step 511: 0.3009616732597351
Loss at step 512: 0.08302707970142365
Loss at step 513: 0.16727232933044434
Loss at step 514: 0.04260321334004402
Loss at step 515: 0.026417827233672142
Loss at step 516: 0.19039200246334076
Loss at step 517: 0.0709252879023552
Loss at step 518: 0.13565336167812347
Loss at step 519: 0.012515240348875523
Loss at step 520: 0.12060479074716568
Loss at step 521: 0.07998057454824448
Loss at step 522: 0.14678633213043213
Loss at step 523: 0.31614232063293457
Loss at step 524: 0.06598233431577682
Loss at step 525: 0.03311075642704964
Loss at step 526: 0.1706785410642624
Loss at step 527: 0.3052540123462677
Loss at step 528: 0.03978803753852844
Loss at step 529: 0.025598404929041862
Loss at step 530: 0.22352923452854156
Loss at step 531: 0.028998028486967087
Loss at step 532: 0.0924149826169014
Loss at step 533: 0.014650491066277027
Loss at step 534: 0.2530108392238617
Loss at step 535: 0.01828644797205925
Loss at step 536: 0.4147736132144928
Loss at step 537: 0.23679757118225098
Loss at step 538: 0.009276377968490124
Loss at step 539: 0.21428993344306946
Loss at step 540: 0.09222371876239777
Loss at step 541: 0.04044592007994652
Loss at step 542: 0.10671264678239822
Loss at step 543: 0.026578838005661964
Loss at step 544: 0.1815193146467209
Loss at step 545: 0.03435001149773598
Loss at step 546: 0.12878212332725525
Loss at step 547: 0.03929268196225166
Loss at step 548: 0.2729920446872711
Loss at step 549: 0.17172645032405853
Loss at step 550: 0.14901018142700195
Loss at step 551: 0.2784731686115265
Loss at step 552: 0.20664137601852417
Loss at step 553: 0.4842943251132965
Loss at step 554: 0.03398757800459862
Loss at step 555: 0.2214687019586563
Loss at step 556: 0.204192653298378
Loss at step 557: 0.222037672996521
Loss at step 558: 0.0741439238190651
Loss at step 559: 0.00450664758682251
Loss at step 560: 0.13947349786758423
Loss at step 561: 0.016674891114234924
Loss at step 562: 0.0983528196811676
Loss at step 563: 0.05472380667924881
Loss at step 564: 0.01431182399392128
Loss at step 565: 0.23765072226524353
Loss at step 566: 0.17674285173416138
Loss at step 567: 0.19464103877544403
Loss at step 568: 0.12947584688663483
Loss at step 569: 0.05304607376456261
Loss at step 570: 0.090177521109581
Loss at step 571: 0.19256435334682465
Loss at step 572: 0.1060490608215332
Loss at step 573: 0.01888180337846279
Loss at step 574: 0.041151538491249084
Loss at step 575: 0.18193788826465607
Loss at step 576: 0.10931047797203064
Loss at step 577: 0.33105796575546265
Loss at step 578: 0.04594903439283371
Loss at step 579: 0.18647626042366028
Loss at step 580: 0.006532733328640461
Loss at step 581: 0.04390201345086098
Loss at step 582: 0.18121299147605896
Loss at step 583: 0.1039421334862709
Loss at step 584: 0.040166597813367844
Loss at step 585: 0.14107230305671692
Loss at step 586: 0.11330229789018631
Loss at step 587: 0.1116790845990181
Loss at step 588: 0.0883285254240036
Loss at step 589: 0.11769910901784897
Loss at step 590: 0.039331164211034775
Loss at step 591: 0.039026740938425064
Loss at step 592: 0.12499914318323135
Loss at step 593: 0.2174193263053894
Loss at step 594: 0.09831788390874863
Loss at step 595: 0.1732531189918518
Loss at step 596: 0.34531277418136597
Loss at step 597: 0.05678870901465416
Loss at step 598: 0.01387585885822773
Loss at step 599: 0.09738568961620331
Saving training state...
[2025-08-01 15:29:45,532] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 15:29:53,297] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 600: 0.2919110059738159
Loss at step 601: 0.031227707862854004
Loss at step 602: 0.12165579944849014
Loss at step 603: 0.194532573223114
Loss at step 604: 0.08647487312555313
Loss at step 605: 0.07350505143404007
Loss at step 606: 0.13631953299045563
Loss at step 607: 0.03783212602138519
Loss at step 608: 0.03168774023652077
Loss at step 609: 0.01927717588841915
Loss at step 610: 0.025188006460666656
Loss at step 611: 0.32229161262512207
Loss at step 612: 0.1313895881175995
Loss at step 613: 0.023514019325375557
Loss at step 614: 0.3233695924282074
Loss at step 615: 0.4327258765697479
Loss at step 616: 0.059260293841362
Loss at step 617: 0.0
Loss at step 618: 0.09517032653093338
Loss at step 619: 0.07337632030248642
Loss at step 620: 0.1266157031059265
Loss at step 621: 0.053889669477939606
Loss at step 622: 0.1016649454832077
Loss at step 623: 0.16693465411663055
Loss at step 624: 0.43314048647880554
Loss at step 625: 0.1713598519563675
Loss at step 626: 0.23721875250339508
Loss at step 627: 0.046826109290122986
Loss at step 628: 0.13519516587257385
Loss at step 629: 0.1953149437904358
Loss at step 630: 0.4519835114479065
Loss at step 631: 0.060438256710767746
Loss at step 632: 0.01993511989712715
Loss at step 633: 0.1218792051076889
Loss at step 634: 0.052480436861515045
Loss at step 635: 0.15792641043663025
Loss at step 636: 0.06602495908737183
Loss at step 637: 0.014942983165383339
Loss at step 638: 0.27310967445373535
Loss at step 639: 0.052503425627946854
Loss at step 640: 0.08350841701030731
Loss at step 641: 0.14162400364875793
Loss at step 642: 0.02060898207128048
Loss at step 643: 0.17936906218528748
Loss at step 644: 0.10026402771472931
Loss at step 645: 0.42054906487464905
Loss at step 646: 0.21166999638080597
Loss at step 647: 0.05860232934355736
Loss at step 648: 0.0660490095615387
Loss at step 649: 0.008955598808825016
Loss at step 650: 0.11611388623714447
Loss at step 651: 0.060687996447086334
Loss at step 652: 0.04188171774148941
Loss at step 653: 0.41221383213996887
Loss at step 654: 0.023204796016216278
Loss at step 655: 0.2522794306278229
Loss at step 656: 0.12590673565864563
Loss at step 657: 0.09316755086183548
Loss at step 658: 0.08786027133464813
Loss at step 659: 0.12338585406541824
Loss at step 660: 0.18923123180866241
Loss at step 661: 0.005012840032577515
Loss at step 662: 0.019101815298199654
Loss at step 663: 0.09737613797187805
Loss at step 664: 0.0
Loss at step 665: 0.02202143520116806
Loss at step 666: 0.10381142050027847
Loss at step 667: 0.3001028001308441
Loss at step 668: 0.08799655735492706
Loss at step 669: 0.576402485370636
Loss at step 670: 0.16508269309997559
Loss at step 671: 0.4231714904308319
Loss at step 672: 0.18078120052814484
Loss at step 673: 0.008618324995040894
Loss at step 674: 0.03836815059185028
Loss at step 675: 0.0340726263821125
Loss at step 676: 0.24501237273216248
Loss at step 677: 0.2715974450111389
Loss at step 678: 0.26984527707099915
Loss at step 679: 0.018062030896544456
Loss at step 680: 0.04536910727620125
Loss at step 681: 0.029822520911693573
Loss at step 682: 0.025680115446448326
Loss at step 683: 0.09021145105361938
Loss at step 684: 0.0847105160355568
Loss at step 685: 0.34655314683914185
Loss at step 686: 0.08212731033563614
Loss at step 687: 0.25607502460479736
Loss at step 688: 0.077801413834095
Loss at step 689: 0.25628796219825745
Loss at step 690: 0.1527603715658188
Loss at step 691: 0.09011566638946533
Loss at step 692: 0.32587099075317383
Loss at step 693: 0.0036470387130975723
Loss at step 694: 0.022936126217246056
Loss at step 695: 0.2731456160545349
Loss at step 696: 0.10671854764223099
Loss at step 697: 0.084903784096241
Loss at step 698: 0.2170782834291458
Loss at step 699: 0.09564156830310822
Loss at step 700: 0.1647624522447586
Loss at step 701: 0.07634963095188141
Loss at step 702: 0.2880326509475708
Loss at step 703: 0.06786053627729416
Loss at step 704: 0.014441611245274544
Loss at step 705: 0.12727637588977814
Loss at step 706: 0.013753209263086319
Loss at step 707: 0.33774709701538086
Loss at step 708: 0.06607917696237564
Loss at step 709: 0.37236467003822327
Loss at step 710: 0.10381559282541275
Loss at step 711: 0.0840647742152214
Loss at step 712: 0.2715601623058319
Loss at step 713: 0.3148423731327057
Loss at step 714: 0.4730280041694641
Loss at step 715: 0.37268126010894775
Loss at step 716: 0.06139100342988968
Loss at step 717: 0.12917593121528625
Loss at step 718: 0.00968415942043066
Loss at step 719: 0.09955431520938873
Loss at step 720: 0.051853906363248825
Loss at step 721: 0.023139895871281624
Loss at step 722: 0.03799986466765404
Loss at step 723: 0.07183591276407242
Loss at step 724: 0.08260410279035568
Loss at step 725: 0.08715007454156876
Loss at step 726: 0.010221044532954693
Loss at step 727: 0.041943009942770004
Loss at step 728: 0.06755968928337097
Loss at step 729: 0.07729944586753845
Loss at step 730: 0.19412696361541748
Loss at step 731: 0.34076735377311707
Loss at step 732: 0.11786317080259323
Loss at step 733: 0.1579573005437851
Loss at step 734: 0.0415533073246479
Loss at step 735: 0.2011905163526535
Loss at step 736: 0.31514257192611694
Loss at step 737: 0.10857547074556351
Loss at step 738: 0.10569854825735092
Loss at step 739: 0.016674594953656197
Loss at step 740: 0.29514700174331665
Loss at step 741: 0.054997045546770096
Loss at step 742: 0.18757569789886475
Loss at step 743: 0.1952836960554123
Loss at step 744: 0.01706886477768421
Loss at step 745: 0.08580759912729263
Loss at step 746: 0.26039445400238037
Loss at step 747: 0.03426431491971016
Loss at step 748: 0.0
Loss at step 749: 0.10928607732057571
Loss at step 750: 0.45333465933799744
Loss at step 751: 0.14120842516422272
Loss at step 752: 0.1743081957101822
Loss at step 753: 0.23242151737213135
Loss at step 754: 0.09675581753253937
Loss at step 755: 0.10317659378051758
Loss at step 756: 0.12475211173295975
Loss at step 757: 0.36689165234565735
Loss at step 758: 0.17952606081962585
Loss at step 759: 0.02654542773962021
Loss at step 760: 0.20619098842144012
Loss at step 761: 0.30257558822631836
Loss at step 762: 0.14040228724479675
Loss at step 763: 0.03330261632800102
Loss at step 764: 0.11616078019142151
Loss at step 765: 0.15529949963092804
Loss at step 766: 0.1973266750574112
Loss at step 767: 0.2908881604671478
Loss at step 768: 0.3033369481563568
Loss at step 769: 0.12338729202747345
Loss at step 770: 0.11182914674282074
Loss at step 771: 0.14453192055225372
Loss at step 772: 0.1208854615688324
Loss at step 773: 0.20797374844551086
Loss at step 774: 0.13229815661907196
Loss at step 775: 0.13938602805137634
Loss at step 776: 0.05472153425216675
Loss at step 777: 0.20469850301742554
Loss at step 778: 0.13992182910442352
Loss at step 779: 0.16992725431919098
Loss at step 780: 0.14931514859199524
Loss at step 781: 0.1403210610151291
Loss at step 782: 0.6164899468421936
Loss at step 783: 0.0608140304684639
Loss at step 784: 0.08906658738851547
Loss at step 785: 0.06192343682050705
Loss at step 786: 0.2179412692785263
Loss at step 787: 0.1314392238855362
Loss at step 788: 0.19456444680690765
Loss at step 789: 0.08013350516557693
Loss at step 790: 0.07465023547410965
Loss at step 791: 0.2610775828361511
Loss at step 792: 0.07025377452373505
Loss at step 793: 0.3956853151321411
Loss at step 794: 0.1986207365989685
Loss at step 795: 0.1714242547750473
Loss at step 796: 0.3410329520702362
Loss at step 797: 0.14690051972866058
Loss at step 798: 0.0
Loss at step 799: 0.31116989254951477
Saving training state...
[2025-08-01 16:39:21,691] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 16:39:28,201] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 800: 0.41524648666381836
Loss at step 801: 0.05885526165366173
Loss at step 802: 0.29453009366989136
Loss at step 803: 0.16868405044078827
Loss at step 804: 0.04774317145347595
Loss at step 805: 0.1149776354432106
Loss at step 806: 0.01560869812965393
Loss at step 807: 0.02688443474471569
Loss at step 808: 0.26280704140663147
Loss at step 809: 0.11508741974830627
Loss at step 810: 0.1314108669757843
Loss at step 811: 0.06852449476718903
Loss at step 812: 0.12691578269004822
Loss at step 813: 0.07614272087812424
Loss at step 814: 0.3229839503765106
Loss at step 815: 0.015747230499982834
Loss at step 816: 0.07331273704767227
Loss at step 817: 0.2645929753780365
Loss at step 818: 0.07390106469392776
Loss at step 819: 0.2337944358587265
Loss at step 820: 0.34705185890197754
Loss at step 821: 0.163955956697464
Loss at step 822: 0.26388296484947205
Loss at step 823: 0.07451513409614563
Loss at step 824: 0.041636981070041656
Loss at step 825: 0.12314320355653763
Loss at step 826: 0.28529149293899536
Loss at step 827: 0.02532021328806877
Loss at step 828: 0.29921334981918335
Loss at step 829: 0.28322872519493103
Loss at step 830: 0.13469038903713226
Loss at step 831: 0.14322622120380402
Loss at step 832: 0.044712383300065994
Loss at step 833: 0.041480958461761475
Loss at step 834: 0.10624267160892487
Loss at step 835: 0.2627067267894745
Loss at step 836: 0.08720117807388306
Loss at step 837: 0.012739731930196285
Loss at step 838: 0.02911032922565937
Loss at step 839: 0.027498995885252953
Loss at step 840: 0.08860942721366882
Loss at step 841: 0.10502951592206955
Loss at step 842: 0.06356779485940933
Loss at step 843: 0.37773022055625916
Loss at step 844: 0.03800037130713463
Loss at step 845: 0.041057340800762177
Loss at step 846: 0.3192416727542877
Loss at step 847: 0.15986289083957672
Loss at step 848: 0.1090020164847374
Loss at step 849: 0.061567649245262146
Loss at step 850: 0.04643101245164871
Loss at step 851: 0.19466561079025269
Loss at step 852: 0.15982480347156525
Loss at step 853: 0.23331817984580994
Loss at step 854: 0.02132456935942173
Loss at step 855: 0.09815690666437149
Loss at step 856: 0.0967189222574234
Loss at step 857: 0.09594584256410599
Loss at step 858: 0.15060783922672272
Loss at step 859: 0.17175154387950897
Loss at step 860: 0.296132892370224
Loss at step 861: 0.06997284293174744
Loss at step 862: 0.125710591673851
Loss at step 863: 0.18630234897136688
Loss at step 864: 0.1545519232749939
Loss at step 865: 0.11698742210865021
Loss at step 866: 0.18558269739151
Loss at step 867: 0.09990415722131729
Loss at step 868: 0.08304686844348907
Loss at step 869: 0.14264562726020813
Loss at step 870: 0.30439677834510803
Loss at step 871: 0.15252329409122467
Loss at step 872: 0.06442485004663467
Loss at step 873: 0.04208281263709068
Loss at step 874: 0.05509680509567261
Loss at step 875: 0.08747070282697678
Loss at step 876: 0.059355735778808594
Loss at step 877: 0.12545061111450195
Loss at step 878: 0.12311333417892456
Loss at step 879: 0.20010460913181305
Loss at step 880: 0.29528868198394775
Loss at step 881: 0.05744922533631325
Loss at step 882: 0.008942216634750366
Loss at step 883: 0.19609668850898743
Loss at step 884: 0.4552758038043976
Loss at step 885: 0.09279672801494598
Loss at step 886: 0.15167507529258728
Loss at step 887: 0.2037012279033661
Loss at step 888: 0.0894017443060875
Loss at step 889: 0.09113038331270218
Loss at step 890: 0.015490422956645489
Loss at step 891: 0.04725712165236473
Loss at step 892: 0.2765686511993408
Loss at step 893: 0.06943847239017487
Loss at step 894: 0.19507980346679688
Loss at step 895: 0.07692592591047287
Loss at step 896: 0.42302748560905457
Loss at step 897: 0.05283037945628166
Loss at step 898: 0.08018899708986282
Loss at step 899: 0.02424788847565651
Loss at step 900: 0.2690384089946747
Loss at step 901: 0.3256184756755829
Loss at step 902: 0.056546613574028015
Loss at step 903: 0.01339766662567854
Loss at step 904: 0.04074183478951454
Loss at step 905: 0.014170055277645588
Loss at step 906: 0.1263705939054489
Loss at step 907: 0.23976430296897888
Loss at step 908: 0.17693649232387543
Loss at step 909: 0.015924029052257538
Loss at step 910: 0.05462181940674782
Loss at step 911: 0.2710365653038025
Loss at step 912: 0.23037730157375336
Loss at step 913: 0.06574758887290955
Loss at step 914: 0.16947714984416962
Loss at step 915: 0.07996823638677597
Loss at step 916: 0.08722627907991409
Loss at step 917: 0.21655674278736115
Loss at step 918: 0.0845252051949501
Loss at step 919: 0.05831855162978172
Loss at step 920: 0.2990817129611969
Loss at step 921: 0.05962388589978218
Loss at step 922: 0.07151634246110916
Loss at step 923: 0.21766765415668488
Loss at step 924: 0.27728071808815
Loss at step 925: 0.031932201236486435
Loss at step 926: 0.057118408381938934
Loss at step 927: 0.11159390211105347
Loss at step 928: 0.06785047054290771
Loss at step 929: 0.3434632420539856
Loss at step 930: 0.2685798406600952
Loss at step 931: 0.18551817536354065
Loss at step 932: 0.15012666583061218
Loss at step 933: 0.2813734710216522
Loss at step 934: 0.11044648289680481
Loss at step 935: 0.3386160135269165
Loss at step 936: 0.0817478746175766
Loss at step 937: 0.14487068355083466
Loss at step 938: 0.21435561776161194
Loss at step 939: 0.08338360488414764
Loss at step 940: 0.012429860420525074
Loss at step 941: 0.030362384393811226
Loss at step 942: 0.09483741968870163
Loss at step 943: 0.05396242439746857
Loss at step 944: 0.055956508964300156
Loss at step 945: 0.1132100522518158
Loss at step 946: 0.08763696998357773
Loss at step 947: 0.4085496962070465
Loss at step 948: 0.07357330620288849
Loss at step 949: 0.02411663718521595
Loss at step 950: 0.046251386404037476
Loss at step 951: 0.12590515613555908
Loss at step 952: 0.03669596463441849
Loss at step 953: 0.11899882555007935
Loss at step 954: 0.06341429054737091
Loss at step 955: 0.10593272745609283
Loss at step 956: 0.5794177651405334
Loss at step 957: 0.14344298839569092
Loss at step 958: 0.081387460231781
Loss at step 959: 0.33363988995552063
Loss at step 960: 0.1251736283302307
Loss at step 961: 0.14441539347171783
Loss at step 962: 0.0606810599565506
Loss at step 963: 0.20300175249576569
Loss at step 964: 0.35774528980255127
Loss at step 965: 0.0901632234454155
Loss at step 966: 0.1976604163646698
Loss at step 967: 0.0228301752358675
Loss at step 968: 0.14052505791187286
Loss at step 969: 0.3103838562965393
Loss at step 970: 0.020638981834053993
Loss at step 971: 0.059969447553157806
Loss at step 972: 0.39754506945610046
Loss at step 973: 0.009517998434603214
Loss at step 974: 0.3144597113132477
Loss at step 975: 0.028136413544416428
Loss at step 976: 0.04216794669628143
Loss at step 977: 0.029791880398988724
Loss at step 978: 0.11668820679187775
Loss at step 979: 0.017753228545188904
Loss at step 980: 0.12531428039073944
Loss at step 981: 0.08389847725629807
Loss at step 982: 0.32734882831573486
Loss at step 983: 0.03883914276957512
Loss at step 984: 0.11747178435325623
Loss at step 985: 0.10404622554779053
Loss at step 986: 0.0361141711473465
Loss at step 987: 0.024411439895629883
Loss at step 988: 0.013781256973743439
Loss at step 989: 0.053529150784015656
Loss at step 990: 0.09938395768404007
Loss at step 991: 0.19422657787799835
Loss at step 992: 0.27333083748817444
Loss at step 993: 0.2637408673763275
Loss at step 994: 0.24259275197982788
Loss at step 995: 0.06930501759052277
Loss at step 996: 0.0793825089931488
Loss at step 997: 0.07954046130180359
Loss at step 998: 0.15744219720363617
Loss at step 999: 0.4103243052959442
Saving training state...
[2025-08-01 17:49:25,739] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 17:49:32,174] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1000: 0.20853246748447418
Loss at step 1001: 0.11646426469087601
Loss at step 1002: 0.6159995794296265
Loss at step 1003: 0.2666838467121124
Loss at step 1004: 0.08118776977062225
Loss at step 1005: 0.00990231242030859
Loss at step 1006: 0.45774224400520325
Loss at step 1007: 0.03918487951159477
Loss at step 1008: 0.2022816687822342
Loss at step 1009: 0.13186536729335785
Loss at step 1010: 0.08125525712966919
Loss at step 1011: 0.15088848769664764
Loss at step 1012: 0.09868869930505753
Loss at step 1013: 0.015371745452284813
Loss at step 1014: 0.3129807412624359
Loss at step 1015: 0.10045268386602402
Loss at step 1016: 0.068925641477108
Loss at step 1017: 0.04469676315784454
Loss at step 1018: 0.1240258663892746
Loss at step 1019: 0.009333190508186817
Loss at step 1020: 0.29263997077941895
Loss at step 1021: 0.3356695771217346
Loss at step 1022: 0.10533677041530609
Loss at step 1023: 0.1600397229194641
Loss at step 1024: 0.040752679109573364
Loss at step 1025: 0.06356598436832428
Loss at step 1026: 0.2284814566373825
Loss at step 1027: 0.09132307767868042
Loss at step 1028: 0.14654068648815155
Loss at step 1029: 0.40863552689552307
Loss at step 1030: 0.060238853096961975
Loss at step 1031: 0.4555732309818268
Loss at step 1032: 0.05889464169740677
Loss at step 1033: 0.17701247334480286
Loss at step 1034: 0.2945414185523987
Loss at step 1035: 0.32530710101127625
Loss at step 1036: 0.016951970756053925
Loss at step 1037: 0.16757462918758392
Loss at step 1038: 0.05317186191678047
Loss at step 1039: 0.035546280443668365
Loss at step 1040: 0.17634719610214233
Loss at step 1041: 0.0733383521437645
Loss at step 1042: 0.35195738077163696
Loss at step 1043: 0.07179417461156845
Loss at step 1044: 0.1715398132801056
Loss at step 1045: 0.11618615686893463
Loss at step 1046: 0.4336448907852173
Loss at step 1047: 0.1064717024564743
Loss at step 1048: 0.15328477323055267
Loss at step 1049: 0.059477902948856354
Loss at step 1050: 0.12188272178173065
Loss at step 1051: 0.020476941019296646
Loss at step 1052: 0.30194342136383057
Loss at step 1053: 0.07753460109233856
Loss at step 1054: 0.008767757564783096
Loss at step 1055: 0.3382734954357147
Loss at step 1056: 0.08965059369802475
Loss at step 1057: 0.12842367589473724
Loss at step 1058: 0.03674159571528435
Loss at step 1059: 0.12756891548633575
Loss at step 1060: 0.19425860047340393
Loss at step 1061: 0.4331950545310974
Loss at step 1062: 0.027738191187381744
Loss at step 1063: 0.12063638865947723
Loss at step 1064: 0.07408193498849869
Loss at step 1065: 0.1825174242258072
Loss at step 1066: 0.06764821708202362
Loss at step 1067: 0.19006356596946716
Loss at step 1068: 0.07042700052261353
Loss at step 1069: 0.15945710241794586
Loss at step 1070: 0.16570690274238586
Loss at step 1071: 0.23898671567440033
Loss at step 1072: 0.23269358277320862
Loss at step 1073: 0.11419455707073212
Loss at step 1074: 0.03377750888466835
Loss at step 1075: 0.05549994111061096
Loss at step 1076: 0.08634747564792633
Loss at step 1077: 0.058110639452934265
Loss at step 1078: 0.10322873294353485
Loss at step 1079: 0.09103664010763168
Loss at step 1080: 0.043341558426618576
Loss at step 1081: 0.20711971819400787
Loss at step 1082: 0.2124539017677307
Loss at step 1083: 0.02907674014568329
Loss at step 1084: 0.12320839613676071
Loss at step 1085: 0.05346336215734482
Loss at step 1086: 0.20971544086933136
Loss at step 1087: 0.14988115429878235
Loss at step 1088: 0.46210533380508423
Loss at step 1089: 0.14878353476524353
Loss at step 1090: 0.1557634472846985
Loss at step 1091: 0.00419726874679327
Loss at step 1092: 0.11859573423862457
Loss at step 1093: 0.061402592808008194
Loss at step 1094: 0.09750786423683167
Loss at step 1095: 0.18117734789848328
Loss at step 1096: 0.03813612088561058
Loss at step 1097: 0.09331236034631729
Loss at step 1098: 0.22336825728416443
Loss at step 1099: 0.05265432223677635
Loss at step 1100: 0.12845447659492493
Loss at step 1101: 0.09891240298748016
Loss at step 1102: 0.05842055752873421
Loss at step 1103: 0.06774935871362686
Loss at step 1104: 0.11234772950410843
Loss at step 1105: 0.09526890516281128
Loss at step 1106: 0.14695391058921814
Loss at step 1107: 0.014932384714484215
Loss at step 1108: 0.5598741173744202
Loss at step 1109: 0.04281717538833618
Loss at step 1110: 0.11032943427562714
Loss at step 1111: 0.14209870994091034
Loss at step 1112: 0.21453391015529633
Loss at step 1113: 0.22421671450138092
Loss at step 1114: 0.07472791522741318
Loss at step 1115: 0.1580173820257187
Loss at step 1116: 0.08974124491214752
Loss at step 1117: 0.0625019446015358
Loss at step 1118: 0.1833503097295761
Loss at step 1119: 0.3094951808452606
Loss at step 1120: 0.02577430196106434
Loss at step 1121: 0.05525846406817436
Loss at step 1122: 0.04610975459218025
Loss at step 1123: 0.04579053074121475
Loss at step 1124: 0.13986217975616455
Loss at step 1125: 0.016251852735877037
Loss at step 1126: 0.037274546921253204
Loss at step 1127: 0.24890151619911194
Loss at step 1128: 0.32143574953079224
Loss at step 1129: 0.023558802902698517
Loss at step 1130: 0.29250648617744446
Loss at step 1131: 0.07205085456371307
Loss at step 1132: 0.0
Loss at step 1133: 0.13642138242721558
Loss at step 1134: 0.22158917784690857
Loss at step 1135: 0.029968278482556343
Loss at step 1136: 0.070877805352211
Loss at step 1137: 0.10734713077545166
Loss at step 1138: 0.3169860541820526
Loss at step 1139: 0.2554810047149658
Loss at step 1140: 0.029201967641711235
Loss at step 1141: 0.046916522085666656
Loss at step 1142: 0.0
Loss at step 1143: 0.05316951870918274
Loss at step 1144: 0.11507221311330795
Loss at step 1145: 0.21732032299041748
Loss at step 1146: 0.026518629863858223
Loss at step 1147: 0.19274024665355682
Loss at step 1148: 0.021025849506258965
Loss at step 1149: 0.013799511827528477
Loss at step 1150: 0.3169657289981842
Loss at step 1151: 0.03651801496744156
Loss at step 1152: 0.32086312770843506
Loss at step 1153: 0.24524414539337158
Loss at step 1154: 0.07766377180814743
Loss at step 1155: 0.0205245241522789
Loss at step 1156: 0.022959943860769272
Loss at step 1157: 0.3633967339992523
Loss at step 1158: 0.052369624376297
Loss at step 1159: 0.32763051986694336
Loss at step 1160: 0.5474111437797546
Loss at step 1161: 0.22461390495300293
Loss at step 1162: 0.1230374202132225
Loss at step 1163: 0.03192390874028206
Loss at step 1164: 0.05948764085769653
Loss at step 1165: 0.03571665287017822
Loss at step 1166: 0.05005224049091339
Loss at step 1167: 0.04780592769384384
Loss at step 1168: 0.08772162348031998
Loss at step 1169: 0.039200667291879654
Loss at step 1170: 0.011855033226311207
Loss at step 1171: 0.4431041479110718
Loss at step 1172: 0.034365005791187286
Loss at step 1173: 0.40920260548591614
Loss at step 1174: 0.10180546343326569
Loss at step 1175: 0.3436673879623413
Loss at step 1176: 0.09874053299427032
Loss at step 1177: 0.490304559469223
Loss at step 1178: 0.029463347047567368
Loss at step 1179: 0.027697287499904633
Loss at step 1180: 0.1779385507106781
Loss at step 1181: 0.013671460561454296
Loss at step 1182: 0.11434776335954666
Loss at step 1183: 0.13905048370361328
Loss at step 1184: 0.32803648710250854
Loss at step 1185: 0.014227835461497307
Loss at step 1186: 0.1398058831691742
Loss at step 1187: 0.13725176453590393
Loss at step 1188: 0.031700633466243744
Loss at step 1189: 0.03612329438328743
Loss at step 1190: 0.16945083439350128
Loss at step 1191: 0.09527783840894699
Loss at step 1192: 0.09688775986433029
Loss at step 1193: 0.008617413230240345
Loss at step 1194: 0.34517142176628113
Loss at step 1195: 0.029910234734416008
Loss at step 1196: 0.17190712690353394
Loss at step 1197: 0.1758139282464981
Loss at step 1198: 0.06634355336427689
Loss at step 1199: 0.12635652720928192
Saving training state...
[2025-08-01 18:59:42,204] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 18:59:48,619] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1200: 0.03237779438495636
Loss at step 1201: 0.1359591782093048
Loss at step 1202: 0.17484518885612488
Loss at step 1203: 0.07218212634325027
Loss at step 1204: 0.037601206451654434
Loss at step 1205: 0.12095443904399872
Loss at step 1206: 0.07783886045217514
Loss at step 1207: 0.014527407474815845
Loss at step 1208: 0.06766808778047562
Loss at step 1209: 0.10827593505382538
Loss at step 1210: 0.09666026383638382
Loss at step 1211: 0.11688578873872757
Loss at step 1212: 0.1268264800310135
Loss at step 1213: 0.05422959849238396
Loss at step 1214: 0.045505162328481674
Loss at step 1215: 0.25317198038101196
Loss at step 1216: 0.3279443085193634
Loss at step 1217: 0.09866870939731598
Loss at step 1218: 0.1006445437669754
Loss at step 1219: 0.2181735634803772
Loss at step 1220: 0.05790742486715317
Loss at step 1221: 0.0941605269908905
Loss at step 1222: 0.11842700093984604
Loss at step 1223: 0.07328517735004425
Loss at step 1224: 0.07668492197990417
Loss at step 1225: 0.13515694439411163
Loss at step 1226: 0.3086373209953308
Loss at step 1227: 0.046853918582201004
Loss at step 1228: 0.2046191543340683
Loss at step 1229: 0.027485884726047516
Loss at step 1230: 0.11735749244689941
Loss at step 1231: 0.012910418212413788
Loss at step 1232: 0.03255350515246391
Loss at step 1233: 0.038007233291864395
Loss at step 1234: 0.1567143201828003
Loss at step 1235: 0.10474658757448196
Loss at step 1236: 0.2708052098751068
Loss at step 1237: 0.16574949026107788
Loss at step 1238: 0.5064979195594788
Loss at step 1239: 0.1466444879770279
Loss at step 1240: 0.14947755634784698
Loss at step 1241: 0.06535952538251877
Loss at step 1242: 0.2796115279197693
Loss at step 1243: 0.05061686411499977
Loss at step 1244: 0.390603244304657
Loss at step 1245: 0.09202419221401215
Loss at step 1246: 0.007460788823664188
Loss at step 1247: 0.008088244125247002
Loss at step 1248: 0.29429998993873596
Loss at step 1249: 0.22787614166736603
Loss at step 1250: 0.1648469716310501
Loss at step 1251: 0.17539480328559875
Loss at step 1252: 0.028454234823584557
Loss at step 1253: 0.10646539181470871
Loss at step 1254: 0.22387778759002686
Loss at step 1255: 0.11180207878351212
Loss at step 1256: 0.11949155479669571
Loss at step 1257: 0.2359345257282257
Loss at step 1258: 0.21063105762004852
Loss at step 1259: 0.0432237908244133
Loss at step 1260: 0.17278802394866943
Loss at step 1261: 0.23959285020828247
Loss at step 1262: 0.07473728060722351
Loss at step 1263: 0.09127593040466309
Loss at step 1264: 0.025456007570028305
Loss at step 1265: 0.03862806782126427
Loss at step 1266: 0.08416759222745895
Loss at step 1267: 0.0266508050262928
Loss at step 1268: 0.02765960432589054
Loss at step 1269: 0.38064485788345337
Loss at step 1270: 0.16282334923744202
Loss at step 1271: 0.12023422122001648
Loss at step 1272: 0.03319300711154938
Loss at step 1273: 0.23402786254882812
Loss at step 1274: 0.473397821187973
Loss at step 1275: 0.03526298701763153
Loss at step 1276: 0.0689752846956253
Loss at step 1277: 0.06690117716789246
Loss at step 1278: 0.09534090757369995
Loss at step 1279: 0.0
Loss at step 1280: 0.09665805846452713
Loss at step 1281: 0.2749032974243164
Loss at step 1282: 0.0326048769056797
Loss at step 1283: 0.21422810852527618
Loss at step 1284: 0.21228794753551483
Loss at step 1285: 0.1576424241065979
Loss at step 1286: 0.0996280387043953
Loss at step 1287: 0.11554726958274841
Loss at step 1288: 0.015575875528156757
Loss at step 1289: 0.22987757623195648
Loss at step 1290: 0.0
Loss at step 1291: 0.23101578652858734
Loss at step 1292: 0.03742741420865059
Loss at step 1293: 0.3274462819099426
Loss at step 1294: 0.14270354807376862
Loss at step 1295: 0.24971041083335876
Loss at step 1296: 0.015157828107476234
Loss at step 1297: 0.06319878995418549
Loss at step 1298: 0.1651057004928589
Loss at step 1299: 0.19783227145671844
Loss at step 1300: 0.14663776755332947
Loss at step 1301: 0.26778388023376465
Loss at step 1302: 0.08427087217569351
Loss at step 1303: 0.10759291052818298
Loss at step 1304: 0.1321169137954712
Loss at step 1305: 0.0841531828045845
Loss at step 1306: 0.07218863070011139
Loss at step 1307: 0.09188645333051682
Loss at step 1308: 0.1153082326054573
Loss at step 1309: 0.009929943829774857
Loss at step 1310: 0.2340041548013687
Loss at step 1311: 0.05241812393069267
Loss at step 1312: 0.02159130573272705
Loss at step 1313: 0.07990937680006027
Loss at step 1314: 0.45062878727912903
Loss at step 1315: 0.017183158546686172
Loss at step 1316: 0.20815953612327576
Loss at step 1317: 0.16634829342365265
Loss at step 1318: 0.09052164852619171
Loss at step 1319: 0.19415701925754547
Loss at step 1320: 0.3667319416999817
Loss at step 1321: 0.2097965031862259
Loss at step 1322: 0.018278328701853752
Loss at step 1323: 0.03935378044843674
Loss at step 1324: 0.09380406141281128
Loss at step 1325: 0.03600693494081497
Loss at step 1326: 0.01795232482254505
Loss at step 1327: 0.04258282855153084
Loss at step 1328: 0.053752802312374115
Loss at step 1329: 0.09993554651737213
Loss at step 1330: 0.11753562092781067
Loss at step 1331: 0.042465463280677795
Loss at step 1332: 0.2535478472709656
Loss at step 1333: 0.17050884664058685
Loss at step 1334: 0.03581736609339714
Loss at step 1335: 0.29813459515571594
Loss at step 1336: 0.04493027180433273
Loss at step 1337: 0.17378127574920654
Loss at step 1338: 0.07376862317323685
Loss at step 1339: 0.08401572704315186
Loss at step 1340: 0.0
Loss at step 1341: 0.16678889095783234
Loss at step 1342: 0.26831918954849243
Loss at step 1343: 0.06423357129096985
Loss at step 1344: 0.12020470947027206
Loss at step 1345: 0.06737793236970901
Loss at step 1346: 0.03291532024741173
Loss at step 1347: 0.02042652666568756
Loss at step 1348: 0.01533367671072483
Loss at step 1349: 0.02031243033707142
Loss at step 1350: 0.1083255484700203
Loss at step 1351: 0.026117974892258644
Loss at step 1352: 0.07106825709342957
Loss at step 1353: 0.07262875139713287
Loss at step 1354: 0.06563305109739304
Loss at step 1355: 0.04617536813020706
Loss at step 1356: 0.01416870765388012
Loss at step 1357: 0.08913910388946533
Loss at step 1358: 0.23002077639102936
Loss at step 1359: 0.09092644602060318
Loss at step 1360: 0.019265834242105484
Loss at step 1361: 0.03141805902123451
Loss at step 1362: 0.28626105189323425
Loss at step 1363: 0.10031499713659286
Loss at step 1364: 0.15932683646678925
Loss at step 1365: 0.07437373697757721
Loss at step 1366: 0.15344451367855072
Loss at step 1367: 0.06757121533155441
Loss at step 1368: 0.34464675188064575
Loss at step 1369: 0.10753078013658524
Loss at step 1370: 0.0596545971930027
Loss at step 1371: 0.15215003490447998
Loss at step 1372: 0.0
Loss at step 1373: 0.23945507407188416
Loss at step 1374: 0.2496824562549591
Loss at step 1375: 0.07589828968048096
Loss at step 1376: 0.07624015212059021
Loss at step 1377: 0.04059673100709915
Loss at step 1378: 0.1835593283176422
Loss at step 1379: 0.19717533886432648
Loss at step 1380: 0.3082840144634247
Loss at step 1381: 0.29839026927948
Loss at step 1382: 0.019796336069703102
Loss at step 1383: 0.45594727993011475
Loss at step 1384: 0.04003041610121727
Loss at step 1385: 0.14213517308235168
Loss at step 1386: 0.10402387380599976
Loss at step 1387: 0.08923624455928802
Loss at step 1388: 0.2615702450275421
Loss at step 1389: 0.18080419301986694
Loss at step 1390: 0.08767254650592804
Loss at step 1391: 0.25068098306655884
Loss at step 1392: 0.06642301380634308
Loss at step 1393: 0.16354411840438843
Loss at step 1394: 0.0788513720035553
Loss at step 1395: 0.20768555998802185
Loss at step 1396: 0.0944690853357315
Loss at step 1397: 0.12011159956455231
Loss at step 1398: 0.008849740028381348
Loss at step 1399: 0.055783044546842575
Saving training state...
[2025-08-01 20:10:36,832] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 20:10:43,358] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1400: 0.2689572870731354
Loss at step 1401: 0.04228126257658005
Loss at step 1402: 0.37864574790000916
Loss at step 1403: 0.27511388063430786
Loss at step 1404: 0.09701605141162872
Loss at step 1405: 0.35074588656425476
Loss at step 1406: 0.062027957290410995
Loss at step 1407: 0.03532029688358307
Loss at step 1408: 0.31457242369651794
Loss at step 1409: 0.07165871560573578
Loss at step 1410: 0.09415638446807861
Loss at step 1411: 0.40497568249702454
Loss at step 1412: 0.028344281017780304
Loss at step 1413: 0.11555174738168716
Loss at step 1414: 0.06239750608801842
Loss at step 1415: 0.09383933246135712
Loss at step 1416: 0.0
Loss at step 1417: 0.0
Loss at step 1418: 0.10718943178653717
Loss at step 1419: 0.14347873628139496
Loss at step 1420: 0.08246219158172607
Loss at step 1421: 0.09681828320026398
Loss at step 1422: 0.2817257046699524
Loss at step 1423: 0.12364889681339264
Loss at step 1424: 0.12327892333269119
Loss at step 1425: 0.03680577129125595
Loss at step 1426: 0.44941577315330505
Loss at step 1427: 0.14564254879951477
Loss at step 1428: 0.13408076763153076
Loss at step 1429: 0.1343800574541092
Loss at step 1430: 0.10167630761861801
Loss at step 1431: 0.21327731013298035
Loss at step 1432: 0.10201990604400635
Loss at step 1433: 0.0324050597846508
Loss at step 1434: 0.05455075949430466
Loss at step 1435: 0.1840531975030899
Loss at step 1436: 0.08476647734642029
Loss at step 1437: 0.03361045941710472
Loss at step 1438: 0.01972506381571293
Loss at step 1439: 0.06730645895004272
Loss at step 1440: 0.042028140276670456
Loss at step 1441: 0.030734192579984665
Loss at step 1442: 0.06964635848999023
Loss at step 1443: 0.0
Loss at step 1444: 0.06693318486213684
Loss at step 1445: 0.28781914710998535
Loss at step 1446: 0.02613270841538906
Loss at step 1447: 0.12422237545251846
Loss at step 1448: 0.059425465762615204
Loss at step 1449: 0.07396591454744339
Loss at step 1450: 0.02251177467405796
Loss at step 1451: 0.041762575507164
Loss at step 1452: 0.02514582872390747
Loss at step 1453: 0.06840205192565918
Loss at step 1454: 0.3627775013446808
Loss at step 1455: 0.14377361536026
Loss at step 1456: 0.05263744667172432
Loss at step 1457: 0.12291207909584045
Loss at step 1458: 0.07659634947776794
Loss at step 1459: 0.02945888787508011
Loss at step 1460: 0.3038598299026489
Loss at step 1461: 0.06558790057897568
Loss at step 1462: 0.049915194511413574
Loss at step 1463: 0.056468766182661057
Loss at step 1464: 0.028627971187233925
Loss at step 1465: 0.1255132257938385
Loss at step 1466: 0.10069690644741058
Loss at step 1467: 0.14137430489063263
Loss at step 1468: 0.08515870571136475
Loss at step 1469: 0.35041844844818115
Loss at step 1470: 0.01293953787535429
Loss at step 1471: 0.02739866077899933
Loss at step 1472: 0.2925282418727875
Loss at step 1473: 0.060649558901786804
Loss at step 1474: 0.17701691389083862
Loss at step 1475: 0.05784885957837105
Loss at step 1476: 0.06041264906525612
Loss at step 1477: 0.10050450265407562
Loss at step 1478: 0.07305581867694855
Loss at step 1479: 0.1327369064092636
Loss at step 1480: 0.2614629864692688
Loss at step 1481: 0.34878647327423096
Loss at step 1482: 0.10210829228162766
Loss at step 1483: 0.025000976398587227
Loss at step 1484: 0.1334470808506012
Loss at step 1485: 0.22863911092281342
Loss at step 1486: 0.11520769447088242
Loss at step 1487: 0.04218516871333122
Loss at step 1488: 0.2336631864309311
Loss at step 1489: 0.05669146031141281
Loss at step 1490: 0.17207752168178558
Loss at step 1491: 0.24037782847881317
Loss at step 1492: 0.02823862060904503
Loss at step 1493: 0.011815067380666733
Loss at step 1494: 0.1672082543373108
Loss at step 1495: 0.028944188728928566
Loss at step 1496: 0.05898657068610191
Loss at step 1497: 0.02494760788977146
Loss at step 1498: 0.22829897701740265
Loss at step 1499: 0.1895180195569992
Loss at step 1500: 0.19695807993412018
Loss at step 1501: 0.19049525260925293
Loss at step 1502: 0.18638931214809418
Loss at step 1503: 0.06655058264732361
Loss at step 1504: 0.05936022475361824
Loss at step 1505: 0.10375712811946869
Loss at step 1506: 0.3343081772327423
Loss at step 1507: 0.027949022129178047
Loss at step 1508: 0.058672644197940826
Loss at step 1509: 0.10222766548395157
Loss at step 1510: 0.004196075722575188
Loss at step 1511: 0.19897496700286865
Loss at step 1512: 0.0
Loss at step 1513: 0.04445567727088928
Loss at step 1514: 0.017117656767368317
Loss at step 1515: 0.10385873168706894
Loss at step 1516: 0.02846642956137657
Loss at step 1517: 0.11666500568389893
Loss at step 1518: 0.07112124562263489
Loss at step 1519: 0.06380098313093185
Loss at step 1520: 0.034112896770238876
Loss at step 1521: 0.02552058733999729
Loss at step 1522: 0.1274465173482895
Loss at step 1523: 0.08887659013271332
Loss at step 1524: 0.24831624329090118
Loss at step 1525: 0.016728002578020096
Loss at step 1526: 0.20040158927440643
Loss at step 1527: 0.13917306065559387
Loss at step 1528: 0.06538115441799164
Loss at step 1529: 0.1709437072277069
Loss at step 1530: 0.03157235309481621
Loss at step 1531: 0.10085524618625641
Loss at step 1532: 0.043340958654880524
Loss at step 1533: 0.08690574020147324
Loss at step 1534: 0.04332085698843002
Loss at step 1535: 0.09718753397464752
Loss at step 1536: 0.08106883615255356
Loss at step 1537: 0.10140766203403473
Loss at step 1538: 0.13279415667057037
Loss at step 1539: 0.11407023668289185
Loss at step 1540: 0.1455397754907608
Loss at step 1541: 0.04480371251702309
Loss at step 1542: 0.16608156263828278
Loss at step 1543: 0.09883478283882141
Loss at step 1544: 0.3979264497756958
Loss at step 1545: 0.2138889729976654
Loss at step 1546: 0.24841846525669098
Loss at step 1547: 0.6674854159355164
Loss at step 1548: 0.02085700072348118
Loss at step 1549: 0.35014447569847107
Loss at step 1550: 0.07486356794834137
Loss at step 1551: 0.4062517583370209
Loss at step 1552: 0.23865878582000732
Loss at step 1553: 0.3490419089794159
Loss at step 1554: 0.09975416958332062
Loss at step 1555: 0.016788188368082047
Loss at step 1556: 0.032450638711452484
Loss at step 1557: 0.08200257271528244
Loss at step 1558: 0.17250068485736847
Loss at step 1559: 0.10985890030860901
Loss at step 1560: 0.013569646514952183
Loss at step 1561: 0.14805039763450623
Loss at step 1562: 0.05366867408156395
Loss at step 1563: 0.037172168493270874
Loss at step 1564: 0.08318951725959778
Loss at step 1565: 0.03468926250934601
Loss at step 1566: 0.41504713892936707
Loss at step 1567: 0.04144543036818504
Loss at step 1568: 0.0556037500500679
Loss at step 1569: 0.37795183062553406
Loss at step 1570: 0.07336454838514328
Loss at step 1571: 0.23554788529872894
Loss at step 1572: 0.3193672001361847
Loss at step 1573: 0.0957302376627922
Loss at step 1574: 0.30286523699760437
Loss at step 1575: 0.026738246902823448
Loss at step 1576: 0.029785113409161568
Loss at step 1577: 0.11726884543895721
Loss at step 1578: 0.09858938306570053
Loss at step 1579: 0.07069400697946548
Loss at step 1580: 0.043122947216033936
Loss at step 1581: 0.12384351342916489
Loss at step 1582: 0.08728203177452087
Loss at step 1583: 0.02681364119052887
Loss at step 1584: 0.09369146078824997
Loss at step 1585: 0.006233224179595709
Loss at step 1586: 0.09809362888336182
Loss at step 1587: 0.3648871183395386
Loss at step 1588: 0.08710387349128723
Loss at step 1589: 0.05121526122093201
Loss at step 1590: 0.15842007100582123
Loss at step 1591: 0.3295900821685791
Loss at step 1592: 0.16482223570346832
Loss at step 1593: 0.22644242644309998
Loss at step 1594: 0.02593943104147911
Loss at step 1595: 0.13157354295253754
Loss at step 1596: 0.07984663546085358
Loss at step 1597: 0.24313746392726898
Loss at step 1598: 0.25284257531166077
Loss at step 1599: 0.03073769249022007
Saving training state...
[2025-08-01 21:21:28,137] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 21:21:34,583] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1600: 0.13540205359458923
Loss at step 1601: 0.21186751127243042
Loss at step 1602: 0.05879804864525795
Loss at step 1603: 0.09125439822673798
Loss at step 1604: 0.12304158508777618
Loss at step 1605: 0.04150718078017235
Loss at step 1606: 0.08321811258792877
Loss at step 1607: 0.3126155734062195
Loss at step 1608: 0.04149431735277176
Loss at step 1609: 0.05044296383857727
Loss at step 1610: 0.20157618820667267
Loss at step 1611: 0.09236405789852142
Loss at step 1612: 0.056421514600515366
Loss at step 1613: 0.00853822287172079
Loss at step 1614: 0.01432714331895113
Loss at step 1615: 0.20253098011016846
Loss at step 1616: 0.16592209041118622
Loss at step 1617: 0.030967330560088158
Loss at step 1618: 0.16890504956245422
Loss at step 1619: 0.26880475878715515
Loss at step 1620: 0.14132848381996155
Loss at step 1621: 0.01609938219189644
Loss at step 1622: 0.11529014259576797
Loss at step 1623: 0.044653456658124924
Loss at step 1624: 0.10768478363752365
Loss at step 1625: 0.418735533952713
Loss at step 1626: 0.15464265644550323
Loss at step 1627: 0.013263016939163208
Loss at step 1628: 0.04840093478560448
Loss at step 1629: 0.07356218993663788
Loss at step 1630: 0.06259654462337494
Loss at step 1631: 0.21680162847042084
Loss at step 1632: 0.16809934377670288
Loss at step 1633: 0.16314256191253662
Loss at step 1634: 0.2125043123960495
Loss at step 1635: 0.11775495111942291
Loss at step 1636: 0.21144072711467743
Loss at step 1637: 0.11788376420736313
Loss at step 1638: 0.16426819562911987
Loss at step 1639: 0.2162354439496994
Loss at step 1640: 0.16884931921958923
Loss at step 1641: 0.2928621172904968
Loss at step 1642: 0.11790690571069717
Loss at step 1643: 0.19777657091617584
Loss at step 1644: 0.16125324368476868
Loss at step 1645: 0.4067235589027405
Loss at step 1646: 0.08613264560699463
Loss at step 1647: 0.09474848955869675
Loss at step 1648: 0.10593080520629883
Loss at step 1649: 0.15026690065860748
Loss at step 1650: 0.13996444642543793
Loss at step 1651: 0.15269939601421356
Loss at step 1652: 0.14387014508247375
Loss at step 1653: 0.3837437629699707
Loss at step 1654: 0.13686197996139526
Loss at step 1655: 0.2212137132883072
Loss at step 1656: 0.16130760312080383
Loss at step 1657: 0.022207247093319893
Loss at step 1658: 0.06681182235479355
Loss at step 1659: 0.04886738955974579
Loss at step 1660: 0.017198853194713593
Loss at step 1661: 0.22656254470348358
Loss at step 1662: 0.12665030360221863
Loss at step 1663: 0.1320056915283203
Loss at step 1664: 0.10440455377101898
Loss at step 1665: 0.2637822926044464
Loss at step 1666: 0.15819044411182404
Loss at step 1667: 0.13176831603050232
Loss at step 1668: 0.039785467088222504
Loss at step 1669: 0.042516112327575684
Loss at step 1670: 0.075080506503582
Loss at step 1671: 0.10499797016382217
Loss at step 1672: 0.10284263640642166
Loss at step 1673: 0.19776558876037598
Loss at step 1674: 0.03566000983119011
Loss at step 1675: 0.3924594521522522
Loss at step 1676: 0.05372888222336769
Loss at step 1677: 0.0525038056075573
Loss at step 1678: 0.04206259921193123
Loss at step 1679: 0.09603574126958847
Loss at step 1680: 0.019018538296222687
Loss at step 1681: 0.3023473620414734
Loss at step 1682: 0.14779391884803772
Loss at step 1683: 0.0973898321390152
Loss at step 1684: 0.05157030001282692
Loss at step 1685: 0.027807267382740974
Loss at step 1686: 0.15279924869537354
Loss at step 1687: 0.0046073454432189465
Loss at step 1688: 0.0997200757265091
Loss at step 1689: 0.0
Loss at step 1690: 0.04099617898464203
Loss at step 1691: 0.046615954488515854
Loss at step 1692: 0.06238184869289398
Loss at step 1693: 0.1346992403268814
Loss at step 1694: 0.0063521103002130985
Loss at step 1695: 0.3561234176158905
Loss at step 1696: 0.030879134312272072
Loss at step 1697: 0.2455095648765564
Loss at step 1698: 0.10066627711057663
Loss at step 1699: 0.2601417601108551
Loss at step 1700: 0.0
Loss at step 1701: 0.183500736951828
Loss at step 1702: 0.22482597827911377
Loss at step 1703: 0.4047757387161255
Loss at step 1704: 0.012985815294086933
Loss at step 1705: 0.18165355920791626
Loss at step 1706: 0.04603778198361397
Loss at step 1707: 0.03978642821311951
Loss at step 1708: 0.1459745615720749
Loss at step 1709: 0.12338729947805405
Loss at step 1710: 0.37605422735214233
Loss at step 1711: 0.12843559682369232
Loss at step 1712: 0.03186364844441414
Loss at step 1713: 0.060473307967185974
Loss at step 1714: 0.3714555501937866
Loss at step 1715: 0.08766579627990723
Loss at step 1716: 0.03457637131214142
Loss at step 1717: 0.3292601406574249
Loss at step 1718: 0.17972949147224426
Loss at step 1719: 0.19765815138816833
Loss at step 1720: 0.037994466722011566
Loss at step 1721: 0.09145829826593399
Loss at step 1722: 0.0651828944683075
Loss at step 1723: 0.4948343336582184
Loss at step 1724: 0.06126086413860321
Loss at step 1725: 0.3104473054409027
Loss at step 1726: 0.12018624693155289
Loss at step 1727: 0.16032983362674713
Loss at step 1728: 0.03848828375339508
Loss at step 1729: 0.08352580666542053
Loss at step 1730: 0.042444583028554916
Loss at step 1731: 0.05805494263768196
Loss at step 1732: 0.043822530657052994
Loss at step 1733: 0.25180113315582275
Loss at step 1734: 0.07089744508266449
Loss at step 1735: 0.08106004446744919
Loss at step 1736: 0.054309405386447906
Loss at step 1737: 0.06734247505664825
Loss at step 1738: 0.14582513272762299
Loss at step 1739: 0.6029462218284607
Loss at step 1740: 0.11904117465019226
Loss at step 1741: 0.14988696575164795
Loss at step 1742: 0.04942206293344498
Loss at step 1743: 0.24514128267765045
Loss at step 1744: 0.0906260535120964
Loss at step 1745: 0.044280536472797394
Loss at step 1746: 0.10033026337623596
Loss at step 1747: 0.09442532062530518
Loss at step 1748: 0.17755387723445892
Loss at step 1749: 0.10077977925539017
Loss at step 1750: 0.0705336257815361
Loss at step 1751: 0.07539555430412292
Loss at step 1752: 0.1916406750679016
Loss at step 1753: 0.08186574280261993
Loss at step 1754: 0.05603841319680214
Loss at step 1755: 0.19205595552921295
Loss at step 1756: 0.2770335376262665
Loss at step 1757: 0.040264468640089035
Loss at step 1758: 0.2680802047252655
Loss at step 1759: 0.22052595019340515
Loss at step 1760: 0.0
Loss at step 1761: 0.0955994725227356
Loss at step 1762: 0.2773950397968292
Loss at step 1763: 0.3755831718444824
Loss at step 1764: 0.3203273117542267
Loss at step 1765: 0.3021964132785797
Loss at step 1766: 0.11636893451213837
Loss at step 1767: 0.22194041311740875
Loss at step 1768: 0.30295366048812866
Loss at step 1769: 0.06259218603372574
Loss at step 1770: 0.06611331552267075
Loss at step 1771: 0.334844172000885
Loss at step 1772: 0.0
Loss at step 1773: 0.06868433952331543
Loss at step 1774: 0.496428519487381
Loss at step 1775: 0.38534852862358093
Loss at step 1776: 0.0660986602306366
Loss at step 1777: 0.23727600276470184
Loss at step 1778: 0.04161783307790756
Loss at step 1779: 0.15718744695186615
Loss at step 1780: 0.06607399135828018
Loss at step 1781: 0.1785539835691452
Loss at step 1782: 0.3077827990055084
Loss at step 1783: 0.1752483993768692
Loss at step 1784: 0.1181773692369461
Loss at step 1785: 0.1404547095298767
Loss at step 1786: 0.06600137054920197
Loss at step 1787: 0.019836219027638435
Loss at step 1788: 0.18289650976657867
Loss at step 1789: 0.07773515582084656
Loss at step 1790: 0.21399666368961334
Loss at step 1791: 0.050993725657463074
Loss at step 1792: 0.1637629121541977
Loss at step 1793: 0.23609276115894318
Loss at step 1794: 0.06776537001132965
Loss at step 1795: 0.09588958323001862
Loss at step 1796: 0.3067706227302551
Loss at step 1797: 0.02916898764669895
Loss at step 1798: 0.1622355878353119
Loss at step 1799: 0.3986174464225769
Saving training state...
[2025-08-01 22:31:27,121] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 22:31:33,756] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 1800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 1800: 0.10262826830148697
Loss at step 1801: 0.06616228073835373
Loss at step 1802: 0.16127343475818634
Loss at step 1803: 0.044770631939172745
Loss at step 1804: 0.14908188581466675
Loss at step 1805: 0.17775215208530426
Loss at step 1806: 0.4519856572151184
Loss at step 1807: 0.046705443412065506
Loss at step 1808: 0.08435144275426865
Loss at step 1809: 0.33005887269973755
Loss at step 1810: 0.0046487972140312195
Loss at step 1811: 0.0980234146118164
Loss at step 1812: 0.15901505947113037
Loss at step 1813: 0.1514730155467987
Loss at step 1814: 0.4034557640552521
Loss at step 1815: 0.019626257941126823
Loss at step 1816: 0.2640916407108307
Loss at step 1817: 0.25744178891181946
Loss at step 1818: 0.16420674324035645
Loss at step 1819: 0.05015178769826889
Loss at step 1820: 0.17929987609386444
Loss at step 1821: 0.044757455587387085
Loss at step 1822: 0.06609662622213364
Loss at step 1823: 0.26228728890419006
Loss at step 1824: 0.16796821355819702
Loss at step 1825: 0.07508796453475952
Loss at step 1826: 0.0972205251455307
Loss at step 1827: 0.06451873481273651
Loss at step 1828: 0.010451347567141056
Loss at step 1829: 0.17238663136959076
Loss at step 1830: 0.03268904611468315
Loss at step 1831: 0.5258039236068726
Loss at step 1832: 0.1247888058423996
Loss at step 1833: 0.13383351266384125
Loss at step 1834: 0.3286883234977722
Loss at step 1835: 0.10155975073575974
Loss at step 1836: 0.08261056989431381
Loss at step 1837: 0.09751830995082855
Loss at step 1838: 0.021291041746735573
Loss at step 1839: 0.014264635741710663
Loss at step 1840: 0.11242340505123138
Loss at step 1841: 0.04217648133635521
Loss at step 1842: 0.05249887332320213
Loss at step 1843: 0.21145085990428925
Loss at step 1844: 0.06916158646345139
Loss at step 1845: 0.09682688862085342
Loss at step 1846: 0.008406557142734528
Loss at step 1847: 0.04080747440457344
Loss at step 1848: 0.037057213485240936
Loss at step 1849: 0.14086177945137024
Loss at step 1850: 0.080206498503685
Loss at step 1851: 0.28715506196022034
Loss at step 1852: 0.045174021273851395
Loss at step 1853: 0.2708020508289337
Loss at step 1854: 0.05994804576039314
Loss at step 1855: 0.14011377096176147
Loss at step 1856: 0.14788706600666046
Loss at step 1857: 0.053983837366104126
Loss at step 1858: 0.10100508481264114
Loss at step 1859: 0.31985029578208923
Loss at step 1860: 0.08377369493246078
Loss at step 1861: 0.04868742823600769
Loss at step 1862: 0.3613872230052948
Loss at step 1863: 0.18002906441688538
Loss at step 1864: 0.20438207685947418
Loss at step 1865: 0.16031749546527863
Loss at step 1866: 0.28889963030815125
Loss at step 1867: 0.059978898614645004
Loss at step 1868: 0.16958843171596527
Loss at step 1869: 0.21213509142398834
Loss at step 1870: 0.11158120632171631
Loss at step 1871: 0.16132505238056183
Loss at step 1872: 0.12440802156925201
Loss at step 1873: 0.2796025574207306
Loss at step 1874: 0.17401139438152313
Loss at step 1875: 0.036969300359487534
Loss at step 1876: 0.14267891645431519
Loss at step 1877: 0.11206591129302979
Loss at step 1878: 0.044372182339429855
Loss at step 1879: 0.04863918572664261
Loss at step 1880: 0.12052977830171585
Loss at step 1881: 0.08055374026298523
Loss at step 1882: 0.3528068959712982
Loss at step 1883: 0.17031924426555634
Loss at step 1884: 0.16467426717281342
Loss at step 1885: 0.05681784451007843
Loss at step 1886: 0.1643897444009781
Loss at step 1887: 0.04031630605459213
Loss at step 1888: 0.22430996596813202
Loss at step 1889: 0.2409171164035797
Loss at step 1890: 0.01334549579769373
Loss at step 1891: 0.22453919053077698
Loss at step 1892: 0.04421475902199745
Loss at step 1893: 0.3269394636154175
Loss at step 1894: 0.3272640109062195
Loss at step 1895: 0.0675993263721466
Loss at step 1896: 0.028528079390525818
Loss at step 1897: 0.3094756007194519
Loss at step 1898: 0.3290565609931946
Loss at step 1899: 0.10493128001689911
Loss at step 1900: 0.016296129673719406
Loss at step 1901: 0.2827034294605255
Loss at step 1902: 0.285038024187088
Loss at step 1903: 0.028267640620470047
Loss at step 1904: 0.03427717834711075
Loss at step 1905: 0.0306546613574028
Loss at step 1906: 0.14831340312957764
Loss at step 1907: 0.04039566591382027
Loss at step 1908: 0.07314521819353104
Loss at step 1909: 0.10194510966539383
Loss at step 1910: 0.01629072055220604
Loss at step 1911: 0.1911032497882843
Loss at step 1912: 0.07528217881917953
Loss at step 1913: 0.010066470131278038
Loss at step 1914: 0.04876803606748581
Loss at step 1915: 0.02177482284605503
Loss at step 1916: 0.08051057904958725
Loss at step 1917: 0.07255053520202637
Loss at step 1918: 0.1500997394323349
Loss at step 1919: 0.3993380665779114
Loss at step 1920: 0.22486265003681183
Loss at step 1921: 0.05456113815307617
Loss at step 1922: 0.007304950151592493
Loss at step 1923: 0.3417363166809082
Loss at step 1924: 0.04737270250916481
Loss at step 1925: 0.3817206621170044
Loss at step 1926: 0.04910815507173538
Loss at step 1927: 0.118675597012043
Loss at step 1928: 0.14684662222862244
Loss at step 1929: 0.10532288998365402
Loss at step 1930: 0.0323229618370533
Loss at step 1931: 0.04641060531139374
Loss at step 1932: 0.14917591214179993
Loss at step 1933: 0.021424707025289536
Loss at step 1934: 0.12599673867225647
Loss at step 1935: 0.18790818750858307
Loss at step 1936: 0.06482269614934921
Loss at step 1937: 0.11027546972036362
Loss at step 1938: 0.409916490316391
Loss at step 1939: 0.154844731092453
Loss at step 1940: 0.027602041140198708
Loss at step 1941: 0.07792973518371582
Loss at step 1942: 0.04914434254169464
Loss at step 1943: 0.26443448662757874
Loss at step 1944: 0.08555443584918976
Loss at step 1945: 0.21136610209941864
Loss at step 1946: 0.1455785632133484
Loss at step 1947: 0.017079073935747147
Loss at step 1948: 0.09842004626989365
Loss at step 1949: 0.09384603798389435
Loss at step 1950: 0.08138230443000793
Loss at step 1951: 0.008483484387397766
Loss at step 1952: 0.0526881068944931
Loss at step 1953: 0.04213352128863335
Loss at step 1954: 0.23150677978992462
Loss at step 1955: 0.14524567127227783
Loss at step 1956: 0.0
Loss at step 1957: 0.3622587025165558
Loss at step 1958: 0.07252006232738495
Loss at step 1959: 0.29805874824523926
Loss at step 1960: 0.13711708784103394
Loss at step 1961: 0.47862908244132996
Loss at step 1962: 0.023995526134967804
Loss at step 1963: 0.1827530860900879
Loss at step 1964: 0.3064787983894348
Loss at step 1965: 0.13884501159191132
Loss at step 1966: 0.15334253013134003
Loss at step 1967: 0.0814158022403717
Loss at step 1968: 0.08846466988325119
Loss at step 1969: 0.12807150185108185
Loss at step 1970: 0.059692081063985825
Loss at step 1971: 0.2630949020385742
Loss at step 1972: 0.2644293010234833
Loss at step 1973: 0.3815677762031555
Loss at step 1974: 0.04931090399622917
Loss at step 1975: 0.08522865176200867
Loss at step 1976: 0.34370651841163635
Loss at step 1977: 0.28982558846473694
Loss at step 1978: 0.06395301967859268
Loss at step 1979: 0.05510316416621208
Loss at step 1980: 0.3380826413631439
Loss at step 1981: 0.3060208857059479
Loss at step 1982: 0.0418146476149559
Loss at step 1983: 0.07903865724802017
Loss at step 1984: 0.3918951153755188
Loss at step 1985: 0.08390633761882782
Loss at step 1986: 0.06335360556840897
Loss at step 1987: 0.022822530940175056
Loss at step 1988: 0.10923416912555695
Loss at step 1989: 0.04496850445866585
Loss at step 1990: 0.3680844008922577
Loss at step 1991: 0.014569303952157497
Loss at step 1992: 0.6304699778556824
Loss at step 1993: 0.07100901007652283
Loss at step 1994: 0.013214276172220707
Loss at step 1995: 0.05652962997555733
Loss at step 1996: 0.051292188465595245
Loss at step 1997: 0.11400339007377625
Loss at step 1998: 0.1957760900259018
Loss at step 1999: 0.14567892253398895
Saving training state...
[2025-08-01 23:42:05,498] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-01 23:42:11,949] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 2000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 2000: 0.044610071927309036
Loss at step 2001: 0.07718246430158615
Loss at step 2002: 0.12240608781576157
Loss at step 2003: 0.17728962004184723
Loss at step 2004: 0.11007099598646164
Loss at step 2005: 0.03397882357239723
Loss at step 2006: 0.09800859540700912
Loss at step 2007: 0.11915089190006256
Loss at step 2008: 0.02634570002555847
Loss at step 2009: 0.140358567237854
Loss at step 2010: 0.03403662145137787
Loss at step 2011: 0.030904702842235565
Loss at step 2012: 0.1690838485956192
Loss at step 2013: 0.06793181598186493
Loss at step 2014: 0.318308025598526
Loss at step 2015: 0.06243792548775673
Loss at step 2016: 0.39210423827171326
Loss at step 2017: 0.20578350126743317
Loss at step 2018: 0.023595644161105156
Loss at step 2019: 0.014933355152606964
Loss at step 2020: 0.40020638704299927
Loss at step 2021: 0.2776913642883301
Loss at step 2022: 0.2478877156972885
Loss at step 2023: 0.18439339101314545
Loss at step 2024: 0.02998361736536026
Loss at step 2025: 0.04661385715007782
Loss at step 2026: 0.03518081083893776
Loss at step 2027: 0.016084711998701096
Loss at step 2028: 0.020688572898507118
Loss at step 2029: 0.1707184910774231
Loss at step 2030: 0.11248955130577087
Loss at step 2031: 0.4631045162677765
Loss at step 2032: 0.1264103353023529
Loss at step 2033: 0.07505104690790176
Loss at step 2034: 0.27504393458366394
Loss at step 2035: 0.019968081265687943
Loss at step 2036: 0.10653708875179291
Loss at step 2037: 0.11596652120351791
Loss at step 2038: 0.41957801580429077
Loss at step 2039: 0.3372805416584015
Loss at step 2040: 0.0983341708779335
Loss at step 2041: 0.1490652859210968
Loss at step 2042: 0.05063018202781677
Loss at step 2043: 0.3808886408805847
Loss at step 2044: 0.05274675041437149
Loss at step 2045: 0.30988696217536926
Loss at step 2046: 0.27148178219795227
Loss at step 2047: 0.28295576572418213
Loss at step 2048: 0.0444318912923336
Loss at step 2049: 0.045926887542009354
Loss at step 2050: 0.02011852152645588
Loss at step 2051: 0.01733282022178173
Loss at step 2052: 0.26154670119285583
Loss at step 2053: 0.5971416234970093
Loss at step 2054: 0.2643382251262665
Loss at step 2055: 0.05804629251360893
Loss at step 2056: 0.33576399087905884
Loss at step 2057: 0.04959235340356827
Loss at step 2058: 0.10768333077430725
Loss at step 2059: 0.11768527328968048
Loss at step 2060: 0.13943052291870117
Loss at step 2061: 0.22506076097488403
Loss at step 2062: 0.08028563857078552
Loss at step 2063: 0.14548182487487793
Loss at step 2064: 0.08853588253259659
Loss at step 2065: 0.04412015527486801
Loss at step 2066: 0.043762125074863434
Loss at step 2067: 0.25103095173835754
Loss at step 2068: 0.2626662850379944
Loss at step 2069: 0.06725695729255676
Loss at step 2070: 0.05149361863732338
Loss at step 2071: 0.02309596911072731
Loss at step 2072: 0.009519277140498161
Loss at step 2073: 0.05200430378317833
Loss at step 2074: 0.06317547708749771
Loss at step 2075: 0.22090500593185425
Loss at step 2076: 0.10418640077114105
Loss at step 2077: 0.054139815270900726
Loss at step 2078: 0.08557473868131638
Loss at step 2079: 0.08068618923425674
Loss at step 2080: 0.08274710178375244
Loss at step 2081: 0.05572390928864479
Loss at step 2082: 0.17239312827587128
Loss at step 2083: 0.15494981408119202
Loss at step 2084: 0.01575731299817562
Loss at step 2085: 0.060419172048568726
Loss at step 2086: 0.017149364575743675
Loss at step 2087: 0.09048981964588165
Loss at step 2088: 0.19171611964702606
Loss at step 2089: 0.02277933433651924
Loss at step 2090: 0.169935941696167
Loss at step 2091: 0.07263905555009842
Loss at step 2092: 0.036313410848379135
Loss at step 2093: 0.045306313782930374
Loss at step 2094: 0.02645573951303959
Loss at step 2095: 0.16529221832752228
Loss at step 2096: 0.18553298711776733
Loss at step 2097: 0.2297256588935852
Loss at step 2098: 0.007932307198643684
Loss at step 2099: 0.31998756527900696
Loss at step 2100: 0.06575413793325424
Loss at step 2101: 0.026010578498244286
Loss at step 2102: 0.04425785690546036
Loss at step 2103: 0.07285723090171814
Loss at step 2104: 0.14376825094223022
Loss at step 2105: 0.09256376326084137
Loss at step 2106: 0.08163692057132721
Loss at step 2107: 0.22334426641464233
Loss at step 2108: 0.05931355059146881
Loss at step 2109: 0.03382505849003792
Loss at step 2110: 0.3276888132095337
Loss at step 2111: 0.36505675315856934
Loss at step 2112: 0.011285368353128433
Loss at step 2113: 0.25954052805900574
Loss at step 2114: 0.13108687102794647
Loss at step 2115: 0.020983507856726646
Loss at step 2116: 0.02375900186598301
Loss at step 2117: 0.16607333719730377
Loss at step 2118: 0.301387220621109
Loss at step 2119: 0.267423152923584
Loss at step 2120: 0.1135694831609726
Loss at step 2121: 0.23805943131446838
Loss at step 2122: 0.051619045436382294
Loss at step 2123: 0.23270930349826813
Loss at step 2124: 0.1379462629556656
Loss at step 2125: 0.040368013083934784
Loss at step 2126: 0.17233659327030182
Loss at step 2127: 0.5751906037330627
Loss at step 2128: 0.13856396079063416
Loss at step 2129: 0.22388054430484772
Loss at step 2130: 0.3098343014717102
Loss at step 2131: 0.04446142166852951
Loss at step 2132: 0.32888147234916687
Loss at step 2133: 0.09261122345924377
Loss at step 2134: 0.06750743091106415
Loss at step 2135: 0.13479003310203552
Loss at step 2136: 0.08189237862825394
Loss at step 2137: 0.08356171101331711
Loss at step 2138: 0.13871416449546814
Loss at step 2139: 0.13377349078655243
Loss at step 2140: 0.1119096428155899
Loss at step 2141: 0.39883068203926086
Loss at step 2142: 0.3194693326950073
Loss at step 2143: 0.12905262410640717
Loss at step 2144: 0.2466515302658081
Loss at step 2145: 0.16928941011428833
Loss at step 2146: 0.046181634068489075
Loss at step 2147: 0.17227371037006378
Loss at step 2148: 0.35042569041252136
Loss at step 2149: 0.0821094885468483
Loss at step 2150: 0.05296434462070465
Loss at step 2151: 0.20778846740722656
Loss at step 2152: 0.022902444005012512
Loss at step 2153: 0.051344964653253555
Loss at step 2154: 0.004927109461277723
Loss at step 2155: 0.300878643989563
Loss at step 2156: 0.13487480580806732
Loss at step 2157: 0.1466764509677887
Loss at step 2158: 0.24256591498851776
Loss at step 2159: 0.08608435839414597
Loss at step 2160: 0.12739066779613495
Loss at step 2161: 0.010477766394615173
Loss at step 2162: 0.013576860539615154
Loss at step 2163: 0.120823934674263
Loss at step 2164: 0.14226314425468445
Loss at step 2165: 0.08133483678102493
Loss at step 2166: 0.24089659750461578
Loss at step 2167: 0.1585390567779541
Loss at step 2168: 0.09070009738206863
Loss at step 2169: 0.1815209537744522
Loss at step 2170: 0.026202211156487465
Loss at step 2171: 0.19667471945285797
Loss at step 2172: 0.17353256046772003
Loss at step 2173: 0.12779535353183746
Loss at step 2174: 0.05503734201192856
Loss at step 2175: 0.1817983239889145
Loss at step 2176: 0.13551509380340576
Loss at step 2177: 0.08755183219909668
Loss at step 2178: 0.010135882534086704
Loss at step 2179: 0.12354414910078049
Loss at step 2180: 0.3069222569465637
Loss at step 2181: 0.10359596461057663
Loss at step 2182: 0.30574527382850647
Loss at step 2183: 0.07545885443687439
Loss at step 2184: 0.054047293961048126
Loss at step 2185: 0.016288606449961662
Loss at step 2186: 0.14636947214603424
Loss at step 2187: 0.03279147669672966
Loss at step 2188: 0.18579643964767456
Loss at step 2189: 0.04370836913585663
Loss at step 2190: 0.07694670557975769
Loss at step 2191: 0.2748834490776062
Loss at step 2192: 0.020896578207612038
Loss at step 2193: 0.06538856774568558
Loss at step 2194: 0.10062021017074585
Loss at step 2195: 0.07451044768095016
Loss at step 2196: 0.24871942400932312
Loss at step 2197: 0.3408304750919342
Loss at step 2198: 0.016482587903738022
Loss at step 2199: 0.15513651072978973
Saving training state...
[2025-08-02 00:52:24,212] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 00:52:30,660] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 2200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 2200: 0.04481622576713562
Loss at step 2201: 0.05449043959379196
Loss at step 2202: 0.04205402731895447
Loss at step 2203: 0.07852310687303543
Loss at step 2204: 0.36147189140319824
Loss at step 2205: 0.03814619407057762
Loss at step 2206: 0.14131806790828705
Loss at step 2207: 0.038329776376485825
Loss at step 2208: 0.11430636048316956
Loss at step 2209: 0.026379575952887535
Loss at step 2210: 0.025492407381534576
Loss at step 2211: 0.09087259322404861
Loss at step 2212: 0.3095219135284424
Loss at step 2213: 0.05928264558315277
Loss at step 2214: 0.15144243836402893
Loss at step 2215: 0.026956601068377495
Loss at step 2216: 0.08056796342134476
Loss at step 2217: 0.019464068114757538
Loss at step 2218: 0.0666978508234024
Loss at step 2219: 0.1656811684370041
Loss at step 2220: 0.26397231221199036
Loss at step 2221: 0.2159242331981659
Loss at step 2222: 0.11227476596832275
Loss at step 2223: 0.08334005624055862
Loss at step 2224: 0.04579027369618416
Loss at step 2225: 0.2305452823638916
Loss at step 2226: 0.15582935512065887
Loss at step 2227: 0.38050103187561035
Loss at step 2228: 0.2408486306667328
Loss at step 2229: 0.06679977476596832
Loss at step 2230: 0.22011032700538635
Loss at step 2231: 0.0698716938495636
Loss at step 2232: 0.09632480144500732
Loss at step 2233: 0.03776609152555466
Loss at step 2234: 0.20284292101860046
Loss at step 2235: 0.02411816082894802
Loss at step 2236: 0.040140409022569656
Loss at step 2237: 0.3837970197200775
Loss at step 2238: 0.0
Loss at step 2239: 0.203842893242836
Loss at step 2240: 0.3886812627315521
Loss at step 2241: 0.3712330460548401
Loss at step 2242: 0.4244491457939148
Loss at step 2243: 0.05760188400745392
Loss at step 2244: 0.04883352294564247
Loss at step 2245: 0.18923319876194
Loss at step 2246: 0.04671841487288475
Loss at step 2247: 0.32380056381225586
Loss at step 2248: 0.12698756158351898
Loss at step 2249: 0.27313631772994995
Loss at step 2250: 0.13997456431388855
Loss at step 2251: 0.1144702285528183
Loss at step 2252: 0.12210982292890549
Loss at step 2253: 0.08316043019294739
Loss at step 2254: 0.009912540204823017
Loss at step 2255: 0.11341393738985062
Loss at step 2256: 0.2891584038734436
Loss at step 2257: 0.12551915645599365
Loss at step 2258: 0.13865956664085388
Loss at step 2259: 0.03773583844304085
Loss at step 2260: 0.08636203408241272
Loss at step 2261: 0.08436360955238342
Loss at step 2262: 0.055392295122146606
Loss at step 2263: 0.036910541355609894
Loss at step 2264: 0.42946144938468933
Loss at step 2265: 0.35104644298553467
Loss at step 2266: 0.1825554519891739
Loss at step 2267: 0.10075495392084122
Loss at step 2268: 0.36017748713493347
Loss at step 2269: 0.18611104786396027
Loss at step 2270: 0.018735840916633606
Loss at step 2271: 0.10250285267829895
Loss at step 2272: 0.2960875928401947
Loss at step 2273: 0.09235575795173645
Loss at step 2274: 0.12614163756370544
Loss at step 2275: 0.3744317591190338
Loss at step 2276: 0.2888273298740387
Loss at step 2277: 0.23999541997909546
Loss at step 2278: 0.09743368625640869
Loss at step 2279: 0.01408199779689312
Loss at step 2280: 0.04819972440600395
Loss at step 2281: 0.052759576588869095
Loss at step 2282: 0.09807924926280975
Loss at step 2283: 0.17953141033649445
Loss at step 2284: 0.12891708314418793
Loss at step 2285: 0.0682494044303894
Loss at step 2286: 0.17761589586734772
Loss at step 2287: 0.0519435852766037
Loss at step 2288: 0.45070579648017883
Loss at step 2289: 0.01524429116398096
Loss at step 2290: 0.3564431071281433
Loss at step 2291: 0.28276851773262024
Loss at step 2292: 0.022145919501781464
Loss at step 2293: 0.06734558194875717
Loss at step 2294: 0.06136687472462654
Loss at step 2295: 0.01959436573088169
Loss at step 2296: 0.014399600215256214
Loss at step 2297: 0.057778917253017426
Loss at step 2298: 0.01842464879155159
Loss at step 2299: 0.27674955129623413
Loss at step 2300: 0.10422936826944351
Loss at step 2301: 0.3505786955356598
Loss at step 2302: 0.32621389627456665
Loss at step 2303: 0.09161949902772903
Loss at step 2304: 0.04649015888571739
Loss at step 2305: 0.17120376229286194
Loss at step 2306: 0.31248193979263306
Loss at step 2307: 0.2861211597919464
Loss at step 2308: 0.12920746207237244
Loss at step 2309: 0.03489470109343529
Loss at step 2310: 0.4097217321395874
Loss at step 2311: 0.02528860792517662
Loss at step 2312: 0.0068582408130168915
Loss at step 2313: 0.008169764652848244
Loss at step 2314: 0.04370831325650215
Loss at step 2315: 0.06308360397815704
Loss at step 2316: 0.028143156319856644
Loss at step 2317: 0.0769442468881607
Loss at step 2318: 0.0732896625995636
Loss at step 2319: 0.15661002695560455
Loss at step 2320: 0.09538459777832031
Loss at step 2321: 0.11944412440061569
Loss at step 2322: 0.2313021868467331
Loss at step 2323: 0.015210079960525036
Loss at step 2324: 0.34725216031074524
Loss at step 2325: 0.11977879703044891
Loss at step 2326: 0.11852753162384033
Loss at step 2327: 0.02347332239151001
Loss at step 2328: 0.10382097214460373
Loss at step 2329: 0.027790797874331474
Loss at step 2330: 0.06088000163435936
Loss at step 2331: 0.09063249081373215
Loss at step 2332: 0.05143355205655098
Loss at step 2333: 0.4094966948032379
Loss at step 2334: 0.10424806922674179
Loss at step 2335: 0.01326548121869564
Loss at step 2336: 0.008858525194227695
Loss at step 2337: 0.04354570806026459
Loss at step 2338: 0.31688082218170166
Loss at step 2339: 0.0835028737783432
Loss at step 2340: 0.2976585328578949
Loss at step 2341: 0.23422302305698395
Loss at step 2342: 0.39314204454421997
Loss at step 2343: 0.08861794322729111
Loss at step 2344: 0.11533601582050323
Loss at step 2345: 0.070871502161026
Loss at step 2346: 0.04845293238759041
Loss at step 2347: 0.2063915878534317
Loss at step 2348: 0.1504840850830078
Loss at step 2349: 0.0262741781771183
Loss at step 2350: 0.07361344993114471
Loss at step 2351: 0.06298044323921204
Loss at step 2352: 0.05580796301364899
Loss at step 2353: 0.1222936362028122
Loss at step 2354: 0.09024011343717575
Loss at step 2355: 0.4090069532394409
Loss at step 2356: 0.03354944288730621
Loss at step 2357: 0.18094153702259064
Loss at step 2358: 0.16281116008758545
Loss at step 2359: 0.05761450529098511
Loss at step 2360: 0.225382998585701
Loss at step 2361: 0.07189866155385971
Loss at step 2362: 0.16408158838748932
Loss at step 2363: 0.14987464249134064
Loss at step 2364: 0.41981908679008484
Loss at step 2365: 0.02105536125600338
Loss at step 2366: 0.11952630430459976
Loss at step 2367: 0.18723754584789276
Loss at step 2368: 0.18913544714450836
Loss at step 2369: 0.2691316306591034
Loss at step 2370: 0.15882757306098938
Loss at step 2371: 0.09965813159942627
Loss at step 2372: 0.29764991998672485
Loss at step 2373: 0.06870157271623611
Loss at step 2374: 0.192751944065094
Loss at step 2375: 0.06339891999959946
Loss at step 2376: 0.12233636528253555
Loss at step 2377: 0.3807166516780853
Loss at step 2378: 0.14483721554279327
Loss at step 2379: 0.42395082116127014
Loss at step 2380: 0.02339538000524044
Loss at step 2381: 0.1873827427625656
Loss at step 2382: 0.07582522183656693
Loss at step 2383: 0.16151860356330872
Loss at step 2384: 0.4423940181732178
Loss at step 2385: 0.068280428647995
Loss at step 2386: 0.15426896512508392
Loss at step 2387: 0.16704143583774567
Loss at step 2388: 0.1075541079044342
Loss at step 2389: 0.058261506259441376
Loss at step 2390: 0.04023563489317894
Loss at step 2391: 0.0542740635573864
Loss at step 2392: 0.27747491002082825
Loss at step 2393: 0.2369087040424347
Loss at step 2394: 0.07504380494356155
Loss at step 2395: 0.08868172764778137
Loss at step 2396: 0.06507325917482376
Loss at step 2397: 0.009622927755117416
Loss at step 2398: 0.0
Loss at step 2399: 0.049208663403987885
Saving training state...
[2025-08-02 02:03:17,643] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 02:03:24,197] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 2400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 2400: 0.08955492079257965
Loss at step 2401: 0.1184011697769165
Loss at step 2402: 0.027312597259879112
Loss at step 2403: 0.2647528052330017
Loss at step 2404: 0.1884259134531021
Loss at step 2405: 0.024327456951141357
Loss at step 2406: 0.3504893183708191
Loss at step 2407: 0.06938816606998444
Loss at step 2408: 0.16469460725784302
Loss at step 2409: 0.3841330111026764
Loss at step 2410: 0.041578926146030426
Loss at step 2411: 0.11648533493280411
Loss at step 2412: 0.10905265063047409
Loss at step 2413: 0.06906752288341522
Loss at step 2414: 0.10662064701318741
Loss at step 2415: 0.13122086226940155
Loss at step 2416: 0.11808101087808609
Loss at step 2417: 0.22043491899967194
Loss at step 2418: 0.3366980254650116
Loss at step 2419: 0.16506701707839966
Loss at step 2420: 0.10667271167039871
Loss at step 2421: 0.016428183764219284
Loss at step 2422: 0.13203684985637665
Loss at step 2423: 0.04897135868668556
Loss at step 2424: 0.38347113132476807
Loss at step 2425: 0.2594728469848633
Loss at step 2426: 0.30403652787208557
Loss at step 2427: 0.15740208327770233
Loss at step 2428: 0.44843220710754395
Loss at step 2429: 0.06699725240468979
Loss at step 2430: 0.20301586389541626
Loss at step 2431: 0.019063202664256096
Loss at step 2432: 0.37367185950279236
Loss at step 2433: 0.023663634434342384
Loss at step 2434: 0.020575908944010735
Loss at step 2435: 0.011733068153262138
Loss at step 2436: 0.013041055761277676
Loss at step 2437: 0.18338508903980255
Loss at step 2438: 0.31105875968933105
Loss at step 2439: 0.07274705916643143
Loss at step 2440: 0.3964332342147827
Loss at step 2441: 0.21007288992404938
Loss at step 2442: 0.2747131884098053
Loss at step 2443: 0.08228450268507004
Loss at step 2444: 0.1549198478460312
Loss at step 2445: 0.04742693901062012
Loss at step 2446: 0.27755114436149597
Loss at step 2447: 0.049321383237838745
Loss at step 2448: 0.05109075456857681
Loss at step 2449: 0.068901926279068
Loss at step 2450: 0.08355136215686798
Loss at step 2451: 0.366451233625412
Loss at step 2452: 0.038847070187330246
Loss at step 2453: 0.07569940388202667
Loss at step 2454: 0.19835583865642548
Loss at step 2455: 0.4051658511161804
Loss at step 2456: 0.27602872252464294
Loss at step 2457: 0.04154739901423454
Loss at step 2458: 0.08280450105667114
Loss at step 2459: 0.06544362753629684
Loss at step 2460: 0.10146458446979523
Loss at step 2461: 0.02139025740325451
Loss at step 2462: 0.03364412486553192
Loss at step 2463: 0.08202225714921951
Loss at step 2464: 0.05800563469529152
Loss at step 2465: 0.16174890100955963
Loss at step 2466: 0.004596424754709005
Loss at step 2467: 0.08109535276889801
Loss at step 2468: 0.10821213573217392
Loss at step 2469: 0.146831214427948
Loss at step 2470: 0.09310510754585266
Loss at step 2471: 0.2635214030742645
Loss at step 2472: 0.19119441509246826
Loss at step 2473: 0.09180095791816711
Loss at step 2474: 0.07005484402179718
Loss at step 2475: 0.22652886807918549
Loss at step 2476: 0.23624791204929352
Loss at step 2477: 0.017384620383381844
Loss at step 2478: 0.2240075170993805
Loss at step 2479: 0.11836986243724823
Loss at step 2480: 0.05879485607147217
Loss at step 2481: 0.029481377452611923
Loss at step 2482: 0.18718178570270538
Loss at step 2483: 0.07623103260993958
Loss at step 2484: 0.09543214738368988
Loss at step 2485: 0.1493811011314392
Loss at step 2486: 0.2026173621416092
Loss at step 2487: 0.1305713653564453
Loss at step 2488: 0.2843366861343384
Loss at step 2489: 0.02793329395353794
Loss at step 2490: 0.26283982396125793
Loss at step 2491: 0.2541619539260864
Loss at step 2492: 0.46255362033843994
Loss at step 2493: 0.09012030065059662
Loss at step 2494: 0.15495824813842773
Loss at step 2495: 0.2905259430408478
Loss at step 2496: 0.06021624431014061
Loss at step 2497: 0.04473273828625679
Loss at step 2498: 0.5450599789619446
Loss at step 2499: 0.27729856967926025
Loss at step 2500: 0.4191807210445404
Loss at step 2501: 0.11466577649116516
Loss at step 2502: 0.39195749163627625
Loss at step 2503: 0.06199999898672104
Loss at step 2504: 0.10402265191078186
Loss at step 2505: 0.2528112232685089
Loss at step 2506: 0.13958154618740082
Loss at step 2507: 0.012998691760003567
Loss at step 2508: 0.05203738808631897
Loss at step 2509: 0.08027582615613937
Loss at step 2510: 0.04237683117389679
Loss at step 2511: 0.1035759299993515
Loss at step 2512: 0.36931106448173523
Loss at step 2513: 0.28184494376182556
Loss at step 2514: 0.2423344999551773
Loss at step 2515: 0.17066845297813416
Loss at step 2516: 0.24993491172790527
Loss at step 2517: 0.20282316207885742
Loss at step 2518: 0.06472708284854889
Loss at step 2519: 0.0187656432390213
Loss at step 2520: 0.010083256289362907
Loss at step 2521: 0.37382352352142334
Loss at step 2522: 0.1154061108827591
Loss at step 2523: 0.360871285200119
Loss at step 2524: 0.060282111167907715
Loss at step 2525: 0.02359151467680931
Loss at step 2526: 0.08915132284164429
Loss at step 2527: 0.308022141456604
Loss at step 2528: 0.08242575824260712
Loss at step 2529: 0.23746055364608765
Loss at step 2530: 0.08587586134672165
Loss at step 2531: 0.045082662254571915
Loss at step 2532: 0.09105560928583145
Loss at step 2533: 0.12114986777305603
Loss at step 2534: 0.39015546441078186
Loss at step 2535: 0.7595714926719666
Loss at step 2536: 0.1818143129348755
Loss at step 2537: 0.12554529309272766
Loss at step 2538: 0.02741788700222969
Loss at step 2539: 0.05369750037789345
Loss at step 2540: 0.029749630019068718
Loss at step 2541: 0.08229051530361176
Loss at step 2542: 0.07379043102264404
Loss at step 2543: 0.258541077375412
Loss at step 2544: 0.37222105264663696
Loss at step 2545: 0.3202053904533386
Loss at step 2546: 0.11481742560863495
Loss at step 2547: 0.028472786769270897
Loss at step 2548: 0.05704839155077934
Loss at step 2549: 0.3292103111743927
Loss at step 2550: 0.22673219442367554
Loss at step 2551: 0.1668580025434494
Loss at step 2552: 0.11403697729110718
Loss at step 2553: 0.022811351343989372
Loss at step 2554: 0.04786761477589607
Loss at step 2555: 0.09067379683256149
Loss at step 2556: 0.03820373862981796
Loss at step 2557: 0.35979801416397095
Loss at step 2558: 0.05772336572408676
Loss at step 2559: 0.1615549921989441
Loss at step 2560: 0.10332498699426651
Loss at step 2561: 0.07865671068429947
Loss at step 2562: 0.09716524183750153
Loss at step 2563: 0.11773917824029922
Loss at step 2564: 0.10382173955440521
Loss at step 2565: 0.09261639416217804
Loss at step 2566: 0.3833812177181244
Loss at step 2567: 0.06153257563710213
Loss at step 2568: 0.011082728393375874
Loss at step 2569: 0.031910240650177
Loss at step 2570: 0.04621841013431549
Loss at step 2571: 0.1894323229789734
Loss at step 2572: 0.09178745001554489
Loss at step 2573: 0.11966980248689651
Loss at step 2574: 0.13552738726139069
Loss at step 2575: 0.03710946440696716
Loss at step 2576: 0.11095643043518066
Loss at step 2577: 0.052705708891153336
Loss at step 2578: 0.03910233452916145
Loss at step 2579: 0.2112269401550293
Loss at step 2580: 0.033995818346738815
Loss at step 2581: 0.05923125147819519
Loss at step 2582: 0.21442075073719025
Loss at step 2583: 0.062066175043582916
Loss at step 2584: 0.12243367731571198
Loss at step 2585: 0.17543405294418335
Loss at step 2586: 0.028290295973420143
Loss at step 2587: 0.16575488448143005
Loss at step 2588: 0.20062893629074097
Loss at step 2589: 0.23414376378059387
Loss at step 2590: 0.13458751142024994
Loss at step 2591: 0.04883596673607826
Loss at step 2592: 0.03210943564772606
Loss at step 2593: 0.20726293325424194
Loss at step 2594: 0.10449467599391937
Loss at step 2595: 0.06548921018838882
Loss at step 2596: 0.1151006668806076
Loss at step 2597: 0.028082704171538353
Loss at step 2598: 0.5815007090568542
Loss at step 2599: 0.12100572139024734
Saving training state...
[2025-08-02 03:15:03,628] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 03:15:10,144] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 2600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 2600: 0.015241438522934914
Loss at step 2601: 0.02188456431031227
Loss at step 2602: 0.019192632287740707
Loss at step 2603: 0.024058161303400993
Loss at step 2604: 0.13297274708747864
Loss at step 2605: 0.08378798514604568
Loss at step 2606: 0.033417776226997375
Loss at step 2607: 0.16226236522197723
Loss at step 2608: 0.011362219229340553
Loss at step 2609: 0.13356700539588928
Loss at step 2610: 0.11093734204769135
Loss at step 2611: 0.10628540813922882
Loss at step 2612: 0.06206417828798294
Loss at step 2613: 0.1972200870513916
Loss at step 2614: 0.021055171266198158
Loss at step 2615: 0.11204742640256882
Loss at step 2616: 0.015285656787455082
Loss at step 2617: 0.27656760811805725
Loss at step 2618: 0.21633872389793396
Loss at step 2619: 0.15292489528656006
Loss at step 2620: 0.1408682018518448
Loss at step 2621: 0.3930601477622986
Loss at step 2622: 0.10382205247879028
Loss at step 2623: 0.13593119382858276
Loss at step 2624: 0.34289631247520447
Loss at step 2625: 0.5900663137435913
Loss at step 2626: 0.0
Loss at step 2627: 0.1278301328420639
Loss at step 2628: 0.13687607645988464
Loss at step 2629: 0.040335606783628464
Loss at step 2630: 0.1313725709915161
Loss at step 2631: 0.07253460586071014
Loss at step 2632: 0.08806855231523514
Loss at step 2633: 0.1450374573469162
Loss at step 2634: 0.3207012414932251
Loss at step 2635: 0.021010451018810272
Loss at step 2636: 0.2951502501964569
Loss at step 2637: 0.10072929412126541
Loss at step 2638: 0.140353262424469
Loss at step 2639: 0.11769367009401321
Loss at step 2640: 0.056937266141176224
Loss at step 2641: 0.05026168376207352
Loss at step 2642: 0.04114604741334915
Loss at step 2643: 0.20804573595523834
Loss at step 2644: 0.17471624910831451
Loss at step 2645: 0.05924684554338455
Loss at step 2646: 0.1697913408279419
Loss at step 2647: 0.13102401793003082
Loss at step 2648: 0.1858098804950714
Loss at step 2649: 0.14646895229816437
Loss at step 2650: 0.06987679749727249
Loss at step 2651: 0.07634983956813812
Loss at step 2652: 0.04219234362244606
Loss at step 2653: 0.024060111492872238
Loss at step 2654: 0.033765219151973724
Loss at step 2655: 0.19892844557762146
Loss at step 2656: 0.2680456340312958
Loss at step 2657: 0.36153915524482727
Loss at step 2658: 0.17137065529823303
Loss at step 2659: 0.3046526312828064
Loss at step 2660: 0.324582040309906
Loss at step 2661: 0.18939900398254395
Loss at step 2662: 0.17009258270263672
Loss at step 2663: 0.11238959431648254
Loss at step 2664: 0.4092291593551636
Loss at step 2665: 0.10070493817329407
Loss at step 2666: 0.19253140687942505
Loss at step 2667: 0.03864697366952896
Loss at step 2668: 0.3319796621799469
Loss at step 2669: 0.3283991813659668
Loss at step 2670: 0.0
Loss at step 2671: 0.1026131734251976
Loss at step 2672: 0.11323318630456924
Loss at step 2673: 0.11144649237394333
Loss at step 2674: 0.33444082736968994
Loss at step 2675: 0.1795380562543869
Loss at step 2676: 0.11729085445404053
Loss at step 2677: 0.07301865518093109
Loss at step 2678: 0.11257830262184143
Loss at step 2679: 0.1013268381357193
Loss at step 2680: 0.08492815494537354
Loss at step 2681: 0.11715012788772583
Loss at step 2682: 0.07821627706289291
Loss at step 2683: 0.03568774461746216
Loss at step 2684: 0.013803614303469658
Loss at step 2685: 0.12376413494348526
Loss at step 2686: 0.3656359314918518
Loss at step 2687: 0.19014215469360352
Loss at step 2688: 0.08066608756780624
Loss at step 2689: 0.11723364889621735
Loss at step 2690: 0.11210285872220993
Loss at step 2691: 0.0308039803057909
Loss at step 2692: 0.04318960756063461
Loss at step 2693: 0.16059553623199463
Loss at step 2694: 0.07560181617736816
Loss at step 2695: 0.26783496141433716
Loss at step 2696: 0.06523426622152328
Loss at step 2697: 0.06297987699508667
Loss at step 2698: 0.08857648074626923
Loss at step 2699: 0.08015213906764984
Loss at step 2700: 0.04950965940952301
Loss at step 2701: 0.01247398741543293
Loss at step 2702: 0.12542913854122162
Loss at step 2703: 0.036521706730127335
Loss at step 2704: 0.22352401912212372
Loss at step 2705: 0.05160823464393616
Loss at step 2706: 0.05595847964286804
Loss at step 2707: 0.3566257357597351
Loss at step 2708: 0.00986564252525568
Loss at step 2709: 0.17635531723499298
Loss at step 2710: 0.05769248679280281
Loss at step 2711: 0.029524371027946472
Loss at step 2712: 0.12257065623998642
Loss at step 2713: 0.09420933574438095
Loss at step 2714: 0.00899090152233839
Loss at step 2715: 0.12720319628715515
Loss at step 2716: 0.2571583688259125
Loss at step 2717: 0.019047360867261887
Loss at step 2718: 0.31851309537887573
Loss at step 2719: 0.08794757723808289
Loss at step 2720: 0.16418945789337158
Loss at step 2721: 0.07286007702350616
Loss at step 2722: 0.1617736965417862
Loss at step 2723: 0.253079354763031
Loss at step 2724: 0.010007206350564957
Loss at step 2725: 0.324320912361145
Loss at step 2726: 0.13563527166843414
Loss at step 2727: 0.15495866537094116
Loss at step 2728: 0.041678935289382935
Loss at step 2729: 0.31524667143821716
Loss at step 2730: 0.0
Loss at step 2731: 0.07951889932155609
Loss at step 2732: 0.10156430304050446
Loss at step 2733: 0.12744556367397308
Loss at step 2734: 0.2820640504360199
Loss at step 2735: 0.059071943163871765
Loss at step 2736: 0.37216514348983765
Loss at step 2737: 0.014172699302434921
Loss at step 2738: 0.3689627945423126
Loss at step 2739: 0.2811739146709442
Loss at step 2740: 0.25928404927253723
Loss at step 2741: 0.049254219979047775
Loss at step 2742: 0.10831791162490845
Loss at step 2743: 0.04461158812046051
Loss at step 2744: 0.05276960879564285
Loss at step 2745: 0.06563808023929596
Loss at step 2746: 0.4738014042377472
Loss at step 2747: 0.052199047058820724
Loss at step 2748: 0.008476270362734795
Loss at step 2749: 0.1595887690782547
Loss at step 2750: 0.08861641585826874
Loss at step 2751: 0.24121446907520294
Loss at step 2752: 0.27283066511154175
Loss at step 2753: 0.12552928924560547
Loss at step 2754: 0.08091562241315842
Loss at step 2755: 0.0048864153213799
Loss at step 2756: 0.11613984405994415
Loss at step 2757: 0.12523633241653442
Loss at step 2758: 0.14209400117397308
Loss at step 2759: 0.018496261909604073
Loss at step 2760: 0.1337272673845291
Loss at step 2761: 0.2957526445388794
Loss at step 2762: 0.16742566227912903
Loss at step 2763: 0.07175234705209732
Loss at step 2764: 0.26862889528274536
Loss at step 2765: 0.021769719198346138
Loss at step 2766: 0.04879038408398628
Loss at step 2767: 0.2316078245639801
Loss at step 2768: 0.03304939344525337
Loss at step 2769: 0.20649075508117676
Loss at step 2770: 0.020288627594709396
Loss at step 2771: 0.05393342673778534
Loss at step 2772: 0.07946129888296127
Loss at step 2773: 0.1252419501543045
Loss at step 2774: 0.039648205041885376
Loss at step 2775: 0.0378502681851387
Loss at step 2776: 0.040573205798864365
Loss at step 2777: 0.07562343031167984
Loss at step 2778: 0.05194578319787979
Loss at step 2779: 0.18535710871219635
Loss at step 2780: 0.08513135462999344
Loss at step 2781: 0.0
Loss at step 2782: 0.2953888773918152
Loss at step 2783: 0.12600907683372498
Loss at step 2784: 0.342448353767395
Loss at step 2785: 0.11900283396244049
Loss at step 2786: 0.3385520279407501
Loss at step 2787: 0.14585500955581665
Loss at step 2788: 0.349677175283432
Loss at step 2789: 0.10822699218988419
Loss at step 2790: 0.09391593933105469
Loss at step 2791: 0.07946368306875229
Loss at step 2792: 0.17594893276691437
Loss at step 2793: 0.008349672891199589
Loss at step 2794: 0.11924713104963303
Loss at step 2795: 0.16201920807361603
Loss at step 2796: 0.07610031217336655
Loss at step 2797: 0.025362445041537285
Loss at step 2798: 0.19001801311969757
Loss at step 2799: 0.12373903393745422
Saving training state...
[2025-08-02 04:26:05,278] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 04:26:11,871] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 2800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 2800: 0.140017569065094
Loss at step 2801: 0.4272283613681793
Loss at step 2802: 0.13036729395389557
Loss at step 2803: 0.16888083517551422
Loss at step 2804: 0.2612091600894928
Loss at step 2805: 0.058918897062540054
Loss at step 2806: 0.19253818690776825
Loss at step 2807: 0.0984121635556221
Loss at step 2808: 0.1759137362241745
Loss at step 2809: 0.1764305979013443
Loss at step 2810: 0.11030042916536331
Loss at step 2811: 0.11062834411859512
Loss at step 2812: 0.053424134850502014
Loss at step 2813: 0.16637444496154785
Loss at step 2814: 0.0636216253042221
Loss at step 2815: 0.009633221663534641
Loss at step 2816: 0.06284213066101074
Loss at step 2817: 0.0889165997505188
Loss at step 2818: 0.014466377906501293
Loss at step 2819: 0.3935592770576477
Loss at step 2820: 0.02894553914666176
Loss at step 2821: 0.2868102788925171
Loss at step 2822: 0.12034452706575394
Loss at step 2823: 0.20988845825195312
Loss at step 2824: 0.0802343487739563
Loss at step 2825: 0.12206313759088516
Loss at step 2826: 0.1027388870716095
Loss at step 2827: 0.15121228992938995
Loss at step 2828: 0.4095659554004669
Loss at step 2829: 0.25761738419532776
Loss at step 2830: 0.07323659211397171
Loss at step 2831: 0.0688793882727623
Loss at step 2832: 0.08144424855709076
Loss at step 2833: 0.05183415487408638
Loss at step 2834: 0.0413355827331543
Loss at step 2835: 0.07336188852787018
Loss at step 2836: 0.18682977557182312
Loss at step 2837: 0.18011130392551422
Loss at step 2838: 0.3238139748573303
Loss at step 2839: 0.16791439056396484
Loss at step 2840: 0.03165031224489212
Loss at step 2841: 0.17481161653995514
Loss at step 2842: 0.5128205418586731
Loss at step 2843: 0.20897461473941803
Loss at step 2844: 0.07432981580495834
Loss at step 2845: 0.12780751287937164
Loss at step 2846: 0.11554817110300064
Loss at step 2847: 0.3142559826374054
Loss at step 2848: 0.042595766484737396
Loss at step 2849: 0.3625142276287079
Loss at step 2850: 0.10741310566663742
Loss at step 2851: 0.03852732479572296
Loss at step 2852: 0.03235068544745445
Loss at step 2853: 0.15777456760406494
Loss at step 2854: 0.06649822741746902
Loss at step 2855: 0.023862965404987335
Loss at step 2856: 0.04874429479241371
Loss at step 2857: 0.13973119854927063
Loss at step 2858: 0.10270805656909943
Loss at step 2859: 0.38033998012542725
Loss at step 2860: 0.15285086631774902
Loss at step 2861: 0.21421866118907928
Loss at step 2862: 0.12383841723203659
Loss at step 2863: 0.2529512345790863
Loss at step 2864: 0.011226505972445011
Loss at step 2865: 0.5238568782806396
Loss at step 2866: 0.08162519335746765
Loss at step 2867: 0.024211883544921875
Loss at step 2868: 0.07717738300561905
Loss at step 2869: 0.07145551592111588
Loss at step 2870: 0.22126401960849762
Loss at step 2871: 0.4474821984767914
Loss at step 2872: 0.1418161392211914
Loss at step 2873: 0.01879938691854477
Loss at step 2874: 0.1005576103925705
Loss at step 2875: 0.23472340404987335
Loss at step 2876: 0.3917751610279083
Loss at step 2877: 0.5277475118637085
Loss at step 2878: 0.008662521839141846
Loss at step 2879: 0.14476563036441803
Loss at step 2880: 0.009794569574296474
Loss at step 2881: 0.010401577688753605
Loss at step 2882: 0.08442066609859467
Loss at step 2883: 0.09307213872671127
Loss at step 2884: 0.05198543518781662
Loss at step 2885: 0.5651596188545227
Loss at step 2886: 0.29174894094467163
Loss at step 2887: 0.2732168436050415
Loss at step 2888: 0.053255293518304825
Loss at step 2889: 0.2516016960144043
Loss at step 2890: 0.28409433364868164
Loss at step 2891: 0.4247981607913971
Loss at step 2892: 0.3403107225894928
Loss at step 2893: 0.24009957909584045
Loss at step 2894: 0.08409512788057327
Loss at step 2895: 0.3347131907939911
Loss at step 2896: 0.075044646859169
Loss at step 2897: 0.08477000892162323
Loss at step 2898: 0.23280249536037445
Loss at step 2899: 0.2731100916862488
Loss at step 2900: 0.4520750343799591
Loss at step 2901: 0.16811615228652954
Loss at step 2902: 0.21991313993930817
Loss at step 2903: 0.1406250298023224
Loss at step 2904: 0.0713905394077301
Loss at step 2905: 0.1861868053674698
Loss at step 2906: 0.05784321203827858
Loss at step 2907: 0.06314844638109207
Loss at step 2908: 0.04235779866576195
Loss at step 2909: 0.05539031699299812
Loss at step 2910: 0.16677908599376678
Loss at step 2911: 0.13551324605941772
Loss at step 2912: 0.02244102768599987
Loss at step 2913: 0.29107996821403503
Loss at step 2914: 0.1800287365913391
Loss at step 2915: 0.03155365586280823
Loss at step 2916: 0.025600990280508995
Loss at step 2917: 0.23541750013828278
Loss at step 2918: 0.3975364863872528
Loss at step 2919: 0.04933573678135872
Loss at step 2920: 0.013043142855167389
Loss at step 2921: 0.16659891605377197
Loss at step 2922: 0.042342934757471085
Loss at step 2923: 0.06734392791986465
Loss at step 2924: 0.02570243738591671
Loss at step 2925: 0.09892043471336365
Loss at step 2926: 0.01857948862016201
Loss at step 2927: 0.028430940583348274
Loss at step 2928: 0.1667933315038681
Loss at step 2929: 0.07232780754566193
Loss at step 2930: 0.14811620116233826
Loss at step 2931: 0.04188626632094383
Loss at step 2932: 0.05225498974323273
Loss at step 2933: 0.4450547397136688
Loss at step 2934: 0.35483428835868835
Loss at step 2935: 0.20720459520816803
Loss at step 2936: 0.049937888979911804
Loss at step 2937: 0.08538834005594254
Loss at step 2938: 0.37134259939193726
Loss at step 2939: 0.3083961606025696
Loss at step 2940: 0.08916173875331879
Loss at step 2941: 0.04816776514053345
Loss at step 2942: 0.15354089438915253
Loss at step 2943: 0.04029926285147667
Loss at step 2944: 0.2008250206708908
Loss at step 2945: 0.20379488170146942
Loss at step 2946: 0.13364072144031525
Loss at step 2947: 0.04797017201781273
Loss at step 2948: 0.04992997646331787
Loss at step 2949: 0.08582501858472824
Loss at step 2950: 0.083406962454319
Loss at step 2951: 0.06971082836389542
Loss at step 2952: 0.16935385763645172
Loss at step 2953: 0.1312214732170105
Loss at step 2954: 0.3830214738845825
Loss at step 2955: 0.01502233650535345
Loss at step 2956: 0.04234923794865608
Loss at step 2957: 0.10469015687704086
Loss at step 2958: 0.2441636472940445
Loss at step 2959: 0.02164771407842636
Loss at step 2960: 0.15542420744895935
Loss at step 2961: 0.26117488741874695
Loss at step 2962: 0.3316028416156769
Loss at step 2963: 0.22387418150901794
Loss at step 2964: 0.053348369896411896
Loss at step 2965: 0.1898365169763565
Loss at step 2966: 0.12992043793201447
Loss at step 2967: 0.14897388219833374
Loss at step 2968: 0.4107126295566559
Loss at step 2969: 0.17695419490337372
Loss at step 2970: 0.06262236833572388
Loss at step 2971: 0.014620932750403881
Loss at step 2972: 0.586127519607544
Loss at step 2973: 0.34753039479255676
Loss at step 2974: 0.31813883781433105
Loss at step 2975: 0.39229047298431396
Loss at step 2976: 0.05786246061325073
Loss at step 2977: 0.19084538519382477
Loss at step 2978: 0.06709016114473343
Loss at step 2979: 0.11173725128173828
Loss at step 2980: 0.07641897350549698
Loss at step 2981: 0.10133101791143417
Loss at step 2982: 0.07150746881961823
Loss at step 2983: 0.25008997321128845
Loss at step 2984: 0.047461993992328644
Loss at step 2985: 0.33204352855682373
Loss at step 2986: 0.2436760514974594
Loss at step 2987: 0.3205144703388214
Loss at step 2988: 0.2834331691265106
Loss at step 2989: 0.06592370569705963
Loss at step 2990: 0.11544249951839447
Loss at step 2991: 0.1263318508863449
Loss at step 2992: 0.01381748542189598
Loss at step 2993: 0.24953892827033997
Loss at step 2994: 0.019551118835806847
Loss at step 2995: 0.022181347012519836
Loss at step 2996: 0.44481194019317627
Loss at step 2997: 0.06453144550323486
Loss at step 2998: 0.04563148319721222
Loss at step 2999: 0.07188240438699722
Saving training state...
[2025-08-02 05:36:00,878] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 05:36:07,388] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 3000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 3000: 0.027983812615275383
Loss at step 3001: 0.22165848314762115
Loss at step 3002: 0.10608024150133133
Loss at step 3003: 0.18662847578525543
Loss at step 3004: 0.11783473193645477
Loss at step 3005: 0.2440469115972519
Loss at step 3006: 0.29911771416664124
Loss at step 3007: 0.17895150184631348
Loss at step 3008: 0.0794064849615097
Loss at step 3009: 0.08751041442155838
Loss at step 3010: 0.01598583161830902
Loss at step 3011: 0.06914371252059937
Loss at step 3012: 0.04296740144491196
Loss at step 3013: 0.38267117738723755
Loss at step 3014: 0.21124133467674255
Loss at step 3015: 0.04538917541503906
Loss at step 3016: 0.07637371122837067
Loss at step 3017: 0.040442273020744324
Loss at step 3018: 0.044162195175886154
Loss at step 3019: 0.09357862174510956
Loss at step 3020: 0.09715308994054794
Loss at step 3021: 0.2515178620815277
Loss at step 3022: 0.054383497685194016
Loss at step 3023: 0.3704751431941986
Loss at step 3024: 0.368897944688797
Loss at step 3025: 0.13261646032333374
Loss at step 3026: 0.18172098696231842
Loss at step 3027: 0.14681749045848846
Loss at step 3028: 0.19221600890159607
Loss at step 3029: 0.05949929356575012
Loss at step 3030: 0.03572775796055794
Loss at step 3031: 0.12127028405666351
Loss at step 3032: 0.037916626781225204
Loss at step 3033: 0.19488345086574554
Loss at step 3034: 0.09974002838134766
Loss at step 3035: 0.2567542493343353
Loss at step 3036: 0.21541032195091248
Loss at step 3037: 0.10558314621448517
Loss at step 3038: 0.037026021629571915
Loss at step 3039: 0.05396163463592529
Loss at step 3040: 0.08630447834730148
Loss at step 3041: 0.12731416523456573
Loss at step 3042: 0.016558095812797546
Loss at step 3043: 0.0633477121591568
Loss at step 3044: 0.10253539681434631
Loss at step 3045: 0.20536616444587708
Loss at step 3046: 0.36957496404647827
Loss at step 3047: 0.25814881920814514
Loss at step 3048: 0.0626121461391449
Loss at step 3049: 0.1929820030927658
Loss at step 3050: 0.05803963541984558
Loss at step 3051: 0.08691342920064926
Loss at step 3052: 0.0819147527217865
Loss at step 3053: 0.07456952333450317
Loss at step 3054: 0.17713092267513275
Loss at step 3055: 0.0962069034576416
Loss at step 3056: 0.08589252084493637
Loss at step 3057: 0.09608837962150574
Loss at step 3058: 0.043988361954689026
Loss at step 3059: 0.24348171055316925
Loss at step 3060: 0.035226430743932724
Loss at step 3061: 0.13681380450725555
Loss at step 3062: 0.22825077176094055
Loss at step 3063: 0.16401764750480652
Loss at step 3064: 0.15403689444065094
Loss at step 3065: 0.09916122257709503
Loss at step 3066: 0.4683077335357666
Loss at step 3067: 0.24198496341705322
Loss at step 3068: 0.03377683088183403
Loss at step 3069: 0.0535326786339283
Loss at step 3070: 0.19156010448932648
Loss at step 3071: 0.04526744782924652
Loss at step 3072: 0.0602935291826725
Loss at step 3073: 0.14844290912151337
Loss at step 3074: 0.24084262549877167
Loss at step 3075: 0.07380451261997223
Loss at step 3076: 0.148726224899292
Loss at step 3077: 0.03305699676275253
Loss at step 3078: 0.18652617931365967
Loss at step 3079: 0.17688894271850586
Loss at step 3080: 0.10740862786769867
Loss at step 3081: 0.0828063040971756
Loss at step 3082: 0.008447692729532719
Loss at step 3083: 0.031132573261857033
Loss at step 3084: 0.0537259466946125
Loss at step 3085: 0.019222397357225418
Loss at step 3086: 0.1837465465068817
Loss at step 3087: 0.2882920503616333
Loss at step 3088: 0.06255393475294113
Loss at step 3089: 0.00888017751276493
Loss at step 3090: 0.16350632905960083
Loss at step 3091: 0.1904921978712082
Loss at step 3092: 0.058745838701725006
Loss at step 3093: 0.12856131792068481
Loss at step 3094: 0.24879206717014313
Loss at step 3095: 0.15945357084274292
Loss at step 3096: 0.24267199635505676
Loss at step 3097: 0.05088924616575241
Loss at step 3098: 0.6453193426132202
Loss at step 3099: 0.012811054475605488
Loss at step 3100: 0.09921786934137344
Loss at step 3101: 0.16575032472610474
Loss at step 3102: 0.1902768760919571
Loss at step 3103: 0.04471151530742645
Loss at step 3104: 0.17344170808792114
Loss at step 3105: 0.07585834711790085
Loss at step 3106: 0.24595917761325836
Loss at step 3107: 0.2514859735965729
Loss at step 3108: 0.4577471613883972
Loss at step 3109: 0.3291177749633789
Loss at step 3110: 0.06185964494943619
Loss at step 3111: 0.14236581325531006
Loss at step 3112: 0.08087685704231262
Loss at step 3113: 0.015491615049540997
Loss at step 3114: 0.2195359766483307
Loss at step 3115: 0.01969393901526928
Loss at step 3116: 0.1228334903717041
Loss at step 3117: 0.07059716433286667
Loss at step 3118: 0.2582634687423706
Loss at step 3119: 0.026573162525892258
Loss at step 3120: 0.3767935037612915
Loss at step 3121: 0.013301550410687923
Loss at step 3122: 0.05874457210302353
Loss at step 3123: 0.31614944338798523
Loss at step 3124: 0.2163424789905548
Loss at step 3125: 0.22945775091648102
Loss at step 3126: 0.4076738655567169
Loss at step 3127: 0.03751008212566376
Loss at step 3128: 0.19098591804504395
Loss at step 3129: 0.13658738136291504
Loss at step 3130: 0.3691503703594208
Loss at step 3131: 0.09817343950271606
Loss at step 3132: 0.05274167284369469
Loss at step 3133: 0.048516396433115005
Loss at step 3134: 0.028359021991491318
Loss at step 3135: 0.41230520606040955
Loss at step 3136: 0.23074226081371307
Loss at step 3137: 0.018402982503175735
Loss at step 3138: 0.2398797571659088
Loss at step 3139: 0.15482507646083832
Loss at step 3140: 0.2591577470302582
Loss at step 3141: 0.2273864448070526
Loss at step 3142: 0.12453357875347137
Loss at step 3143: 0.2723170518875122
Loss at step 3144: 0.07778119295835495
Loss at step 3145: 0.030317822471261024
Loss at step 3146: 0.2885685861110687
Loss at step 3147: 0.1306227296590805
Loss at step 3148: 0.3052457571029663
Loss at step 3149: 0.1168292686343193
Loss at step 3150: 0.5263016223907471
Loss at step 3151: 0.1542729288339615
Loss at step 3152: 0.14736951887607574
Loss at step 3153: 0.35494130849838257
Loss at step 3154: 0.1473846137523651
Loss at step 3155: 0.42164984345436096
Loss at step 3156: 0.19346340000629425
Loss at step 3157: 0.007438264321535826
Loss at step 3158: 0.0
Loss at step 3159: 0.39292094111442566
Loss at step 3160: 0.12799124419689178
Loss at step 3161: 0.3221496343612671
Loss at step 3162: 0.20378106832504272
Loss at step 3163: 0.0
Loss at step 3164: 0.11379982531070709
Loss at step 3165: 0.07782358676195145
Loss at step 3166: 0.011164665222167969
Loss at step 3167: 0.06346476078033447
Loss at step 3168: 0.22496077418327332
Loss at step 3169: 0.013412708416581154
Loss at step 3170: 0.09318704158067703
Loss at step 3171: 0.05330394580960274
Loss at step 3172: 0.27883780002593994
Loss at step 3173: 0.013300728052854538
Loss at step 3174: 0.034658607095479965
Loss at step 3175: 0.19837696850299835
Loss at step 3176: 0.08684269338846207
Loss at step 3177: 0.10402598232030869
Loss at step 3178: 0.22519128024578094
Loss at step 3179: 0.1306159496307373
Loss at step 3180: 0.27667510509490967
Loss at step 3181: 0.1726040095090866
Loss at step 3182: 0.10837823152542114
Loss at step 3183: 0.14132316410541534
Loss at step 3184: 0.48510652780532837
Loss at step 3185: 0.02834189310669899
Loss at step 3186: 0.14727185666561127
Loss at step 3187: 0.2755321264266968
Loss at step 3188: 0.3236507475376129
Loss at step 3189: 0.30699577927589417
Loss at step 3190: 0.10211703181266785
Loss at step 3191: 0.12136555463075638
Loss at step 3192: 0.020404867827892303
Loss at step 3193: 0.11035647243261337
Loss at step 3194: 0.1350485235452652
Loss at step 3195: 0.15003536641597748
Loss at step 3196: 0.36277902126312256
Loss at step 3197: 0.2637488543987274
Loss at step 3198: 0.10997690260410309
Loss at step 3199: 0.03102734684944153
Saving training state...
[2025-08-02 06:47:01,116] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 06:47:07,738] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 3200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 3200: 0.029444683343172073
Loss at step 3201: 0.19377267360687256
Loss at step 3202: 0.22913260757923126
Loss at step 3203: 0.4609515964984894
Loss at step 3204: 0.012110979296267033
Loss at step 3205: 0.033903639763593674
Loss at step 3206: 0.19794043898582458
Loss at step 3207: 0.22786477208137512
Loss at step 3208: 0.010086487978696823
Loss at step 3209: 0.19610808789730072
Loss at step 3210: 0.20019961893558502
Loss at step 3211: 0.21048176288604736
Loss at step 3212: 0.10273075848817825
Loss at step 3213: 0.06906519085168839
Loss at step 3214: 0.18565107882022858
Loss at step 3215: 0.09098120033740997
Loss at step 3216: 0.03305547684431076
Loss at step 3217: 0.5405802130699158
Loss at step 3218: 0.3185718357563019
Loss at step 3219: 0.25934839248657227
Loss at step 3220: 0.08040264248847961
Loss at step 3221: 0.12074558436870575
Loss at step 3222: 0.1092437207698822
Loss at step 3223: 0.07037707418203354
Loss at step 3224: 0.20089782774448395
Loss at step 3225: 0.09871653467416763
Loss at step 3226: 0.20058590173721313
Loss at step 3227: 0.3551006019115448
Loss at step 3228: 0.09853953123092651
Loss at step 3229: 0.09527504444122314
Loss at step 3230: 0.3637353181838989
Loss at step 3231: 0.04908793792128563
Loss at step 3232: 0.06476139277219772
Loss at step 3233: 0.16413918137550354
Loss at step 3234: 0.20009882748126984
Loss at step 3235: 0.07697229087352753
Loss at step 3236: 0.32048553228378296
Loss at step 3237: 0.21705956757068634
Loss at step 3238: 0.14163029193878174
Loss at step 3239: 0.24990405142307281
Loss at step 3240: 0.04475532844662666
Loss at step 3241: 0.28219231963157654
Loss at step 3242: 0.1201300323009491
Loss at step 3243: 0.13889527320861816
Loss at step 3244: 0.26151517033576965
Loss at step 3245: 0.1905359923839569
Loss at step 3246: 0.17744536697864532
Loss at step 3247: 0.37994185090065
Loss at step 3248: 0.2593654692173004
Loss at step 3249: 0.16146238148212433
Loss at step 3250: 0.01916785165667534
Loss at step 3251: 0.27948907017707825
Loss at step 3252: 0.37657228112220764
Loss at step 3253: 0.16494536399841309
Loss at step 3254: 0.25053828954696655
Loss at step 3255: 0.0664677619934082
Loss at step 3256: 0.1346558779478073
Loss at step 3257: 0.09100224077701569
Loss at step 3258: 0.18426860868930817
Loss at step 3259: 0.1545821577310562
Loss at step 3260: 0.04129086807370186
Loss at step 3261: 0.09330044686794281
Loss at step 3262: 0.039341557770967484
Loss at step 3263: 0.07990238815546036
Loss at step 3264: 0.30459269881248474
Loss at step 3265: 0.029208308085799217
Loss at step 3266: 0.165226012468338
Loss at step 3267: 0.2856079936027527
Loss at step 3268: 0.5371575355529785
Loss at step 3269: 0.4169009029865265
Loss at step 3270: 0.2212335467338562
Loss at step 3271: 0.10829091817140579
Loss at step 3272: 0.047103315591812134
Loss at step 3273: 0.15654884278774261
Loss at step 3274: 0.006712059024721384
Loss at step 3275: 0.218296617269516
Loss at step 3276: 0.18930751085281372
Loss at step 3277: 0.18788541853427887
Loss at step 3278: 0.5272802710533142
Loss at step 3279: 0.0514148473739624
Loss at step 3280: 0.1480303406715393
Loss at step 3281: 0.09123025834560394
Loss at step 3282: 0.08195704966783524
Loss at step 3283: 0.1638297140598297
Loss at step 3284: 0.12982216477394104
Loss at step 3285: 0.029623666778206825
Loss at step 3286: 0.12840043008327484
Loss at step 3287: 0.043158020824193954
Loss at step 3288: 0.043435391038656235
Loss at step 3289: 0.06380651891231537
Loss at step 3290: 0.06459774076938629
Loss at step 3291: 0.03403623774647713
Loss at step 3292: 0.12126821279525757
Loss at step 3293: 0.07688534259796143
Loss at step 3294: 0.11336816847324371
Loss at step 3295: 0.11644118279218674
Loss at step 3296: 0.16381117701530457
Loss at step 3297: 0.05901655554771423
Loss at step 3298: 0.1329069435596466
Loss at step 3299: 0.0530211366713047
Loss at step 3300: 0.07769724726676941
Loss at step 3301: 0.16704967617988586
Loss at step 3302: 0.07236102968454361
Loss at step 3303: 0.04048890247941017
Loss at step 3304: 0.06271372735500336
Loss at step 3305: 0.3241933286190033
Loss at step 3306: 0.23386137187480927
Loss at step 3307: 0.21263383328914642
Loss at step 3308: 0.08537673205137253
Loss at step 3309: 0.08057247847318649
Loss at step 3310: 0.058382149785757065
Loss at step 3311: 0.22516576945781708
Loss at step 3312: 0.0986265018582344
Loss at step 3313: 0.05301443487405777
Loss at step 3314: 0.2678801417350769
Loss at step 3315: 0.017526375129818916
Loss at step 3316: 0.2873837351799011
Loss at step 3317: 0.1192660704255104
Loss at step 3318: 0.07594286650419235
Loss at step 3319: 0.013151299208402634
Loss at step 3320: 0.22562275826931
Loss at step 3321: 0.04360419511795044
Loss at step 3322: 0.2937776744365692
Loss at step 3323: 0.0971376821398735
Loss at step 3324: 0.3938959240913391
Loss at step 3325: 0.14078015089035034
Loss at step 3326: 0.1896885484457016
Loss at step 3327: 0.017398476600646973
Loss at step 3328: 0.07322140783071518
Loss at step 3329: 0.06690365076065063
Loss at step 3330: 0.43133634328842163
Loss at step 3331: 0.2807637155056
Loss at step 3332: 0.055473074316978455
Loss at step 3333: 0.16767209768295288
Loss at step 3334: 0.1808784008026123
Loss at step 3335: 0.06605829298496246
Loss at step 3336: 0.21284432709217072
Loss at step 3337: 0.23851561546325684
Loss at step 3338: 0.035213205963373184
Loss at step 3339: 0.033364228904247284
Loss at step 3340: 0.5394188761711121
Loss at step 3341: 0.20787855982780457
Loss at step 3342: 0.006215538829565048
Loss at step 3343: 0.15668758749961853
Loss at step 3344: 0.30114662647247314
Loss at step 3345: 0.13829468190670013
Loss at step 3346: 0.10898606479167938
Loss at step 3347: 0.5192673206329346
Loss at step 3348: 0.31594911217689514
Loss at step 3349: 0.05075161159038544
Loss at step 3350: 0.2051788568496704
Loss at step 3351: 0.31435146927833557
Loss at step 3352: 0.6214798092842102
Loss at step 3353: 0.04785219952464104
Loss at step 3354: 0.19154198467731476
Loss at step 3355: 0.03532903268933296
Loss at step 3356: 0.18482239544391632
Loss at step 3357: 0.2574498951435089
Loss at step 3358: 0.03105281852185726
Loss at step 3359: 0.07979539036750793
Loss at step 3360: 0.13277217745780945
Loss at step 3361: 0.2827109396457672
Loss at step 3362: 0.17386718094348907
Loss at step 3363: 0.10561366379261017
Loss at step 3364: 0.027592413127422333
Loss at step 3365: 0.47160983085632324
Loss at step 3366: 0.009421434253454208
Loss at step 3367: 0.05080646276473999
Loss at step 3368: 0.19640356302261353
Loss at step 3369: 0.0936519205570221
Loss at step 3370: 0.016804754734039307
Loss at step 3371: 0.1998257040977478
Loss at step 3372: 0.13626791536808014
Loss at step 3373: 0.296011745929718
Loss at step 3374: 0.18062961101531982
Loss at step 3375: 0.10993602126836777
Loss at step 3376: 0.3361586034297943
Loss at step 3377: 0.12480071187019348
Loss at step 3378: 0.170509934425354
Loss at step 3379: 0.015816237777471542
Loss at step 3380: 0.07588507235050201
Loss at step 3381: 0.09780890494585037
Loss at step 3382: 0.02168194018304348
Loss at step 3383: 0.020640850067138672
Loss at step 3384: 0.11615405976772308
Loss at step 3385: 0.22736456990242004
Loss at step 3386: 0.0628092959523201
Loss at step 3387: 0.061045676469802856
Loss at step 3388: 0.008861246518790722
Loss at step 3389: 0.03354561701416969
Loss at step 3390: 0.10045439749956131
Loss at step 3391: 0.24937772750854492
Loss at step 3392: 0.030465872958302498
Loss at step 3393: 0.5127361416816711
Loss at step 3394: 0.06151030212640762
Loss at step 3395: 0.16607189178466797
Loss at step 3396: 0.05846215784549713
Loss at step 3397: 0.3555508852005005
Loss at step 3398: 0.4594285488128662
Loss at step 3399: 0.013272719457745552
Saving training state...
[2025-08-02 07:56:04,655] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 07:56:11,088] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 3400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 3400: 0.14579562842845917
Loss at step 3401: 0.43365493416786194
Loss at step 3402: 0.07695915549993515
Loss at step 3403: 0.4796162545681
Loss at step 3404: 0.3863637447357178
Loss at step 3405: 0.02833731099963188
Loss at step 3406: 0.376488596200943
Loss at step 3407: 0.08644554018974304
Loss at step 3408: 0.13862591981887817
Loss at step 3409: 0.4006686508655548
Loss at step 3410: 0.019863983616232872
Loss at step 3411: 0.029658688232302666
Loss at step 3412: 0.13309653103351593
Loss at step 3413: 0.12580853700637817
Loss at step 3414: 0.22605974972248077
Loss at step 3415: 0.06357715278863907
Loss at step 3416: 0.009955079294741154
Loss at step 3417: 0.010683818720281124
Loss at step 3418: 0.13173897564411163
Loss at step 3419: 0.1449217051267624
Loss at step 3420: 0.16465789079666138
Loss at step 3421: 0.10954070091247559
Loss at step 3422: 0.1718454211950302
Loss at step 3423: 0.16764481365680695
Loss at step 3424: 0.058394890278577805
Loss at step 3425: 0.4333989918231964
Loss at step 3426: 0.46883484721183777
Loss at step 3427: 0.029694491997361183
Loss at step 3428: 0.31365951895713806
Loss at step 3429: 0.07618296146392822
Loss at step 3430: 0.08582432568073273
Loss at step 3431: 0.2756505310535431
Loss at step 3432: 0.0
Loss at step 3433: 0.15361325442790985
Loss at step 3434: 0.21521447598934174
Loss at step 3435: 0.02878943271934986
Loss at step 3436: 0.12234768271446228
Loss at step 3437: 0.10045609623193741
Loss at step 3438: 0.37179291248321533
Loss at step 3439: 0.10283251106739044
Loss at step 3440: 0.06554005295038223
Loss at step 3441: 0.08732873946428299
Loss at step 3442: 0.1213456317782402
Loss at step 3443: 0.3834865391254425
Loss at step 3444: 0.0872240737080574
Loss at step 3445: 0.05879881978034973
Loss at step 3446: 0.0589013546705246
Loss at step 3447: 0.6212942600250244
Loss at step 3448: 0.5193706750869751
Loss at step 3449: 0.043654102832078934
Loss at step 3450: 0.11862654983997345
Loss at step 3451: 0.14639973640441895
Loss at step 3452: 0.029388168826699257
Loss at step 3453: 0.25097835063934326
Loss at step 3454: 0.5001446604728699
Loss at step 3455: 0.14214389026165009
Loss at step 3456: 0.031971924006938934
Loss at step 3457: 0.07009227573871613
Loss at step 3458: 0.07684400677680969
Loss at step 3459: 0.32034632563591003
Loss at step 3460: 0.5475545525550842
Loss at step 3461: 0.06210479885339737
Loss at step 3462: 0.041173674166202545
Loss at step 3463: 0.06647142767906189
Loss at step 3464: 0.19364289939403534
Loss at step 3465: 0.05768449231982231
Loss at step 3466: 0.036128733307123184
Loss at step 3467: 0.032186005264520645
Loss at step 3468: 0.07080669701099396
Loss at step 3469: 0.09358563274145126
Loss at step 3470: 0.10314756631851196
Loss at step 3471: 0.25341734290122986
Loss at step 3472: 0.1451294869184494
Loss at step 3473: 0.3131296932697296
Loss at step 3474: 0.13674123585224152
Loss at step 3475: 0.06189385801553726
Loss at step 3476: 0.20558539032936096
Loss at step 3477: 0.17884719371795654
Loss at step 3478: 0.09336304664611816
Loss at step 3479: 0.18494589626789093
Loss at step 3480: 0.09134521335363388
Loss at step 3481: 0.057038500905036926
Loss at step 3482: 0.020100198686122894
Loss at step 3483: 0.060147445648908615
Loss at step 3484: 0.2979622483253479
Loss at step 3485: 0.03769911453127861
Loss at step 3486: 0.025844641029834747
Loss at step 3487: 0.17464742064476013
Loss at step 3488: 0.10866350680589676
Loss at step 3489: 0.0161508247256279
Loss at step 3490: 0.10776594281196594
Loss at step 3491: 0.009266952984035015
Loss at step 3492: 0.07207728177309036
Loss at step 3493: 0.1579926311969757
Loss at step 3494: 0.10782846808433533
Loss at step 3495: 0.033756181597709656
Loss at step 3496: 0.05689462646842003
Loss at step 3497: 0.06444603949785233
Loss at step 3498: 0.026083124801516533
Loss at step 3499: 0.0969632938504219
Loss at step 3500: 0.050658438354730606
Loss at step 3501: 0.19113682210445404
Loss at step 3502: 0.0600171834230423
Loss at step 3503: 0.08487136662006378
Loss at step 3504: 0.01568385399878025
Loss at step 3505: 0.24142323434352875
Loss at step 3506: 0.046806253492832184
Loss at step 3507: 0.116206094622612
Loss at step 3508: 0.08158822357654572
Loss at step 3509: 0.5324501395225525
Loss at step 3510: 0.20661801099777222
Loss at step 3511: 0.44276922941207886
Loss at step 3512: 0.07901020348072052
Loss at step 3513: 0.011961243115365505
Loss at step 3514: 0.27822089195251465
Loss at step 3515: 0.11075221747159958
Loss at step 3516: 0.19043350219726562
Loss at step 3517: 0.02017267607152462
Loss at step 3518: 0.09364387392997742
Loss at step 3519: 0.4729333519935608
Loss at step 3520: 0.13782818615436554
Loss at step 3521: 0.29906976222991943
Loss at step 3522: 0.1193053349852562
Loss at step 3523: 0.062005799263715744
Loss at step 3524: 0.3721659779548645
Loss at step 3525: 0.19861382246017456
Loss at step 3526: 0.09194400906562805
Loss at step 3527: 0.2825791537761688
Loss at step 3528: 0.2895650863647461
Loss at step 3529: 0.0586618036031723
Loss at step 3530: 0.11635041981935501
Loss at step 3531: 0.20164553821086884
Loss at step 3532: 0.08688488602638245
Loss at step 3533: 0.04592234641313553
Loss at step 3534: 0.4038151800632477
Loss at step 3535: 0.05128120630979538
Loss at step 3536: 0.06157190725207329
Loss at step 3537: 0.03735192120075226
Loss at step 3538: 0.18816354870796204
Loss at step 3539: 0.044430892914533615
Loss at step 3540: 0.0620267279446125
Loss at step 3541: 0.22245945036411285
Loss at step 3542: 0.05216607078909874
Loss at step 3543: 0.1909826695919037
Loss at step 3544: 0.019565850496292114
Loss at step 3545: 0.08118574321269989
Loss at step 3546: 0.28770574927330017
Loss at step 3547: 0.051107000559568405
Loss at step 3548: 0.06482605636119843
Loss at step 3549: 0.23141413927078247
Loss at step 3550: 0.18318578600883484
Loss at step 3551: 0.32351154088974
Loss at step 3552: 0.19877153635025024
Loss at step 3553: 0.0645655170083046
Loss at step 3554: 0.16224700212478638
Loss at step 3555: 0.1327628493309021
Loss at step 3556: 0.2998254597187042
Loss at step 3557: 0.12487559765577316
Loss at step 3558: 0.1552625596523285
Loss at step 3559: 0.11618468910455704
Loss at step 3560: 0.10163610428571701
Loss at step 3561: 0.02438356727361679
Loss at step 3562: 0.026090607047080994
Loss at step 3563: 0.18501834571361542
Loss at step 3564: 0.07667597383260727
Loss at step 3565: 0.09748345613479614
Loss at step 3566: 0.08971056342124939
Loss at step 3567: 0.30294039845466614
Loss at step 3568: 0.2363915890455246
Loss at step 3569: 0.09869952499866486
Loss at step 3570: 0.07982058823108673
Loss at step 3571: 0.04876774549484253
Loss at step 3572: 0.007636754773557186
Loss at step 3573: 0.026444192975759506
Loss at step 3574: 0.1100691556930542
Loss at step 3575: 0.16983801126480103
Loss at step 3576: 0.321035236120224
Loss at step 3577: 0.0492827408015728
Loss at step 3578: 0.43065065145492554
Loss at step 3579: 0.051167380064725876
Loss at step 3580: 0.42419272661209106
Loss at step 3581: 0.14820343255996704
Loss at step 3582: 0.020648881793022156
Loss at step 3583: 0.18288220465183258
Loss at step 3584: 0.024011319503188133
Loss at step 3585: 0.3378695547580719
Loss at step 3586: 0.029832914471626282
Loss at step 3587: 0.13024692237377167
Loss at step 3588: 0.3833846151828766
Loss at step 3589: 0.06111958995461464
Loss at step 3590: 0.28952035307884216
Loss at step 3591: 0.19922953844070435
Loss at step 3592: 0.0997265949845314
Loss at step 3593: 0.0
Loss at step 3594: 0.06853620707988739
Loss at step 3595: 0.1347506046295166
Loss at step 3596: 0.029542235657572746
Loss at step 3597: 0.21655090153217316
Loss at step 3598: 0.21870183944702148
Loss at step 3599: 0.03785612806677818
Saving training state...
[2025-08-02 09:07:15,047] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 09:07:21,559] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 3600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 3600: 0.5228675603866577
Loss at step 3601: 0.4361956715583801
Loss at step 3602: 0.16453933715820312
Loss at step 3603: 0.017548557370901108
Loss at step 3604: 0.33062687516212463
Loss at step 3605: 0.12219274044036865
Loss at step 3606: 0.021970119327306747
Loss at step 3607: 0.38202762603759766
Loss at step 3608: 0.07287699729204178
Loss at step 3609: 0.10525747388601303
Loss at step 3610: 0.19265995919704437
Loss at step 3611: 0.15621306002140045
Loss at step 3612: 0.05578410625457764
Loss at step 3613: 0.05281760171055794
Loss at step 3614: 0.12042923271656036
Loss at step 3615: 0.19203419983386993
Loss at step 3616: 0.04202248156070709
Loss at step 3617: 0.33145081996917725
Loss at step 3618: 0.24542970955371857
Loss at step 3619: 0.19804297387599945
Loss at step 3620: 0.34617432951927185
Loss at step 3621: 0.047754161059856415
Loss at step 3622: 0.07555250078439713
Loss at step 3623: 0.13649989664554596
Loss at step 3624: 0.09170684218406677
Loss at step 3625: 0.2509540915489197
Loss at step 3626: 0.17935213446617126
Loss at step 3627: 0.3239646553993225
Loss at step 3628: 0.3039200007915497
Loss at step 3629: 0.022636594250798225
Loss at step 3630: 0.06692426651716232
Loss at step 3631: 0.3575035035610199
Loss at step 3632: 0.2769030034542084
Loss at step 3633: 0.11108595877885818
Loss at step 3634: 0.051143892109394073
Loss at step 3635: 0.3067879378795624
Loss at step 3636: 0.331371545791626
Loss at step 3637: 0.18850435316562653
Loss at step 3638: 0.1470358818769455
Loss at step 3639: 0.2601712644100189
Loss at step 3640: 0.06526561081409454
Loss at step 3641: 0.2889387607574463
Loss at step 3642: 0.18345840275287628
Loss at step 3643: 0.08696123212575912
Loss at step 3644: 0.10695474594831467
Loss at step 3645: 0.034637901932001114
Loss at step 3646: 0.04491976648569107
Loss at step 3647: 0.35574963688850403
Loss at step 3648: 0.08227686583995819
Loss at step 3649: 0.306772381067276
Loss at step 3650: 0.24033446609973907
Loss at step 3651: 0.267250657081604
Loss at step 3652: 0.0819583386182785
Loss at step 3653: 0.027427902445197105
Loss at step 3654: 0.16771067678928375
Loss at step 3655: 0.29128530621528625
Loss at step 3656: 0.03162086009979248
Loss at step 3657: 0.23924314975738525
Loss at step 3658: 0.13503260910511017
Loss at step 3659: 0.2711926996707916
Loss at step 3660: 0.08168654143810272
Loss at step 3661: 0.19889377057552338
Loss at step 3662: 0.11522511392831802
Loss at step 3663: 0.10629276186227798
Loss at step 3664: 0.469317227602005
Loss at step 3665: 0.034429311752319336
Loss at step 3666: 0.24090786278247833
Loss at step 3667: 0.28315842151641846
Loss at step 3668: 0.057551342993974686
Loss at step 3669: 0.07199041545391083
Loss at step 3670: 0.11376577615737915
Loss at step 3671: 0.0825531855225563
Loss at step 3672: 0.05698224902153015
Loss at step 3673: 0.03449563309550285
Loss at step 3674: 0.4186980724334717
Loss at step 3675: 0.07749337702989578
Loss at step 3676: 0.4062628746032715
Loss at step 3677: 0.1231086328625679
Loss at step 3678: 0.03928009793162346
Loss at step 3679: 0.01486533135175705
Loss at step 3680: 0.21122170984745026
Loss at step 3681: 0.3806699812412262
Loss at step 3682: 0.1465977132320404
Loss at step 3683: 0.06752416491508484
Loss at step 3684: 0.18785002827644348
Loss at step 3685: 0.18897001445293427
Loss at step 3686: 0.025724291801452637
Loss at step 3687: 0.040046144276857376
Loss at step 3688: 0.13076353073120117
Loss at step 3689: 0.3792908489704132
Loss at step 3690: 0.37457820773124695
Loss at step 3691: 0.5709718465805054
Loss at step 3692: 0.10399392247200012
Loss at step 3693: 0.1335834562778473
Loss at step 3694: 0.18270878493785858
Loss at step 3695: 0.02067633531987667
Loss at step 3696: 0.1578506976366043
Loss at step 3697: 0.1432020217180252
Loss at step 3698: 0.021445252001285553
Loss at step 3699: 0.2648909091949463
Loss at step 3700: 0.18223141133785248
Loss at step 3701: 0.2885724604129791
Loss at step 3702: 0.15416944026947021
Loss at step 3703: 0.0429556667804718
Loss at step 3704: 0.022095128893852234
Loss at step 3705: 0.5280318856239319
Loss at step 3706: 0.07637209445238113
Loss at step 3707: 0.023493601009249687
Loss at step 3708: 0.27708375453948975
Loss at step 3709: 0.06168035790324211
Loss at step 3710: 0.16390524804592133
Loss at step 3711: 0.06839998066425323
Loss at step 3712: 0.1951092928647995
Loss at step 3713: 0.11089575290679932
Loss at step 3714: 0.2514898180961609
Loss at step 3715: 0.5793251395225525
Loss at step 3716: 0.07726658135652542
Loss at step 3717: 0.07706757634878159
Loss at step 3718: 0.037095893174409866
Loss at step 3719: 0.11837051063776016
Loss at step 3720: 0.16508932411670685
Loss at step 3721: 0.3940331041812897
Loss at step 3722: 0.14611263573169708
Loss at step 3723: 0.017192291095852852
Loss at step 3724: 0.018855294212698936
Loss at step 3725: 0.18505623936653137
Loss at step 3726: 0.010634923353791237
Loss at step 3727: 0.1596425324678421
Loss at step 3728: 0.044566910713911057
Loss at step 3729: 0.1588565707206726
Loss at step 3730: 0.26455241441726685
Loss at step 3731: 0.1571253538131714
Loss at step 3732: 0.010884766466915607
Loss at step 3733: 0.2672708332538605
Loss at step 3734: 0.15218068659305573
Loss at step 3735: 0.08874858915805817
Loss at step 3736: 0.02114555612206459
Loss at step 3737: 0.052462849766016006
Loss at step 3738: 0.21495740115642548
Loss at step 3739: 0.03974771127104759
Loss at step 3740: 0.1909942924976349
Loss at step 3741: 0.11941695213317871
Loss at step 3742: 0.05148143693804741
Loss at step 3743: 0.16779357194900513
Loss at step 3744: 0.043016016483306885
Loss at step 3745: 0.14429660141468048
Loss at step 3746: 0.034119293093681335
Loss at step 3747: 0.03939789906144142
Loss at step 3748: 0.5420891046524048
Loss at step 3749: 0.24965257942676544
Loss at step 3750: 0.2036294937133789
Loss at step 3751: 0.25296974182128906
Loss at step 3752: 0.15031564235687256
Loss at step 3753: 0.12019797414541245
Loss at step 3754: 0.28472504019737244
Loss at step 3755: 0.3847716748714447
Loss at step 3756: 0.03055771067738533
Loss at step 3757: 0.34348979592323303
Loss at step 3758: 0.4430274963378906
Loss at step 3759: 0.16649724543094635
Loss at step 3760: 0.49200329184532166
Loss at step 3761: 0.2869686186313629
Loss at step 3762: 0.09548480063676834
Loss at step 3763: 0.15034592151641846
Loss at step 3764: 0.056808192282915115
Loss at step 3765: 0.08227130770683289
Loss at step 3766: 0.11894186586141586
Loss at step 3767: 0.04926321282982826
Loss at step 3768: 0.004172345157712698
Loss at step 3769: 0.1409713178873062
Loss at step 3770: 0.11997291445732117
Loss at step 3771: 0.02036653645336628
Loss at step 3772: 0.028226856142282486
Loss at step 3773: 0.17253819108009338
Loss at step 3774: 0.0597296878695488
Loss at step 3775: 0.06514377892017365
Loss at step 3776: 0.0515170581638813
Loss at step 3777: 0.037447936832904816
Loss at step 3778: 0.17654664814472198
Loss at step 3779: 0.3856658637523651
Loss at step 3780: 0.2371920496225357
Loss at step 3781: 0.46004658937454224
Loss at step 3782: 0.34145209193229675
Loss at step 3783: 0.0946173220872879
Loss at step 3784: 0.061578355729579926
Loss at step 3785: 0.0611799992620945
Loss at step 3786: 0.11510232836008072
Loss at step 3787: 0.14172454178333282
Loss at step 3788: 0.03825235739350319
Loss at step 3789: 0.24224641919136047
Loss at step 3790: 0.07565663009881973
Loss at step 3791: 0.18540191650390625
Loss at step 3792: 0.2674902081489563
Loss at step 3793: 0.18181326985359192
Loss at step 3794: 0.2959142029285431
Loss at step 3795: 0.05765063688158989
Loss at step 3796: 0.07552709430456161
Loss at step 3797: 0.1184047982096672
Loss at step 3798: 0.34752586483955383
Loss at step 3799: 0.029396479949355125
Saving training state...
[2025-08-02 10:17:40,839] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 10:17:47,204] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 3800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 3800: 0.22808821499347687
Loss at step 3801: 0.06371235102415085
Loss at step 3802: 0.03956841677427292
Loss at step 3803: 0.2511366903781891
Loss at step 3804: 0.08291929215192795
Loss at step 3805: 0.040972206741571426
Loss at step 3806: 0.08209940046072006
Loss at step 3807: 0.44783392548561096
Loss at step 3808: 0.05397360771894455
Loss at step 3809: 0.3289085030555725
Loss at step 3810: 0.1791786402463913
Loss at step 3811: 0.151239812374115
Loss at step 3812: 0.0719805657863617
Loss at step 3813: 0.11354745179414749
Loss at step 3814: 0.08737418055534363
Loss at step 3815: 0.09624597430229187
Loss at step 3816: 0.008069760166108608
Loss at step 3817: 0.1747068464756012
Loss at step 3818: 0.04265100881457329
Loss at step 3819: 0.5014329552650452
Loss at step 3820: 0.09581324458122253
Loss at step 3821: 0.27743345499038696
Loss at step 3822: 0.3322089612483978
Loss at step 3823: 0.3123246431350708
Loss at step 3824: 0.25108519196510315
Loss at step 3825: 0.018743539229035378
Loss at step 3826: 0.04581334441900253
Loss at step 3827: 0.20763733983039856
Loss at step 3828: 0.01791546493768692
Loss at step 3829: 0.007806993089616299
Loss at step 3830: 0.03781406208872795
Loss at step 3831: 0.06074228137731552
Loss at step 3832: 0.49069467186927795
Loss at step 3833: 0.022951290011405945
Loss at step 3834: 0.02167181484401226
Loss at step 3835: 0.12033581733703613
Loss at step 3836: 0.12081893533468246
Loss at step 3837: 0.028487423434853554
Loss at step 3838: 0.10274014621973038
Loss at step 3839: 0.01911698840558529
Loss at step 3840: 0.1872882843017578
Loss at step 3841: 0.061102379113435745
Loss at step 3842: 0.022670958191156387
Loss at step 3843: 0.04813000187277794
Loss at step 3844: 0.1656656265258789
Loss at step 3845: 0.06525228917598724
Loss at step 3846: 0.17639955878257751
Loss at step 3847: 0.019212935119867325
Loss at step 3848: 0.13875098526477814
Loss at step 3849: 0.11581933498382568
Loss at step 3850: 0.0
Loss at step 3851: 0.023962588980793953
Loss at step 3852: 0.17028146982192993
Loss at step 3853: 0.1746932715177536
Loss at step 3854: 0.07755084335803986
Loss at step 3855: 0.1446501463651657
Loss at step 3856: 0.09151574969291687
Loss at step 3857: 0.2389196902513504
Loss at step 3858: 0.04974740743637085
Loss at step 3859: 0.01717555895447731
Loss at step 3860: 0.15875521302223206
Loss at step 3861: 0.05333453789353371
Loss at step 3862: 0.1976833939552307
Loss at step 3863: 0.09530100971460342
Loss at step 3864: 0.0934116393327713
Loss at step 3865: 0.22555002570152283
Loss at step 3866: 0.030877431854605675
Loss at step 3867: 0.11432100087404251
Loss at step 3868: 0.014616012573242188
Loss at step 3869: 0.07035830616950989
Loss at step 3870: 0.3535080850124359
Loss at step 3871: 0.16146954894065857
Loss at step 3872: 0.3485231101512909
Loss at step 3873: 0.31079572439193726
Loss at step 3874: 0.04678741842508316
Loss at step 3875: 0.24916625022888184
Loss at step 3876: 0.3360587954521179
Loss at step 3877: 0.08542641252279282
Loss at step 3878: 0.30342546105384827
Loss at step 3879: 0.017396433278918266
Loss at step 3880: 0.1387903094291687
Loss at step 3881: 0.01033027283847332
Loss at step 3882: 0.14236046373844147
Loss at step 3883: 0.05139866843819618
Loss at step 3884: 0.09694160521030426
Loss at step 3885: 0.11165662109851837
Loss at step 3886: 0.06230352818965912
Loss at step 3887: 0.023186607286334038
Loss at step 3888: 0.09400195628404617
Loss at step 3889: 0.14515097439289093
Loss at step 3890: 0.26120737195014954
Loss at step 3891: 0.007203403394669294
Loss at step 3892: 0.08140058815479279
Loss at step 3893: 0.05338757485151291
Loss at step 3894: 0.024490809068083763
Loss at step 3895: 0.08550061285495758
Loss at step 3896: 0.027865052223205566
Loss at step 3897: 0.03364824503660202
Loss at step 3898: 0.028080331161618233
Loss at step 3899: 0.07541434466838837
Loss at step 3900: 0.16342434287071228
Loss at step 3901: 0.08389004319906235
Loss at step 3902: 0.04980316013097763
Loss at step 3903: 0.1913682222366333
Loss at step 3904: 0.15442876517772675
Loss at step 3905: 0.1540350466966629
Loss at step 3906: 0.03421686589717865
Loss at step 3907: 0.15457499027252197
Loss at step 3908: 0.16728125512599945
Loss at step 3909: 0.25008898973464966
Loss at step 3910: 0.05382544547319412
Loss at step 3911: 0.06590737402439117
Loss at step 3912: 0.053451236337423325
Loss at step 3913: 0.034691762179136276
Loss at step 3914: 0.048950571566820145
Loss at step 3915: 0.03209034726023674
Loss at step 3916: 0.11605912446975708
Loss at step 3917: 0.04366732016205788
Loss at step 3918: 0.088096484541893
Loss at step 3919: 0.010851394385099411
Loss at step 3920: 0.13053856790065765
Loss at step 3921: 0.0
Loss at step 3922: 0.060175370424985886
Loss at step 3923: 0.01708453707396984
Loss at step 3924: 0.08420959860086441
Loss at step 3925: 0.06894068419933319
Loss at step 3926: 0.06542564183473587
Loss at step 3927: 0.05788864940404892
Loss at step 3928: 0.11598261445760727
Loss at step 3929: 0.20899541676044464
Loss at step 3930: 0.08993864804506302
Loss at step 3931: 0.06997127830982208
Loss at step 3932: 0.09667496383190155
Loss at step 3933: 0.0589849129319191
Loss at step 3934: 0.16249099373817444
Loss at step 3935: 0.12797506153583527
Loss at step 3936: 0.145552858710289
Loss at step 3937: 0.04222344234585762
Loss at step 3938: 0.005803660023957491
Loss at step 3939: 0.056446220725774765
Loss at step 3940: 0.2148987501859665
Loss at step 3941: 0.060954418033361435
Loss at step 3942: 0.10493774712085724
Loss at step 3943: 0.40196529030799866
Loss at step 3944: 0.4792293310165405
Loss at step 3945: 0.49026528000831604
Loss at step 3946: 0.1075667142868042
Loss at step 3947: 0.14120541512966156
Loss at step 3948: 0.0
Loss at step 3949: 0.5971457958221436
Loss at step 3950: 0.06463295221328735
Loss at step 3951: 0.1339513063430786
Loss at step 3952: 0.1281144767999649
Loss at step 3953: 0.03905023634433746
Loss at step 3954: 0.06584109365940094
Loss at step 3955: 0.08491197973489761
Loss at step 3956: 0.1570114642381668
Loss at step 3957: 0.06924690306186676
Loss at step 3958: 0.07226035743951797
Loss at step 3959: 0.24184615910053253
Loss at step 3960: 0.04498014226555824
Loss at step 3961: 0.08159689605236053
Loss at step 3962: 0.04103271663188934
Loss at step 3963: 0.03927723318338394
Loss at step 3964: 0.12649193406105042
Loss at step 3965: 0.08333465456962585
Loss at step 3966: 0.24973247945308685
Loss at step 3967: 0.2688012421131134
Loss at step 3968: 0.053284015506505966
Loss at step 3969: 0.13326284289360046
Loss at step 3970: 0.06810338795185089
Loss at step 3971: 0.09527366608381271
Loss at step 3972: 0.15273940563201904
Loss at step 3973: 0.0969884991645813
Loss at step 3974: 0.48069441318511963
Loss at step 3975: 0.07196303457021713
Loss at step 3976: 0.1825225055217743
Loss at step 3977: 0.14195097982883453
Loss at step 3978: 0.14952756464481354
Loss at step 3979: 0.21322068572044373
Loss at step 3980: 0.1505008488893509
Loss at step 3981: 0.15732088685035706
Loss at step 3982: 0.19586539268493652
Loss at step 3983: 0.03723645582795143
Loss at step 3984: 0.10233286023139954
Loss at step 3985: 0.16103914380073547
Loss at step 3986: 0.12191645056009293
Loss at step 3987: 0.14166714251041412
Loss at step 3988: 0.28298088908195496
Loss at step 3989: 0.09151191264390945
Loss at step 3990: 0.07574276626110077
Loss at step 3991: 0.03175029903650284
Loss at step 3992: 0.05414142459630966
Loss at step 3993: 0.03199080005288124
Loss at step 3994: 0.20438295602798462
Loss at step 3995: 0.3705863058567047
Loss at step 3996: 0.07460480183362961
Loss at step 3997: 0.09878601133823395
Loss at step 3998: 0.07001670449972153
Loss at step 3999: 0.02924083173274994
Saving training state...
[2025-08-02 11:27:15,148] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 11:27:21,580] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 4000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 4000: 0.13101641833782196
Loss at step 4001: 0.4918995201587677
Loss at step 4002: 0.06717414408922195
Loss at step 4003: 0.07200271636247635
Loss at step 4004: 0.18286658823490143
Loss at step 4005: 0.05521155893802643
Loss at step 4006: 0.04494921490550041
Loss at step 4007: 0.09781451523303986
Loss at step 4008: 0.05282824859023094
Loss at step 4009: 0.022933242842555046
Loss at step 4010: 0.1536889523267746
Loss at step 4011: 0.1215326339006424
Loss at step 4012: 0.24999916553497314
Loss at step 4013: 0.17436228692531586
Loss at step 4014: 0.05274733901023865
Loss at step 4015: 0.08566576987504959
Loss at step 4016: 0.06067531928420067
Loss at step 4017: 0.23386074602603912
Loss at step 4018: 0.1295338124036789
Loss at step 4019: 0.20166683197021484
Loss at step 4020: 0.17864258587360382
Loss at step 4021: 0.20895658433437347
Loss at step 4022: 0.35275137424468994
Loss at step 4023: 0.2740876078605652
Loss at step 4024: 0.42334285378456116
Loss at step 4025: 0.005052356980741024
Loss at step 4026: 0.31980106234550476
Loss at step 4027: 0.02304144762456417
Loss at step 4028: 0.4640437066555023
Loss at step 4029: 0.1269586980342865
Loss at step 4030: 0.17309129238128662
Loss at step 4031: 0.22733263671398163
Loss at step 4032: 0.0022466520313173532
Loss at step 4033: 0.2578997313976288
Loss at step 4034: 0.07728255540132523
Loss at step 4035: 0.13577502965927124
Loss at step 4036: 0.20685341954231262
Loss at step 4037: 0.05793684720993042
Loss at step 4038: 0.1381189227104187
Loss at step 4039: 0.10752705484628677
Loss at step 4040: 0.07549329102039337
Loss at step 4041: 0.052515722811222076
Loss at step 4042: 0.16984936594963074
Loss at step 4043: 0.05019371956586838
Loss at step 4044: 0.2363465130329132
Loss at step 4045: 0.2521388530731201
Loss at step 4046: 0.20920652151107788
Loss at step 4047: 0.13692611455917358
Loss at step 4048: 0.3117454946041107
Loss at step 4049: 0.16707104444503784
Loss at step 4050: 0.14582042396068573
Loss at step 4051: 0.08423320949077606
Loss at step 4052: 0.024840187281370163
Loss at step 4053: 0.046088073402643204
Loss at step 4054: 0.14567308127880096
Loss at step 4055: 0.007286746520549059
Loss at step 4056: 0.06513607501983643
Loss at step 4057: 0.12363307178020477
Loss at step 4058: 0.028385300189256668
Loss at step 4059: 0.010599353350698948
Loss at step 4060: 0.21709205210208893
Loss at step 4061: 0.025831343606114388
Loss at step 4062: 0.01870420202612877
Loss at step 4063: 0.3950884938240051
Loss at step 4064: 0.14446184039115906
Loss at step 4065: 0.23438449203968048
Loss at step 4066: 0.3529199957847595
Loss at step 4067: 0.0408690981566906
Loss at step 4068: 0.033919572830200195
Loss at step 4069: 0.6154934763908386
Loss at step 4070: 0.017401304095983505
Loss at step 4071: 0.14288456737995148
Loss at step 4072: 0.0716499611735344
Loss at step 4073: 0.09870360046625137
Loss at step 4074: 0.05240132659673691
Loss at step 4075: 0.26364997029304504
Loss at step 4076: 0.038470763713121414
Loss at step 4077: 0.16191135346889496
Loss at step 4078: 0.2089301496744156
Loss at step 4079: 0.2694814205169678
Loss at step 4080: 0.17810621857643127
Loss at step 4081: 0.049535833299160004
Loss at step 4082: 0.01341333519667387
Loss at step 4083: 0.2827690541744232
Loss at step 4084: 0.2754133641719818
Loss at step 4085: 0.07722992449998856
Loss at step 4086: 0.3090020418167114
Loss at step 4087: 0.1589721441268921
Loss at step 4088: 0.46307554841041565
Loss at step 4089: 0.10406708717346191
Loss at step 4090: 0.25172436237335205
Loss at step 4091: 0.11671632528305054
Loss at step 4092: 0.1356119066476822
Loss at step 4093: 0.02838420681655407
Loss at step 4094: 0.03724629431962967
Loss at step 4095: 0.17016741633415222
Loss at step 4096: 0.29699718952178955
Loss at step 4097: 0.14882269501686096
Loss at step 4098: 0.14416389167308807
Loss at step 4099: 0.012907911092042923
Loss at step 4100: 0.13511809706687927
Loss at step 4101: 0.18131472170352936
Loss at step 4102: 0.1228804960846901
Loss at step 4103: 0.19432801008224487
Loss at step 4104: 0.1822699010372162
Loss at step 4105: 0.2700144052505493
Loss at step 4106: 0.0685136690735817
Loss at step 4107: 0.153549924492836
Loss at step 4108: 0.0
Loss at step 4109: 0.2970788776874542
Loss at step 4110: 0.0821443721652031
Loss at step 4111: 0.3733835816383362
Loss at step 4112: 0.2820819914340973
Loss at step 4113: 0.0432746596634388
Loss at step 4114: 0.004194841720163822
Loss at step 4115: 0.18956750631332397
Loss at step 4116: 0.16913142800331116
Loss at step 4117: 0.04824517294764519
Loss at step 4118: 0.040076687932014465
Loss at step 4119: 0.07830756157636642
Loss at step 4120: 0.1631259024143219
Loss at step 4121: 0.1402890533208847
Loss at step 4122: 0.2650062143802643
Loss at step 4123: 0.11082404106855392
Loss at step 4124: 0.1243596076965332
Loss at step 4125: 0.06042831391096115
Loss at step 4126: 0.043442193418741226
Loss at step 4127: 0.006650038994848728
Loss at step 4128: 0.34717702865600586
Loss at step 4129: 0.2198346108198166
Loss at step 4130: 0.09248027205467224
Loss at step 4131: 0.05666425824165344
Loss at step 4132: 0.0
Loss at step 4133: 0.017453249543905258
Loss at step 4134: 0.045356716960668564
Loss at step 4135: 0.030580049380660057
Loss at step 4136: 0.1238688975572586
Loss at step 4137: 0.09247346222400665
Loss at step 4138: 0.016609681770205498
Loss at step 4139: 0.03744319826364517
Loss at step 4140: 0.040052518248558044
Loss at step 4141: 0.2952483594417572
Loss at step 4142: 0.21201278269290924
Loss at step 4143: 0.17324486374855042
Loss at step 4144: 0.29546085000038147
Loss at step 4145: 0.1558319479227066
Loss at step 4146: 0.32873451709747314
Loss at step 4147: 0.06481393426656723
Loss at step 4148: 0.0782531201839447
Loss at step 4149: 0.12611111998558044
Loss at step 4150: 0.047617994248867035
Loss at step 4151: 0.08994459360837936
Loss at step 4152: 0.015664543956518173
Loss at step 4153: 0.2599875032901764
Loss at step 4154: 0.1899721324443817
Loss at step 4155: 0.0705142468214035
Loss at step 4156: 0.030274195596575737
Loss at step 4157: 0.368149071931839
Loss at step 4158: 0.010988377965986729
Loss at step 4159: 0.010908967815339565
Loss at step 4160: 0.05253183841705322
Loss at step 4161: 0.02633991837501526
Loss at step 4162: 0.07918000221252441
Loss at step 4163: 0.4503161311149597
Loss at step 4164: 0.09228316694498062
Loss at step 4165: 0.14705096185207367
Loss at step 4166: 0.03488459065556526
Loss at step 4167: 0.05253372713923454
Loss at step 4168: 0.12752962112426758
Loss at step 4169: 0.04913543164730072
Loss at step 4170: 0.19969220459461212
Loss at step 4171: 0.06912922114133835
Loss at step 4172: 0.012965495698153973
Loss at step 4173: 0.011461209505796432
Loss at step 4174: 0.1793096512556076
Loss at step 4175: 0.09940316528081894
Loss at step 4176: 0.27874520421028137
Loss at step 4177: 0.30470576882362366
Loss at step 4178: 0.07349040359258652
Loss at step 4179: 0.04002911224961281
Loss at step 4180: 0.1267806589603424
Loss at step 4181: 0.02901967614889145
Loss at step 4182: 0.030154265463352203
Loss at step 4183: 0.21810153126716614
Loss at step 4184: 0.04411739110946655
Loss at step 4185: 0.2743546664714813
Loss at step 4186: 0.010648426599800587
Loss at step 4187: 0.014816798269748688
Loss at step 4188: 0.01204726193100214
Loss at step 4189: 0.057148393243551254
Loss at step 4190: 0.17020627856254578
Loss at step 4191: 0.2588563859462738
Loss at step 4192: 0.16515010595321655
Loss at step 4193: 0.0839434564113617
Loss at step 4194: 0.031853944063186646
Loss at step 4195: 0.29262852668762207
Loss at step 4196: 0.23872961103916168
Loss at step 4197: 0.259960412979126
Loss at step 4198: 0.30799365043640137
Loss at step 4199: 0.048934195190668106
Saving training state...
[2025-08-02 12:38:45,349] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 12:38:51,817] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 4200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 4200: 0.1066519096493721
Loss at step 4201: 0.04083471745252609
Loss at step 4202: 0.2315286248922348
Loss at step 4203: 0.5564824938774109
Loss at step 4204: 0.11586980521678925
Loss at step 4205: 0.009494772180914879
Loss at step 4206: 0.42905914783477783
Loss at step 4207: 0.01578604243695736
Loss at step 4208: 0.019229033961892128
Loss at step 4209: 0.24476025998592377
Loss at step 4210: 0.05751418694853783
Loss at step 4211: 0.13760754466056824
Loss at step 4212: 0.10872486978769302
Loss at step 4213: 0.16585727035999298
Loss at step 4214: 0.058153580874204636
Loss at step 4215: 0.1665155440568924
Loss at step 4216: 0.4539066255092621
Loss at step 4217: 0.1696958839893341
Loss at step 4218: 0.09893650561571121
Loss at step 4219: 0.06043672189116478
Loss at step 4220: 0.24076718091964722
Loss at step 4221: 0.2965411841869354
Loss at step 4222: 0.0686192661523819
Loss at step 4223: 0.2316712737083435
Loss at step 4224: 0.11581530421972275
Loss at step 4225: 0.09101831167936325
Loss at step 4226: 0.11358865350484848
Loss at step 4227: 0.14105917513370514
Loss at step 4228: 0.08179625123739243
Loss at step 4229: 0.13524602353572845
Loss at step 4230: 0.11059936881065369
Loss at step 4231: 0.05668722838163376
Loss at step 4232: 0.11116978526115417
Loss at step 4233: 0.09537278115749359
Loss at step 4234: 0.18279364705085754
Loss at step 4235: 0.19078408181667328
Loss at step 4236: 0.05341876670718193
Loss at step 4237: 0.24952366948127747
Loss at step 4238: 0.059096407145261765
Loss at step 4239: 0.23311704397201538
Loss at step 4240: 0.025248460471630096
Loss at step 4241: 0.15599094331264496
Loss at step 4242: 0.009060428477823734
Loss at step 4243: 0.04246758297085762
Loss at step 4244: 0.3348955810070038
Loss at step 4245: 0.30605971813201904
Loss at step 4246: 0.23966044187545776
Loss at step 4247: 0.06279885023832321
Loss at step 4248: 0.29497694969177246
Loss at step 4249: 0.2505953013896942
Loss at step 4250: 0.2205120474100113
Loss at step 4251: 0.1656302660703659
Loss at step 4252: 0.06720314919948578
Loss at step 4253: 0.48029524087905884
Loss at step 4254: 0.1271861493587494
Loss at step 4255: 0.019561124965548515
Loss at step 4256: 0.5763898491859436
Loss at step 4257: 0.1323658674955368
Loss at step 4258: 0.007410198915749788
Loss at step 4259: 0.1410508006811142
Loss at step 4260: 0.19472040235996246
Loss at step 4261: 0.1564103215932846
Loss at step 4262: 0.01769600436091423
Loss at step 4263: 0.28501033782958984
Loss at step 4264: 0.01045804563909769
Loss at step 4265: 0.14214709401130676
Loss at step 4266: 0.08297950029373169
Loss at step 4267: 0.11034537106752396
Loss at step 4268: 0.09740428626537323
Loss at step 4269: 0.14934735000133514
Loss at step 4270: 0.16204610466957092
Loss at step 4271: 0.04543193802237511
Loss at step 4272: 0.019943872466683388
Loss at step 4273: 0.13694973289966583
Loss at step 4274: 0.3663026988506317
Loss at step 4275: 0.15185879170894623
Loss at step 4276: 0.3815630376338959
Loss at step 4277: 0.06058851629495621
Loss at step 4278: 0.09240615367889404
Loss at step 4279: 0.14299672842025757
Loss at step 4280: 0.007898160256445408
Loss at step 4281: 0.5277333855628967
Loss at step 4282: 0.09342431277036667
Loss at step 4283: 0.0572313629090786
Loss at step 4284: 0.0300553347915411
Loss at step 4285: 0.03607538342475891
Loss at step 4286: 0.23529323935508728
Loss at step 4287: 0.03170917555689812
Loss at step 4288: 0.025238927453756332
Loss at step 4289: 0.06739703565835953
Loss at step 4290: 0.03759252279996872
Loss at step 4291: 0.2653333246707916
Loss at step 4292: 0.06865459680557251
Loss at step 4293: 0.050392668694257736
Loss at step 4294: 0.17080864310264587
Loss at step 4295: 0.04663366451859474
Loss at step 4296: 0.08349582552909851
Loss at step 4297: 0.05782095342874527
Loss at step 4298: 0.031548287719488144
Loss at step 4299: 0.21518294513225555
Loss at step 4300: 0.19273041188716888
Loss at step 4301: 0.12734423577785492
Loss at step 4302: 0.05714752525091171
Loss at step 4303: 0.10202495753765106
Loss at step 4304: 0.11703376471996307
Loss at step 4305: 0.011358818970620632
Loss at step 4306: 0.20675915479660034
Loss at step 4307: 0.04149235412478447
Loss at step 4308: 0.04045328125357628
Loss at step 4309: 0.12020715326070786
Loss at step 4310: 0.32609930634498596
Loss at step 4311: 0.1929105818271637
Loss at step 4312: 0.116574227809906
Loss at step 4313: 0.06597650051116943
Loss at step 4314: 0.22363054752349854
Loss at step 4315: 0.016490167006850243
Loss at step 4316: 0.08791592717170715
Loss at step 4317: 0.0660323053598404
Loss at step 4318: 0.0790562853217125
Loss at step 4319: 0.21255427598953247
Loss at step 4320: 0.03514992445707321
Loss at step 4321: 0.042233217507600784
Loss at step 4322: 0.12749427556991577
Loss at step 4323: 0.03830113634467125
Loss at step 4324: 0.17901907861232758
Loss at step 4325: 0.07315486669540405
Loss at step 4326: 0.04588209092617035
Loss at step 4327: 0.11042274534702301
Loss at step 4328: 0.02661282569169998
Loss at step 4329: 0.28544601798057556
Loss at step 4330: 0.25141003727912903
Loss at step 4331: 0.03953856974840164
Loss at step 4332: 0.10524868220090866
Loss at step 4333: 0.4499187469482422
Loss at step 4334: 0.0694269984960556
Loss at step 4335: 0.24865445494651794
Loss at step 4336: 0.10922664403915405
Loss at step 4337: 0.08253467828035355
Loss at step 4338: 0.016290774568915367
Loss at step 4339: 0.3841468095779419
Loss at step 4340: 0.13833118975162506
Loss at step 4341: 0.24520567059516907
Loss at step 4342: 0.06526559591293335
Loss at step 4343: 0.018693428486585617
Loss at step 4344: 0.2305016666650772
Loss at step 4345: 0.10315514355897903
Loss at step 4346: 0.019204197451472282
Loss at step 4347: 0.19966259598731995
Loss at step 4348: 0.3084551990032196
Loss at step 4349: 0.4103783369064331
Loss at step 4350: 0.2014317363500595
Loss at step 4351: 0.10390207916498184
Loss at step 4352: 0.13904690742492676
Loss at step 4353: 0.038916803896427155
Loss at step 4354: 0.10566651821136475
Loss at step 4355: 0.18825240433216095
Loss at step 4356: 0.16111235320568085
Loss at step 4357: 0.09557491540908813
Loss at step 4358: 0.06826354563236237
Loss at step 4359: 0.3581804931163788
Loss at step 4360: 0.1704452931880951
Loss at step 4361: 0.05511894077062607
Loss at step 4362: 0.11538553237915039
Loss at step 4363: 0.06306543946266174
Loss at step 4364: 0.08109070360660553
Loss at step 4365: 0.04432966187596321
Loss at step 4366: 0.22844859957695007
Loss at step 4367: 0.3211010694503784
Loss at step 4368: 0.007013117428869009
Loss at step 4369: 0.17620112001895905
Loss at step 4370: 0.10270661860704422
Loss at step 4371: 0.021289164200425148
Loss at step 4372: 0.15684707462787628
Loss at step 4373: 0.22763493657112122
Loss at step 4374: 0.08709698170423508
Loss at step 4375: 0.38823580741882324
Loss at step 4376: 0.016227247193455696
Loss at step 4377: 0.024235201999545097
Loss at step 4378: 0.18121738731861115
Loss at step 4379: 0.26994070410728455
Loss at step 4380: 0.05411471053957939
Loss at step 4381: 0.3489488661289215
Loss at step 4382: 0.025865281000733376
Loss at step 4383: 0.2672477662563324
Loss at step 4384: 0.1789557933807373
Loss at step 4385: 0.23496878147125244
Loss at step 4386: 0.11997568607330322
Loss at step 4387: 0.3631613254547119
Loss at step 4388: 0.04569249600172043
Loss at step 4389: 0.05456053465604782
Loss at step 4390: 0.02688560262322426
Loss at step 4391: 0.006289382930845022
Loss at step 4392: 0.1080426573753357
Loss at step 4393: 0.20348747074604034
Loss at step 4394: 0.18283803761005402
Loss at step 4395: 0.05191235989332199
Loss at step 4396: 0.06661012023687363
Loss at step 4397: 0.01184212975203991
Loss at step 4398: 0.059864919632673264
Loss at step 4399: 0.007739721797406673
Saving training state...
[2025-08-02 13:48:50,971] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 13:48:57,472] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 4400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 4400: 0.3228214681148529
Loss at step 4401: 0.13412292301654816
Loss at step 4402: 0.0
Loss at step 4403: 0.26736247539520264
Loss at step 4404: 0.06642308086156845
Loss at step 4405: 0.12572698295116425
Loss at step 4406: 0.2958439886569977
Loss at step 4407: 0.11274956911802292
Loss at step 4408: 0.2352866530418396
Loss at step 4409: 0.05489351972937584
Loss at step 4410: 0.0
Loss at step 4411: 0.2262267768383026
Loss at step 4412: 0.0351899079978466
Loss at step 4413: 0.06511034071445465
Loss at step 4414: 0.22946715354919434
Loss at step 4415: 0.1447308212518692
Loss at step 4416: 0.20931649208068848
Loss at step 4417: 0.11065653711557388
Loss at step 4418: 0.17174936830997467
Loss at step 4419: 0.2483694851398468
Loss at step 4420: 0.22020772099494934
Loss at step 4421: 0.03292815387248993
Loss at step 4422: 0.06587126851081848
Loss at step 4423: 0.05040216073393822
Loss at step 4424: 0.06558467447757721
Loss at step 4425: 0.06387201696634293
Loss at step 4426: 0.08884105086326599
Loss at step 4427: 0.16142678260803223
Loss at step 4428: 0.20385003089904785
Loss at step 4429: 0.14233827590942383
Loss at step 4430: 0.010214501060545444
Loss at step 4431: 0.0180253516882658
Loss at step 4432: 0.06778273731470108
Loss at step 4433: 0.18388769030570984
Loss at step 4434: 0.1060134693980217
Loss at step 4435: 0.038940757513046265
Loss at step 4436: 0.5425950884819031
Loss at step 4437: 0.3439023196697235
Loss at step 4438: 0.24692800641059875
Loss at step 4439: 0.19280213117599487
Loss at step 4440: 0.1843024045228958
Loss at step 4441: 0.03522857651114464
Loss at step 4442: 0.19941355288028717
Loss at step 4443: 0.03148530423641205
Loss at step 4444: 0.06906579434871674
Loss at step 4445: 0.012075123377144337
Loss at step 4446: 0.022425629198551178
Loss at step 4447: 0.13815973699092865
Loss at step 4448: 0.05042126402258873
Loss at step 4449: 0.17450766265392303
Loss at step 4450: 0.10513917356729507
Loss at step 4451: 0.03904296085238457
Loss at step 4452: 0.11862640082836151
Loss at step 4453: 0.086588054895401
Loss at step 4454: 0.09355822205543518
Loss at step 4455: 0.14822064340114594
Loss at step 4456: 0.14810703694820404
Loss at step 4457: 0.08027911186218262
Loss at step 4458: 0.27103137969970703
Loss at step 4459: 0.432294100522995
Loss at step 4460: 0.009574653580784798
Loss at step 4461: 0.26308485865592957
Loss at step 4462: 0.0716310366988182
Loss at step 4463: 0.3139720857143402
Loss at step 4464: 0.0
Loss at step 4465: 0.04588305577635765
Loss at step 4466: 0.06679446250200272
Loss at step 4467: 0.06520815193653107
Loss at step 4468: 0.08723330497741699
Loss at step 4469: 0.035824231803417206
Loss at step 4470: 0.0808510035276413
Loss at step 4471: 0.09317445009946823
Loss at step 4472: 0.011005264706909657
Loss at step 4473: 0.1658744066953659
Loss at step 4474: 0.09316587448120117
Loss at step 4475: 0.06952773034572601
Loss at step 4476: 0.07561153173446655
Loss at step 4477: 0.45570454001426697
Loss at step 4478: 0.17779220640659332
Loss at step 4479: 0.4000982642173767
Loss at step 4480: 0.05210336297750473
Loss at step 4481: 0.19355139136314392
Loss at step 4482: 0.1884433627128601
Loss at step 4483: 0.16055582463741302
Loss at step 4484: 0.4435420036315918
Loss at step 4485: 0.0906870886683464
Loss at step 4486: 0.04333795979619026
Loss at step 4487: 0.20989762246608734
Loss at step 4488: 0.06401887536048889
Loss at step 4489: 0.05864590406417847
Loss at step 4490: 0.04559575021266937
Loss at step 4491: 0.26527076959609985
Loss at step 4492: 0.4547564685344696
Loss at step 4493: 0.09955880790948868
Loss at step 4494: 0.012362468987703323
Loss at step 4495: 0.07386322319507599
Loss at step 4496: 0.2236248254776001
Loss at step 4497: 0.32003557682037354
Loss at step 4498: 0.07193483412265778
Loss at step 4499: 0.05646798014640808
Loss at step 4500: 0.007720313034951687
Loss at step 4501: 0.1490493267774582
Loss at step 4502: 0.07516629248857498
Loss at step 4503: 0.052404776215553284
Loss at step 4504: 0.21250571310520172
Loss at step 4505: 0.23835937678813934
Loss at step 4506: 0.02093871496617794
Loss at step 4507: 0.06704605370759964
Loss at step 4508: 0.3019545376300812
Loss at step 4509: 0.07420957833528519
Loss at step 4510: 0.023662671446800232
Loss at step 4511: 0.0950431078672409
Loss at step 4512: 0.14883935451507568
Loss at step 4513: 0.03463717922568321
Loss at step 4514: 0.025289621204137802
Loss at step 4515: 0.08205345273017883
Loss at step 4516: 0.15422768890857697
Loss at step 4517: 0.10196032375097275
Loss at step 4518: 0.4079042673110962
Loss at step 4519: 0.1755104809999466
Loss at step 4520: 0.40379786491394043
Loss at step 4521: 0.06128635257482529
Loss at step 4522: 0.029661012813448906
Loss at step 4523: 0.3134370446205139
Loss at step 4524: 0.11700379103422165
Loss at step 4525: 0.16191498935222626
Loss at step 4526: 0.08370216190814972
Loss at step 4527: 0.007166494149714708
Loss at step 4528: 0.11486586183309555
Loss at step 4529: 0.04922689497470856
Loss at step 4530: 0.15827389061450958
Loss at step 4531: 0.02912665344774723
Loss at step 4532: 0.04292469099164009
Loss at step 4533: 0.014426694251596928
Loss at step 4534: 0.1255740374326706
Loss at step 4535: 0.23756591975688934
Loss at step 4536: 0.24114876985549927
Loss at step 4537: 0.17488189041614532
Loss at step 4538: 0.06673779338598251
Loss at step 4539: 0.21031366288661957
Loss at step 4540: 0.10483008623123169
Loss at step 4541: 0.4101213216781616
Loss at step 4542: 0.054876092821359634
Loss at step 4543: 0.23822613060474396
Loss at step 4544: 0.03246619552373886
Loss at step 4545: 0.056366581469774246
Loss at step 4546: 0.3222754895687103
Loss at step 4547: 0.4524082541465759
Loss at step 4548: 0.21990154683589935
Loss at step 4549: 0.08705037832260132
Loss at step 4550: 0.39805373549461365
Loss at step 4551: 0.11339565366506577
Loss at step 4552: 0.16066740453243256
Loss at step 4553: 0.09372864663600922
Loss at step 4554: 0.08267612755298615
Loss at step 4555: 0.11058904230594635
Loss at step 4556: 0.1745539903640747
Loss at step 4557: 0.1303621232509613
Loss at step 4558: 0.08518850058317184
Loss at step 4559: 0.21778669953346252
Loss at step 4560: 0.410823255777359
Loss at step 4561: 0.17628106474876404
Loss at step 4562: 0.00619395961984992
Loss at step 4563: 0.3348413407802582
Loss at step 4564: 0.1637672334909439
Loss at step 4565: 0.4200662076473236
Loss at step 4566: 0.08356635272502899
Loss at step 4567: 0.31521254777908325
Loss at step 4568: 0.15326158702373505
Loss at step 4569: 0.26594337821006775
Loss at step 4570: 0.22435785830020905
Loss at step 4571: 0.01157369278371334
Loss at step 4572: 0.12788137793540955
Loss at step 4573: 0.050337906926870346
Loss at step 4574: 0.10369782894849777
Loss at step 4575: 0.3003062605857849
Loss at step 4576: 0.46407562494277954
Loss at step 4577: 0.06115275248885155
Loss at step 4578: 0.18776695430278778
Loss at step 4579: 0.2177920937538147
Loss at step 4580: 0.12685762345790863
Loss at step 4581: 0.042120400816202164
Loss at step 4582: 0.15596415102481842
Loss at step 4583: 0.1473926454782486
Loss at step 4584: 0.019894275814294815
Loss at step 4585: 0.024425402283668518
Loss at step 4586: 0.05371701717376709
Loss at step 4587: 0.02832520380616188
Loss at step 4588: 0.23161280155181885
Loss at step 4589: 0.008711136877536774
Loss at step 4590: 0.1833668053150177
Loss at step 4591: 0.05181027948856354
Loss at step 4592: 0.2622697353363037
Loss at step 4593: 0.1110479012131691
Loss at step 4594: 0.27614229917526245
Loss at step 4595: 0.11360084265470505
Loss at step 4596: 0.28205597400665283
Loss at step 4597: 0.36008110642433167
Loss at step 4598: 0.17379771173000336
Loss at step 4599: 0.13709507882595062
Saving training state...
[2025-08-02 14:59:55,174] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 15:00:01,670] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 4600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 4600: 0.1007082462310791
Loss at step 4601: 0.09555904567241669
Loss at step 4602: 0.03101428598165512
Loss at step 4603: 0.4619638919830322
Loss at step 4604: 0.0887279063463211
Loss at step 4605: 0.08171679079532623
Loss at step 4606: 0.1319725662469864
Loss at step 4607: 0.16791009902954102
Loss at step 4608: 0.04353529214859009
Loss at step 4609: 0.035757433623075485
Loss at step 4610: 0.3099063038825989
Loss at step 4611: 0.07286578416824341
Loss at step 4612: 0.15558505058288574
Loss at step 4613: 0.10860922932624817
Loss at step 4614: 0.046492621302604675
Loss at step 4615: 0.3474918603897095
Loss at step 4616: 0.15258771181106567
Loss at step 4617: 0.0
Loss at step 4618: 0.2413218468427658
Loss at step 4619: 0.22254231572151184
Loss at step 4620: 0.2804065942764282
Loss at step 4621: 0.09864785522222519
Loss at step 4622: 0.4066089391708374
Loss at step 4623: 0.3705107569694519
Loss at step 4624: 0.0961276963353157
Loss at step 4625: 0.1968909502029419
Loss at step 4626: 0.06789590418338776
Loss at step 4627: 0.29246985912323
Loss at step 4628: 0.054960157722234726
Loss at step 4629: 0.12184330075979233
Loss at step 4630: 0.21021604537963867
Loss at step 4631: 0.104358971118927
Loss at step 4632: 0.3945038914680481
Loss at step 4633: 0.029473397880792618
Loss at step 4634: 0.14074136316776276
Loss at step 4635: 0.02514457516372204
Loss at step 4636: 0.038021765649318695
Loss at step 4637: 0.2389046847820282
Loss at step 4638: 0.08098433166742325
Loss at step 4639: 0.47057273983955383
Loss at step 4640: 0.10838344693183899
Loss at step 4641: 0.1305985152721405
Loss at step 4642: 0.061305541545152664
Loss at step 4643: 0.0724552795290947
Loss at step 4644: 0.023020774126052856
Loss at step 4645: 0.06621938198804855
Loss at step 4646: 0.217532217502594
Loss at step 4647: 0.05862702056765556
Loss at step 4648: 0.050575140863657
Loss at step 4649: 0.012906241230666637
Loss at step 4650: 0.032278627157211304
Loss at step 4651: 0.1655007302761078
Loss at step 4652: 0.2506413757801056
Loss at step 4653: 0.5405648350715637
Loss at step 4654: 0.04726167023181915
Loss at step 4655: 0.06418658792972565
Loss at step 4656: 0.015799274668097496
Loss at step 4657: 0.03745622932910919
Loss at step 4658: 0.025616511702537537
Loss at step 4659: 0.07257974147796631
Loss at step 4660: 0.06178976595401764
Loss at step 4661: 0.01246566604822874
Loss at step 4662: 0.2124710977077484
Loss at step 4663: 0.17937175929546356
Loss at step 4664: 0.0552419051527977
Loss at step 4665: 0.06693699210882187
Loss at step 4666: 0.15591712296009064
Loss at step 4667: 0.3431885540485382
Loss at step 4668: 0.1390920877456665
Loss at step 4669: 0.5272319912910461
Loss at step 4670: 0.015746155753731728
Loss at step 4671: 0.10541731864213943
Loss at step 4672: 0.2849484980106354
Loss at step 4673: 0.053195253014564514
Loss at step 4674: 0.03659652918577194
Loss at step 4675: 0.14877347648143768
Loss at step 4676: 0.14831888675689697
Loss at step 4677: 0.14218196272850037
Loss at step 4678: 0.23526252806186676
Loss at step 4679: 0.03402973338961601
Loss at step 4680: 0.26425793766975403
Loss at step 4681: 0.4717237949371338
Loss at step 4682: 0.021420573815703392
Loss at step 4683: 0.05238258093595505
Loss at step 4684: 0.02272668108344078
Loss at step 4685: 0.3078879415988922
Loss at step 4686: 0.23820891976356506
Loss at step 4687: 0.1163761168718338
Loss at step 4688: 0.30542394518852234
Loss at step 4689: 0.09010288119316101
Loss at step 4690: 0.15304236114025116
Loss at step 4691: 0.12189044803380966
Loss at step 4692: 0.08082308620214462
Loss at step 4693: 0.025656497105956078
Loss at step 4694: 0.05131715536117554
Loss at step 4695: 0.05885045975446701
Loss at step 4696: 0.035289473831653595
Loss at step 4697: 0.32828712463378906
Loss at step 4698: 0.4960123598575592
Loss at step 4699: 0.2141406387090683
Loss at step 4700: 0.019572708755731583
Loss at step 4701: 0.11322782933712006
Loss at step 4702: 0.29341745376586914
Loss at step 4703: 0.02911255694925785
Loss at step 4704: 0.16955600678920746
Loss at step 4705: 0.1654505729675293
Loss at step 4706: 0.08262181282043457
Loss at step 4707: 0.29688602685928345
Loss at step 4708: 0.11138895899057388
Loss at step 4709: 0.2898723781108856
Loss at step 4710: 0.03149259462952614
Loss at step 4711: 0.056525617837905884
Loss at step 4712: 0.06900709867477417
Loss at step 4713: 0.19828756153583527
Loss at step 4714: 0.33986517786979675
Loss at step 4715: 0.15827026963233948
Loss at step 4716: 0.23994703590869904
Loss at step 4717: 0.050215307623147964
Loss at step 4718: 0.04073072597384453
Loss at step 4719: 0.43929746747016907
Loss at step 4720: 0.15889480710029602
Loss at step 4721: 0.4850793182849884
Loss at step 4722: 0.305686891078949
Loss at step 4723: 0.053458649665117264
Loss at step 4724: 0.11316487193107605
Loss at step 4725: 0.18098832666873932
Loss at step 4726: 0.08862966299057007
Loss at step 4727: 0.6230058670043945
Loss at step 4728: 0.38821056485176086
Loss at step 4729: 0.03896482288837433
Loss at step 4730: 0.11769969761371613
Loss at step 4731: 0.18687763810157776
Loss at step 4732: 0.04467476159334183
Loss at step 4733: 0.03981531783938408
Loss at step 4734: 0.06795435398817062
Loss at step 4735: 0.08331079035997391
Loss at step 4736: 0.1468973606824875
Loss at step 4737: 0.19226938486099243
Loss at step 4738: 0.15911389887332916
Loss at step 4739: 0.6346821188926697
Loss at step 4740: 0.21097984910011292
Loss at step 4741: 0.23834426701068878
Loss at step 4742: 0.3276185989379883
Loss at step 4743: 0.030803941190242767
Loss at step 4744: 0.21953359246253967
Loss at step 4745: 0.38045141100883484
Loss at step 4746: 0.13343487679958344
Loss at step 4747: 0.07659295946359634
Loss at step 4748: 0.20408882200717926
Loss at step 4749: 0.11459027230739594
Loss at step 4750: 0.5676276683807373
Loss at step 4751: 0.09646065533161163
Loss at step 4752: 0.1396777182817459
Loss at step 4753: 0.08997100591659546
Loss at step 4754: 0.1257643699645996
Loss at step 4755: 0.0858360230922699
Loss at step 4756: 0.1037982702255249
Loss at step 4757: 0.048200882971286774
Loss at step 4758: 0.07565870136022568
Loss at step 4759: 0.2549493610858917
Loss at step 4760: 0.30366089940071106
Loss at step 4761: 0.3506866991519928
Loss at step 4762: 0.3724428415298462
Loss at step 4763: 0.058532536029815674
Loss at step 4764: 0.22221392393112183
Loss at step 4765: 0.012712594121694565
Loss at step 4766: 0.10564177483320236
Loss at step 4767: 0.16548211872577667
Loss at step 4768: 0.10678482800722122
Loss at step 4769: 0.011090445332229137
Loss at step 4770: 0.22764505445957184
Loss at step 4771: 0.022880446165800095
Loss at step 4772: 0.05932459235191345
Loss at step 4773: 0.0677964836359024
Loss at step 4774: 0.033198509365320206
Loss at step 4775: 0.3585834205150604
Loss at step 4776: 0.1946115791797638
Loss at step 4777: 0.4326647222042084
Loss at step 4778: 0.12491686642169952
Loss at step 4779: 0.1539422869682312
Loss at step 4780: 0.03151291236281395
Loss at step 4781: 0.37645918130874634
Loss at step 4782: 0.046715784817934036
Loss at step 4783: 0.30973464250564575
Loss at step 4784: 0.33528172969818115
Loss at step 4785: 0.25984564423561096
Loss at step 4786: 0.036799103021621704
Loss at step 4787: 0.45054492354393005
Loss at step 4788: 0.09234961122274399
Loss at step 4789: 0.44390079379081726
Loss at step 4790: 0.06765724718570709
Loss at step 4791: 0.013820219784975052
Loss at step 4792: 0.33358034491539
Loss at step 4793: 0.06453273445367813
Loss at step 4794: 0.03472626581788063
Loss at step 4795: 0.30438432097435
Loss at step 4796: 0.027416368946433067
Loss at step 4797: 0.614020586013794
Loss at step 4798: 0.12463612854480743
Loss at step 4799: 0.05622873827815056
Saving training state...
[2025-08-02 16:11:49,621] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 16:11:56,096] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 4800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 4800: 0.12460281699895859
Loss at step 4801: 0.010653955861926079
Loss at step 4802: 0.20122165977954865
Loss at step 4803: 0.09208948910236359
Loss at step 4804: 0.3322964906692505
Loss at step 4805: 0.2674526870250702
Loss at step 4806: 0.022686636075377464
Loss at step 4807: 0.1566765159368515
Loss at step 4808: 0.08040398359298706
Loss at step 4809: 0.08724182099103928
Loss at step 4810: 0.25484228134155273
Loss at step 4811: 0.18571923673152924
Loss at step 4812: 0.06371279805898666
Loss at step 4813: 0.20548580586910248
Loss at step 4814: 0.1821819692850113
Loss at step 4815: 0.028584882616996765
Loss at step 4816: 0.022162023931741714
Loss at step 4817: 0.19531850516796112
Loss at step 4818: 0.5749267935752869
Loss at step 4819: 0.25069859623908997
Loss at step 4820: 0.05784854665398598
Loss at step 4821: 0.25035592913627625
Loss at step 4822: 0.05492902547121048
Loss at step 4823: 0.14241845905780792
Loss at step 4824: 0.016596108675003052
Loss at step 4825: 0.27955394983291626
Loss at step 4826: 0.07346897572278976
Loss at step 4827: 0.27967092394828796
Loss at step 4828: 0.1722220927476883
Loss at step 4829: 0.047450531274080276
Loss at step 4830: 0.14863334596157074
Loss at step 4831: 0.07421574741601944
Loss at step 4832: 0.04085958003997803
Loss at step 4833: 0.16591380536556244
Loss at step 4834: 0.12715159356594086
Loss at step 4835: 0.09567055851221085
Loss at step 4836: 0.0
Loss at step 4837: 0.1765316128730774
Loss at step 4838: 0.10186418890953064
Loss at step 4839: 0.045866869390010834
Loss at step 4840: 0.5284169912338257
Loss at step 4841: 0.2471119463443756
Loss at step 4842: 0.12610848248004913
Loss at step 4843: 0.15332497656345367
Loss at step 4844: 0.08865577727556229
Loss at step 4845: 0.28008556365966797
Loss at step 4846: 0.09631440043449402
Loss at step 4847: 0.046465083956718445
Loss at step 4848: 0.16929581761360168
Loss at step 4849: 0.15390346944332123
Loss at step 4850: 0.1981034278869629
Loss at step 4851: 0.05944241210818291
Loss at step 4852: 0.303677499294281
Loss at step 4853: 0.17907431721687317
Loss at step 4854: 0.04612887650728226
Loss at step 4855: 0.23790574073791504
Loss at step 4856: 0.10132486373186111
Loss at step 4857: 0.13526096940040588
Loss at step 4858: 0.08494701236486435
Loss at step 4859: 0.0
Loss at step 4860: 0.09446824342012405
Loss at step 4861: 0.11699485778808594
Loss at step 4862: 0.10554816573858261
Loss at step 4863: 0.04903730750083923
Loss at step 4864: 0.05920492485165596
Loss at step 4865: 0.0687788799405098
Loss at step 4866: 0.06842555850744247
Loss at step 4867: 0.0788436233997345
Loss at step 4868: 0.0
Loss at step 4869: 0.13336920738220215
Loss at step 4870: 0.039523325860500336
Loss at step 4871: 0.4696066379547119
Loss at step 4872: 0.04421047866344452
Loss at step 4873: 0.4011177718639374
Loss at step 4874: 0.33335426449775696
Loss at step 4875: 0.19939720630645752
Loss at step 4876: 0.2063218653202057
Loss at step 4877: 0.28357261419296265
Loss at step 4878: 0.24040593206882477
Loss at step 4879: 0.12784062325954437
Loss at step 4880: 0.14976432919502258
Loss at step 4881: 0.1351621150970459
Loss at step 4882: 0.05840371549129486
Loss at step 4883: 0.02902245707809925
Loss at step 4884: 0.42693260312080383
Loss at step 4885: 0.12441689521074295
Loss at step 4886: 0.09430617839097977
Loss at step 4887: 0.18818826973438263
Loss at step 4888: 0.06454285979270935
Loss at step 4889: 0.1426478624343872
Loss at step 4890: 0.007781777996569872
Loss at step 4891: 0.01727679744362831
Loss at step 4892: 0.2390267550945282
Loss at step 4893: 0.17387768626213074
Loss at step 4894: 0.5011340975761414
Loss at step 4895: 0.12483334541320801
Loss at step 4896: 0.03807888925075531
Loss at step 4897: 0.2447221577167511
Loss at step 4898: 0.3128412365913391
Loss at step 4899: 0.10903915017843246
Loss at step 4900: 0.12841181457042694
Loss at step 4901: 0.282772958278656
Loss at step 4902: 0.13475172221660614
Loss at step 4903: 0.09636182337999344
Loss at step 4904: 0.016129521653056145
Loss at step 4905: 0.028628624975681305
Loss at step 4906: 0.20744259655475616
Loss at step 4907: 0.07071354985237122
Loss at step 4908: 0.5504549741744995
Loss at step 4909: 0.061132412403821945
Loss at step 4910: 0.011115067638456821
Loss at step 4911: 0.1489538997411728
Loss at step 4912: 0.12535083293914795
Loss at step 4913: 0.014363160356879234
Loss at step 4914: 0.1399538815021515
Loss at step 4915: 0.07412269711494446
Loss at step 4916: 0.06527860462665558
Loss at step 4917: 0.3647153675556183
Loss at step 4918: 0.3118593096733093
Loss at step 4919: 0.16156147420406342
Loss at step 4920: 0.02749808318912983
Loss at step 4921: 0.19748321175575256
Loss at step 4922: 0.20440636575222015
Loss at step 4923: 0.10636134445667267
Loss at step 4924: 0.1366785317659378
Loss at step 4925: 0.2769063413143158
Loss at step 4926: 0.07847433537244797
Loss at step 4927: 0.03137495741248131
Loss at step 4928: 0.1681479513645172
Loss at step 4929: 0.08605543524026871
Loss at step 4930: 0.14368599653244019
Loss at step 4931: 0.035733677446842194
Loss at step 4932: 0.3621017634868622
Loss at step 4933: 0.039455264806747437
Loss at step 4934: 0.33761438727378845
Loss at step 4935: 0.3295237421989441
Loss at step 4936: 0.06922051310539246
Loss at step 4937: 0.0581955760717392
Loss at step 4938: 0.345692902803421
Loss at step 4939: 0.22294333577156067
Loss at step 4940: 0.22041912376880646
Loss at step 4941: 0.06371991336345673
Loss at step 4942: 0.12656652927398682
Loss at step 4943: 0.03678084537386894
Loss at step 4944: 0.10838892310857773
Loss at step 4945: 0.1439257115125656
Loss at step 4946: 0.08646449446678162
Loss at step 4947: 0.04135585203766823
Loss at step 4948: 0.014326817356050014
Loss at step 4949: 0.045062609016895294
Loss at step 4950: 0.06292889267206192
Loss at step 4951: 0.11795329302549362
Loss at step 4952: 0.0723714679479599
Loss at step 4953: 0.04883674904704094
Loss at step 4954: 0.06631165742874146
Loss at step 4955: 0.13462650775909424
Loss at step 4956: 0.40016743540763855
Loss at step 4957: 0.02895708754658699
Loss at step 4958: 0.10956530272960663
Loss at step 4959: 0.23867414891719818
Loss at step 4960: 0.1296899914741516
Loss at step 4961: 0.1679137647151947
Loss at step 4962: 0.4534304440021515
Loss at step 4963: 0.19205330312252045
Loss at step 4964: 0.034126102924346924
Loss at step 4965: 0.0969628393650055
Loss at step 4966: 0.043021317571401596
Loss at step 4967: 0.10359635949134827
Loss at step 4968: 0.1514964997768402
Loss at step 4969: 0.035527415573596954
Loss at step 4970: 0.05440179631114006
Loss at step 4971: 0.047366492450237274
Loss at step 4972: 0.014244583435356617
Loss at step 4973: 0.15060970187187195
Loss at step 4974: 0.10163436084985733
Loss at step 4975: 0.20913076400756836
Loss at step 4976: 0.3004749119281769
Loss at step 4977: 0.048812370747327805
Loss at step 4978: 0.08502761274576187
Loss at step 4979: 0.24278369545936584
Loss at step 4980: 0.08716867864131927
Loss at step 4981: 0.2783305048942566
Loss at step 4982: 0.21543654799461365
Loss at step 4983: 0.18059176206588745
Loss at step 4984: 0.050916511565446854
Loss at step 4985: 0.010107925161719322
Loss at step 4986: 0.08602163940668106
Loss at step 4987: 0.03907068818807602
Loss at step 4988: 0.08878976106643677
Loss at step 4989: 0.261704683303833
Loss at step 4990: 0.08818747848272324
Loss at step 4991: 0.12570719420909882
Loss at step 4992: 0.2413102388381958
Loss at step 4993: 0.10750040411949158
Loss at step 4994: 0.33927804231643677
Loss at step 4995: 0.16322378814220428
Loss at step 4996: 0.05914459377527237
Loss at step 4997: 0.11641833186149597
Loss at step 4998: 0.1251274198293686
Loss at step 4999: 0.09597523510456085
Saving training state...
[2025-08-02 17:25:03,624] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 17:25:09,946] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 5000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 5000: 0.010177682153880596
Loss at step 5001: 0.10400991141796112
Loss at step 5002: 0.21909818053245544
Loss at step 5003: 0.09355109930038452
Loss at step 5004: 0.12572962045669556
Loss at step 5005: 0.11467599123716354
Loss at step 5006: 0.06515950709581375
Loss at step 5007: 0.022404221817851067
Loss at step 5008: 0.19778822362422943
Loss at step 5009: 0.4214417338371277
Loss at step 5010: 0.19225414097309113
Loss at step 5011: 0.0418824665248394
Loss at step 5012: 0.021450228989124298
Loss at step 5013: 0.03995261341333389
Loss at step 5014: 0.12441626936197281
Loss at step 5015: 0.4708292782306671
Loss at step 5016: 0.08812740445137024
Loss at step 5017: 0.31917136907577515
Loss at step 5018: 0.15068697929382324
Loss at step 5019: 0.29782697558403015
Loss at step 5020: 0.05314521864056587
Loss at step 5021: 0.23221296072006226
Loss at step 5022: 0.09638672322034836
Loss at step 5023: 0.027606511488556862
Loss at step 5024: 0.3017981946468353
Loss at step 5025: 0.10702909529209137
Loss at step 5026: 0.09425631910562515
Loss at step 5027: 0.005355048459023237
Loss at step 5028: 0.04905319586396217
Loss at step 5029: 0.21358609199523926
Loss at step 5030: 0.009888756088912487
Loss at step 5031: 0.15584538877010345
Loss at step 5032: 0.019717980176210403
Loss at step 5033: 0.2222379446029663
Loss at step 5034: 0.06717153638601303
Loss at step 5035: 0.18150615692138672
Loss at step 5036: 0.07398717105388641
Loss at step 5037: 0.10689286887645721
Loss at step 5038: 0.055339548736810684
Loss at step 5039: 0.16621734201908112
Loss at step 5040: 0.1644020676612854
Loss at step 5041: 0.010128292255103588
Loss at step 5042: 0.02968810498714447
Loss at step 5043: 0.3796437084674835
Loss at step 5044: 0.08782409876585007
Loss at step 5045: 0.16540423035621643
Loss at step 5046: 0.3738165497779846
Loss at step 5047: 0.12890468537807465
Loss at step 5048: 0.07095444947481155
Loss at step 5049: 0.00902868714183569
Loss at step 5050: 0.13241031765937805
Loss at step 5051: 0.10078786313533783
Loss at step 5052: 0.43560126423835754
Loss at step 5053: 0.13725145161151886
Loss at step 5054: 0.4804466962814331
Loss at step 5055: 0.067598856985569
Loss at step 5056: 0.12450315058231354
Loss at step 5057: 0.28965991735458374
Loss at step 5058: 0.1496957689523697
Loss at step 5059: 0.2437177449464798
Loss at step 5060: 0.05851583182811737
Loss at step 5061: 0.10211897641420364
Loss at step 5062: 0.03445592150092125
Loss at step 5063: 0.04854526370763779
Loss at step 5064: 0.023841168731451035
Loss at step 5065: 0.00697555486112833
Loss at step 5066: 0.1314348578453064
Loss at step 5067: 0.18403390049934387
Loss at step 5068: 0.015405724756419659
Loss at step 5069: 0.31340017914772034
Loss at step 5070: 0.06128188967704773
Loss at step 5071: 0.1271069198846817
Loss at step 5072: 0.17300856113433838
Loss at step 5073: 0.12905825674533844
Loss at step 5074: 0.09637411683797836
Loss at step 5075: 0.12137831747531891
Loss at step 5076: 0.4240246117115021
Loss at step 5077: 0.11739474534988403
Loss at step 5078: 0.017539583146572113
Loss at step 5079: 0.1150769516825676
Loss at step 5080: 0.04938704892992973
Loss at step 5081: 0.12334977090358734
Loss at step 5082: 0.07988014072179794
Loss at step 5083: 0.059452444314956665
Loss at step 5084: 0.19754870235919952
Loss at step 5085: 0.15332812070846558
Loss at step 5086: 0.08633635938167572
Loss at step 5087: 0.3155789077281952
Loss at step 5088: 0.06471695005893707
Loss at step 5089: 0.43693265318870544
Loss at step 5090: 0.24744324386119843
Loss at step 5091: 0.01751820556819439
Loss at step 5092: 0.07410667091608047
Loss at step 5093: 0.15467646718025208
Loss at step 5094: 0.25773340463638306
Loss at step 5095: 0.30558422207832336
Loss at step 5096: 0.3342234790325165
Loss at step 5097: 0.1523771435022354
Loss at step 5098: 0.053920723497867584
Loss at step 5099: 0.09738851338624954
Loss at step 5100: 0.07723189145326614
Loss at step 5101: 0.10868154466152191
Loss at step 5102: 0.38274210691452026
Loss at step 5103: 0.12091989815235138
Loss at step 5104: 0.044072043150663376
Loss at step 5105: 0.19251377880573273
Loss at step 5106: 0.029875319451093674
Loss at step 5107: 0.5764403939247131
Loss at step 5108: 0.19038227200508118
Loss at step 5109: 0.4598638117313385
Loss at step 5110: 0.07273010164499283
Loss at step 5111: 0.19971753656864166
Loss at step 5112: 0.03654038906097412
Loss at step 5113: 0.3735315799713135
Loss at step 5114: 0.21984492242336273
Loss at step 5115: 0.054857708513736725
Loss at step 5116: 0.08265693485736847
Loss at step 5117: 0.15668365359306335
Loss at step 5118: 0.0067894961684942245
Loss at step 5119: 0.07213865220546722
Loss at step 5120: 0.15261483192443848
Loss at step 5121: 0.11358658969402313
Loss at step 5122: 0.08019369840621948
Loss at step 5123: 0.3212641179561615
Loss at step 5124: 0.022962646558880806
Loss at step 5125: 0.16928648948669434
Loss at step 5126: 0.10177720338106155
Loss at step 5127: 0.24963034689426422
Loss at step 5128: 0.1711444854736328
Loss at step 5129: 0.5597847700119019
Loss at step 5130: 0.0360407754778862
Loss at step 5131: 0.37128597497940063
Loss at step 5132: 0.01861824095249176
Loss at step 5133: 0.10506493598222733
Loss at step 5134: 0.02535766363143921
Loss at step 5135: 0.18780365586280823
Loss at step 5136: 0.017590133473277092
Loss at step 5137: 0.11733825504779816
Loss at step 5138: 0.10795697569847107
Loss at step 5139: 0.23120276629924774
Loss at step 5140: 0.10372203588485718
Loss at step 5141: 0.41060537099838257
Loss at step 5142: 0.031576454639434814
Loss at step 5143: 0.060692086815834045
Loss at step 5144: 0.2947040796279907
Loss at step 5145: 0.013185814023017883
Loss at step 5146: 0.07230479270219803
Loss at step 5147: 0.07784886658191681
Loss at step 5148: 0.037346865981817245
Loss at step 5149: 0.1076064258813858
Loss at step 5150: 0.004970370791852474
Loss at step 5151: 0.17822903394699097
Loss at step 5152: 0.25648269057273865
Loss at step 5153: 0.24593599140644073
Loss at step 5154: 0.3236943781375885
Loss at step 5155: 0.1336924433708191
Loss at step 5156: 0.07756055891513824
Loss at step 5157: 0.052531298249959946
Loss at step 5158: 0.16882264614105225
Loss at step 5159: 0.2723373472690582
Loss at step 5160: 0.044463444501161575
Loss at step 5161: 0.04616106301546097
Loss at step 5162: 0.0
Loss at step 5163: 0.3931657373905182
Loss at step 5164: 0.03348012641072273
Loss at step 5165: 0.23096460103988647
Loss at step 5166: 0.18160690367221832
Loss at step 5167: 0.3373871147632599
Loss at step 5168: 0.09689422696828842
Loss at step 5169: 0.30354201793670654
Loss at step 5170: 0.13324694335460663
Loss at step 5171: 0.16588935256004333
Loss at step 5172: 0.10820958763360977
Loss at step 5173: 0.044148147106170654
Loss at step 5174: 0.01724301651120186
Loss at step 5175: 0.10574744641780853
Loss at step 5176: 0.2087191343307495
Loss at step 5177: 0.3350822329521179
Loss at step 5178: 0.520964503288269
Loss at step 5179: 0.2311529964208603
Loss at step 5180: 0.1461353749036789
Loss at step 5181: 0.06936996430158615
Loss at step 5182: 0.2457939088344574
Loss at step 5183: 0.0944676622748375
Loss at step 5184: 0.10265068709850311
Loss at step 5185: 0.039225928485393524
Loss at step 5186: 0.007120517548173666
Loss at step 5187: 0.41665753722190857
Loss at step 5188: 0.047198329120874405
Loss at step 5189: 0.17792834341526031
Loss at step 5190: 0.14520011842250824
Loss at step 5191: 0.3125265836715698
Loss at step 5192: 0.05843197926878929
Loss at step 5193: 0.06576186418533325
Loss at step 5194: 0.11567630618810654
Loss at step 5195: 0.010918829590082169
Loss at step 5196: 0.05708722770214081
Loss at step 5197: 0.05039586126804352
Loss at step 5198: 0.1655588299036026
Loss at step 5199: 0.11582453548908234
Saving training state...
[2025-08-02 18:37:02,482] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 18:37:08,826] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 5200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 5200: 0.08946339786052704
Loss at step 5201: 0.01009775884449482
Loss at step 5202: 0.11732420325279236
Loss at step 5203: 0.17575590312480927
Loss at step 5204: 0.318103551864624
Loss at step 5205: 0.15198436379432678
Loss at step 5206: 0.10322489589452744
Loss at step 5207: 0.1998862475156784
Loss at step 5208: 0.3041711747646332
Loss at step 5209: 0.18655769526958466
Loss at step 5210: 0.28275421261787415
Loss at step 5211: 0.1482761651277542
Loss at step 5212: 0.3348224461078644
Loss at step 5213: 0.1408490538597107
Loss at step 5214: 0.1722354143857956
Loss at step 5215: 0.06210365146398544
Loss at step 5216: 0.04267483949661255
Loss at step 5217: 0.05967310816049576
Loss at step 5218: 0.18488653004169464
Loss at step 5219: 0.186386376619339
Loss at step 5220: 0.06782978773117065
Loss at step 5221: 0.09403577446937561
Loss at step 5222: 0.02983884885907173
Loss at step 5223: 0.35580435395240784
Loss at step 5224: 0.05946802720427513
Loss at step 5225: 0.1102302074432373
Loss at step 5226: 0.11199109256267548
Loss at step 5227: 0.12100306898355484
Loss at step 5228: 0.25658929347991943
Loss at step 5229: 0.04257452115416527
Loss at step 5230: 0.07843712717294693
Loss at step 5231: 0.0985264852643013
Loss at step 5232: 0.11475007981061935
Loss at step 5233: 0.24262632429599762
Loss at step 5234: 0.46904948353767395
Loss at step 5235: 0.30283764004707336
Loss at step 5236: 0.27236080169677734
Loss at step 5237: 0.336299329996109
Loss at step 5238: 0.03295701742172241
Loss at step 5239: 0.016234058886766434
Loss at step 5240: 0.31287258863449097
Loss at step 5241: 0.33226948976516724
Loss at step 5242: 0.2288448065519333
Loss at step 5243: 0.11155374348163605
Loss at step 5244: 0.3192916810512543
Loss at step 5245: 0.2340964823961258
Loss at step 5246: 0.31692928075790405
Loss at step 5247: 0.14161695539951324
Loss at step 5248: 0.3628477454185486
Loss at step 5249: 0.2545057237148285
Loss at step 5250: 0.01248922385275364
Loss at step 5251: 0.30311107635498047
Loss at step 5252: 0.0409170426428318
Loss at step 5253: 0.15359899401664734
Loss at step 5254: 0.12397658079862595
Loss at step 5255: 0.15760061144828796
Loss at step 5256: 0.17690393328666687
Loss at step 5257: 0.17502336204051971
Loss at step 5258: 0.017320476472377777
Loss at step 5259: 0.2890055179595947
Loss at step 5260: 0.10323810577392578
Loss at step 5261: 0.17061573266983032
Loss at step 5262: 0.5227332711219788
Loss at step 5263: 0.08476130664348602
Loss at step 5264: 0.017291635274887085
Loss at step 5265: 0.06625708937644958
Loss at step 5266: 0.1112154871225357
Loss at step 5267: 0.13047006726264954
Loss at step 5268: 0.1265653371810913
Loss at step 5269: 0.029966086149215698
Loss at step 5270: 0.034862738102674484
Loss at step 5271: 0.13283167779445648
Loss at step 5272: 0.3400431275367737
Loss at step 5273: 0.1581110805273056
Loss at step 5274: 0.08238807320594788
Loss at step 5275: 0.5098241567611694
Loss at step 5276: 0.13414545357227325
Loss at step 5277: 0.0754690170288086
Loss at step 5278: 0.024062277749180794
Loss at step 5279: 0.09541100263595581
Loss at step 5280: 0.1337755024433136
Loss at step 5281: 0.19778017699718475
Loss at step 5282: 0.1406833678483963
Loss at step 5283: 0.029845578595995903
Loss at step 5284: 0.18669019639492035
Loss at step 5285: 0.3962576985359192
Loss at step 5286: 0.04819151759147644
Loss at step 5287: 0.1929178684949875
Loss at step 5288: 0.03147011622786522
Loss at step 5289: 0.09776706993579865
Loss at step 5290: 0.3033122420310974
Loss at step 5291: 0.04993649199604988
Loss at step 5292: 0.1245163306593895
Loss at step 5293: 0.1589595079421997
Loss at step 5294: 0.20714238286018372
Loss at step 5295: 0.06469070911407471
Loss at step 5296: 0.0
Loss at step 5297: 0.419506311416626
Loss at step 5298: 0.09641848504543304
Loss at step 5299: 0.21737712621688843
Loss at step 5300: 0.21228811144828796
Loss at step 5301: 0.06705187261104584
Loss at step 5302: 0.30093321204185486
Loss at step 5303: 0.12641236186027527
Loss at step 5304: 0.21814654767513275
Loss at step 5305: 0.09117031842470169
Loss at step 5306: 0.07805061340332031
Loss at step 5307: 0.07962596416473389
Loss at step 5308: 0.02548965997993946
Loss at step 5309: 0.49439311027526855
Loss at step 5310: 0.1291946917772293
Loss at step 5311: 0.3034297525882721
Loss at step 5312: 0.17603538930416107
Loss at step 5313: 0.11120612174272537
Loss at step 5314: 0.08859529346227646
Loss at step 5315: 0.12950222194194794
Loss at step 5316: 0.2906073033809662
Loss at step 5317: 0.18067508935928345
Loss at step 5318: 0.0
Loss at step 5319: 0.2673128545284271
Loss at step 5320: 0.06957117468118668
Loss at step 5321: 0.5061305165290833
Loss at step 5322: 0.23807938396930695
Loss at step 5323: 0.10022059082984924
Loss at step 5324: 0.1619010865688324
Loss at step 5325: 0.04182376340031624
Loss at step 5326: 0.1802103966474533
Loss at step 5327: 0.2005918323993683
Loss at step 5328: 0.17593182623386383
Loss at step 5329: 0.06122094765305519
Loss at step 5330: 0.36422640085220337
Loss at step 5331: 0.05644600838422775
Loss at step 5332: 0.03009645827114582
Loss at step 5333: 0.0
Loss at step 5334: 0.03697657957673073
Loss at step 5335: 0.3985120952129364
Loss at step 5336: 0.06582263857126236
Loss at step 5337: 0.057572562247514725
Loss at step 5338: 0.08640899509191513
Loss at step 5339: 0.09413044899702072
Loss at step 5340: 0.10140388458967209
Loss at step 5341: 0.007239318918436766
Loss at step 5342: 0.02248530089855194
Loss at step 5343: 0.02732144482433796
Loss at step 5344: 0.16429702937602997
Loss at step 5345: 0.13639648258686066
Loss at step 5346: 0.37390223145484924
Loss at step 5347: 0.22990919649600983
Loss at step 5348: 0.021857066079974174
Loss at step 5349: 0.02628174237906933
Loss at step 5350: 0.10609190165996552
Loss at step 5351: 0.009936689399182796
Loss at step 5352: 0.08752898126840591
Loss at step 5353: 0.41876354813575745
Loss at step 5354: 0.08546663820743561
Loss at step 5355: 0.3881690502166748
Loss at step 5356: 0.0861017033457756
Loss at step 5357: 0.5699204206466675
Loss at step 5358: 0.16274598240852356
Loss at step 5359: 0.09385500103235245
Loss at step 5360: 0.01826123148202896
Loss at step 5361: 0.06658225506544113
Loss at step 5362: 0.4577394425868988
Loss at step 5363: 0.1817128211259842
Loss at step 5364: 0.07197889685630798
Loss at step 5365: 0.030376311391592026
Loss at step 5366: 0.016598880290985107
Loss at step 5367: 0.1348177194595337
Loss at step 5368: 0.17104661464691162
Loss at step 5369: 0.5822900533676147
Loss at step 5370: 0.09788625687360764
Loss at step 5371: 0.01223448384553194
Loss at step 5372: 0.0717465952038765
Loss at step 5373: 0.14861643314361572
Loss at step 5374: 0.055060841143131256
Loss at step 5375: 0.025206901133060455
Loss at step 5376: 0.06242027506232262
Loss at step 5377: 0.24115271866321564
Loss at step 5378: 0.011133610270917416
Loss at step 5379: 0.11513737589120865
Loss at step 5380: 0.09320986270904541
Loss at step 5381: 0.11400005221366882
Loss at step 5382: 0.13221287727355957
Loss at step 5383: 0.1088634803891182
Loss at step 5384: 0.05443326756358147
Loss at step 5385: 0.14809761941432953
Loss at step 5386: 0.013994376175105572
Loss at step 5387: 0.39973559975624084
Loss at step 5388: 0.22367052733898163
Loss at step 5389: 0.22459177672863007
Loss at step 5390: 0.15464088320732117
Loss at step 5391: 0.07233812659978867
Loss at step 5392: 0.1567387729883194
Loss at step 5393: 0.1291874498128891
Loss at step 5394: 0.02331145852804184
Loss at step 5395: 0.028708921745419502
Loss at step 5396: 0.10597186535596848
Loss at step 5397: 0.06800152361392975
Loss at step 5398: 0.17976757884025574
Loss at step 5399: 0.0575748048722744
Saving training state...
[2025-08-02 19:48:24,501] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 19:48:30,931] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 5400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 5400: 0.3006669282913208
Loss at step 5401: 0.3666859567165375
Loss at step 5402: 0.5579656958580017
Loss at step 5403: 0.2631656229496002
Loss at step 5404: 0.07926671206951141
Loss at step 5405: 0.1070847287774086
Loss at step 5406: 0.042978864163160324
Loss at step 5407: 0.021553639322519302
Loss at step 5408: 0.2107076495885849
Loss at step 5409: 0.22149504721164703
Loss at step 5410: 0.05335488170385361
Loss at step 5411: 0.3447395861148834
Loss at step 5412: 0.03459056466817856
Loss at step 5413: 0.40898141264915466
Loss at step 5414: 0.22515763342380524
Loss at step 5415: 0.19865623116493225
Loss at step 5416: 0.038160234689712524
Loss at step 5417: 0.08470669388771057
Loss at step 5418: 0.16337934136390686
Loss at step 5419: 0.4895908832550049
Loss at step 5420: 0.0723990872502327
Loss at step 5421: 0.02031157724559307
Loss at step 5422: 0.03130682185292244
Loss at step 5423: 0.0696110799908638
Loss at step 5424: 0.24140965938568115
Loss at step 5425: 0.007817943580448627
Loss at step 5426: 0.05835891142487526
Loss at step 5427: 0.06634561717510223
Loss at step 5428: 0.027095839381217957
Loss at step 5429: 0.009051445871591568
Loss at step 5430: 0.27571508288383484
Loss at step 5431: 0.09050809592008591
Loss at step 5432: 0.08680257201194763
Loss at step 5433: 0.016837330535054207
Loss at step 5434: 0.10884939134120941
Loss at step 5435: 0.08640953153371811
Loss at step 5436: 0.09564639627933502
Loss at step 5437: 0.010508636012673378
Loss at step 5438: 0.18187031149864197
Loss at step 5439: 0.0568658784031868
Loss at step 5440: 0.22813697159290314
Loss at step 5441: 0.06958142668008804
Loss at step 5442: 0.07602433115243912
Loss at step 5443: 0.38161277770996094
Loss at step 5444: 0.2765326499938965
Loss at step 5445: 0.10494665056467056
Loss at step 5446: 0.08380323648452759
Loss at step 5447: 0.07545709609985352
Loss at step 5448: 0.06600476056337357
Loss at step 5449: 0.04540382698178291
Loss at step 5450: 0.07616446912288666
Loss at step 5451: 0.09517726302146912
Loss at step 5452: 0.08468803018331528
Loss at step 5453: 0.11362335830926895
Loss at step 5454: 0.20874333381652832
Loss at step 5455: 0.16407553851604462
Loss at step 5456: 0.08951112627983093
Loss at step 5457: 0.3083129823207855
Loss at step 5458: 0.030163690447807312
Loss at step 5459: 0.06409741938114166
Loss at step 5460: 0.17002280056476593
Loss at step 5461: 0.08178028464317322
Loss at step 5462: 0.27148139476776123
Loss at step 5463: 0.02724996767938137
Loss at step 5464: 0.026547260582447052
Loss at step 5465: 0.14743049442768097
Loss at step 5466: 0.05478270724415779
Loss at step 5467: 0.2250908762216568
Loss at step 5468: 0.0553133524954319
Loss at step 5469: 0.1455318033695221
Loss at step 5470: 0.02581525593996048
Loss at step 5471: 0.060497839003801346
Loss at step 5472: 0.10684167593717575
Loss at step 5473: 0.056803684681653976
Loss at step 5474: 0.1975320428609848
Loss at step 5475: 0.0768311470746994
Loss at step 5476: 0.13610371947288513
Loss at step 5477: 0.05833031237125397
Loss at step 5478: 0.23205949366092682
Loss at step 5479: 0.17822830379009247
Loss at step 5480: 0.05173082649707794
Loss at step 5481: 0.08786207437515259
Loss at step 5482: 0.19708563387393951
Loss at step 5483: 0.023260746151208878
Loss at step 5484: 0.13907277584075928
Loss at step 5485: 0.07692697644233704
Loss at step 5486: 0.27128109335899353
Loss at step 5487: 0.060957010835409164
Loss at step 5488: 0.09797819703817368
Loss at step 5489: 0.33293190598487854
Loss at step 5490: 0.13416177034378052
Loss at step 5491: 0.06611363589763641
Loss at step 5492: 0.04212508350610733
Loss at step 5493: 0.08422134071588516
Loss at step 5494: 0.031650181859731674
Loss at step 5495: 0.10413376986980438
Loss at step 5496: 0.17877109348773956
Loss at step 5497: 0.03010394237935543
Loss at step 5498: 0.250318318605423
Loss at step 5499: 0.06269175559282303
Loss at step 5500: 0.30077728629112244
Loss at step 5501: 0.03876063600182533
Loss at step 5502: 0.30684611201286316
Loss at step 5503: 0.0563906654715538
Loss at step 5504: 0.3338749408721924
Loss at step 5505: 0.24019187688827515
Loss at step 5506: 0.046307727694511414
Loss at step 5507: 0.03295328840613365
Loss at step 5508: 0.02338121458888054
Loss at step 5509: 0.353149950504303
Loss at step 5510: 0.05143504962325096
Loss at step 5511: 0.23846523463726044
Loss at step 5512: 0.3338194489479065
Loss at step 5513: 0.0175662562251091
Loss at step 5514: 0.12456369400024414
Loss at step 5515: 0.07467936724424362
Loss at step 5516: 0.13084203004837036
Loss at step 5517: 0.06401541084051132
Loss at step 5518: 0.08225417882204056
Loss at step 5519: 0.09836926311254501
Loss at step 5520: 0.08971183747053146
Loss at step 5521: 0.06390764564275742
Loss at step 5522: 0.5408185124397278
Loss at step 5523: 0.4362325072288513
Loss at step 5524: 0.36911097168922424
Loss at step 5525: 0.20636077225208282
Loss at step 5526: 0.20695382356643677
Loss at step 5527: 0.4551214873790741
Loss at step 5528: 0.02844882756471634
Loss at step 5529: 0.009268496185541153
Loss at step 5530: 0.10304409265518188
Loss at step 5531: 0.21854662895202637
Loss at step 5532: 0.03414657711982727
Loss at step 5533: 0.1161564439535141
Loss at step 5534: 0.2319142073392868
Loss at step 5535: 0.12916472554206848
Loss at step 5536: 0.37140312790870667
Loss at step 5537: 0.07447858899831772
Loss at step 5538: 0.09709464013576508
Loss at step 5539: 0.08898693323135376
Loss at step 5540: 0.04880598187446594
Loss at step 5541: 0.01613234356045723
Loss at step 5542: 0.0771285891532898
Loss at step 5543: 0.1542738825082779
Loss at step 5544: 0.271768718957901
Loss at step 5545: 0.20696860551834106
Loss at step 5546: 0.048038579523563385
Loss at step 5547: 0.44246047735214233
Loss at step 5548: 0.12199313193559647
Loss at step 5549: 0.028663093224167824
Loss at step 5550: 0.20390281081199646
Loss at step 5551: 0.35724350810050964
Loss at step 5552: 0.3395853638648987
Loss at step 5553: 0.2037966102361679
Loss at step 5554: 0.18413080275058746
Loss at step 5555: 0.4713418483734131
Loss at step 5556: 0.043128304183483124
Loss at step 5557: 0.17025616765022278
Loss at step 5558: 0.22821848094463348
Loss at step 5559: 0.3309049904346466
Loss at step 5560: 0.1116778552532196
Loss at step 5561: 0.028902336955070496
Loss at step 5562: 0.23437029123306274
Loss at step 5563: 0.060277946293354034
Loss at step 5564: 0.05263186991214752
Loss at step 5565: 0.3221690058708191
Loss at step 5566: 0.036090102046728134
Loss at step 5567: 0.08012444525957108
Loss at step 5568: 0.04599859192967415
Loss at step 5569: 0.2749904990196228
Loss at step 5570: 0.17466199398040771
Loss at step 5571: 0.07437009364366531
Loss at step 5572: 0.12569719552993774
Loss at step 5573: 0.1926613748073578
Loss at step 5574: 0.17391595244407654
Loss at step 5575: 0.2039368748664856
Loss at step 5576: 0.27907875180244446
Loss at step 5577: 0.2812584638595581
Loss at step 5578: 0.05031254515051842
Loss at step 5579: 0.19707785546779633
Loss at step 5580: 0.3536210060119629
Loss at step 5581: 0.06208672374486923
Loss at step 5582: 0.05261976271867752
Loss at step 5583: 0.11563297361135483
Loss at step 5584: 0.22140544652938843
Loss at step 5585: 0.05924578383564949
Loss at step 5586: 0.24415190517902374
Loss at step 5587: 0.05741695687174797
Loss at step 5588: 0.08178842067718506
Loss at step 5589: 0.10854548960924149
Loss at step 5590: 0.05176664888858795
Loss at step 5591: 0.38121306896209717
Loss at step 5592: 0.04767277091741562
Loss at step 5593: 0.036836013197898865
Loss at step 5594: 0.23483307659626007
Loss at step 5595: 0.13403798639774323
Loss at step 5596: 0.03946451470255852
Loss at step 5597: 0.24459010362625122
Loss at step 5598: 0.039171211421489716
Loss at step 5599: 0.05100564286112785
Saving training state...
[2025-08-02 20:58:48,935] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 20:58:55,454] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 5600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 5600: 0.2633278965950012
Loss at step 5601: 0.1836429089307785
Loss at step 5602: 0.04840012639760971
Loss at step 5603: 0.14380259811878204
Loss at step 5604: 0.42272669076919556
Loss at step 5605: 0.15405185520648956
Loss at step 5606: 0.4019652307033539
Loss at step 5607: 0.08533041179180145
Loss at step 5608: 0.012158534489572048
Loss at step 5609: 0.5210492014884949
Loss at step 5610: 0.3896641731262207
Loss at step 5611: 0.15346954762935638
Loss at step 5612: 0.13022728264331818
Loss at step 5613: 0.0744561031460762
Loss at step 5614: 0.426692932844162
Loss at step 5615: 0.15232427418231964
Loss at step 5616: 0.05647032707929611
Loss at step 5617: 0.15239574015140533
Loss at step 5618: 0.23106035590171814
Loss at step 5619: 0.3006895184516907
Loss at step 5620: 0.050453633069992065
Loss at step 5621: 0.1594650000333786
Loss at step 5622: 0.10058917850255966
Loss at step 5623: 0.47704416513442993
Loss at step 5624: 0.039608925580978394
Loss at step 5625: 0.0465734489262104
Loss at step 5626: 0.14216691255569458
Loss at step 5627: 0.05689433217048645
Loss at step 5628: 0.5314458608627319
Loss at step 5629: 0.015311196446418762
Loss at step 5630: 0.017234796658158302
Loss at step 5631: 0.0738946795463562
Loss at step 5632: 0.15008117258548737
Loss at step 5633: 0.1436348706483841
Loss at step 5634: 0.1957545280456543
Loss at step 5635: 0.3956091105937958
Loss at step 5636: 0.052891165018081665
Loss at step 5637: 0.005022232886403799
Loss at step 5638: 0.2494644671678543
Loss at step 5639: 0.12656176090240479
Loss at step 5640: 0.09518885612487793
Loss at step 5641: 0.05613819137215614
Loss at step 5642: 0.09815827757120132
Loss at step 5643: 0.14756439626216888
Loss at step 5644: 0.07017897069454193
Loss at step 5645: 0.06311079114675522
Loss at step 5646: 0.07457047700881958
Loss at step 5647: 0.3254452645778656
Loss at step 5648: 0.15343233942985535
Loss at step 5649: 0.05362889543175697
Loss at step 5650: 0.45490312576293945
Loss at step 5651: 0.17190690338611603
Loss at step 5652: 0.14061395823955536
Loss at step 5653: 0.21852026879787445
Loss at step 5654: 0.042798202484846115
Loss at step 5655: 0.264497309923172
Loss at step 5656: 0.16406700015068054
Loss at step 5657: 0.0073637221939861774
Loss at step 5658: 0.08288305252790451
Loss at step 5659: 0.08228825032711029
Loss at step 5660: 0.11293038725852966
Loss at step 5661: 0.12820874154567719
Loss at step 5662: 0.06195857375860214
Loss at step 5663: 0.046771783381700516
Loss at step 5664: 0.12228384613990784
Loss at step 5665: 0.07873625308275223
Loss at step 5666: 0.018459197133779526
Loss at step 5667: 0.023975156247615814
Loss at step 5668: 0.07011967897415161
Loss at step 5669: 0.23314878344535828
Loss at step 5670: 0.16273406147956848
Loss at step 5671: 0.009755085222423077
Loss at step 5672: 0.23542821407318115
Loss at step 5673: 0.1129000261425972
Loss at step 5674: 0.26926395297050476
Loss at step 5675: 0.30019092559814453
Loss at step 5676: 0.25671201944351196
Loss at step 5677: 0.12713399529457092
Loss at step 5678: 0.1650693714618683
Loss at step 5679: 0.12144001573324203
Loss at step 5680: 0.3565342128276825
Loss at step 5681: 0.045550718903541565
Loss at step 5682: 0.013544149696826935
Loss at step 5683: 0.27350276708602905
Loss at step 5684: 0.02193417400121689
Loss at step 5685: 0.057015176862478256
Loss at step 5686: 0.10266253352165222
Loss at step 5687: 0.07202721387147903
Loss at step 5688: 0.09432189166545868
Loss at step 5689: 0.01334618590772152
Loss at step 5690: 0.3069545328617096
Loss at step 5691: 0.18214361369609833
Loss at step 5692: 0.1256382018327713
Loss at step 5693: 0.010835141874849796
Loss at step 5694: 0.16694368422031403
Loss at step 5695: 0.6061940789222717
Loss at step 5696: 0.4497527480125427
Loss at step 5697: 0.20209616422653198
Loss at step 5698: 0.076453797519207
Loss at step 5699: 0.16895107924938202
Loss at step 5700: 0.16556595265865326
Loss at step 5701: 0.11003055423498154
Loss at step 5702: 0.1812729388475418
Loss at step 5703: 0.0543767549097538
Loss at step 5704: 0.21025897562503815
Loss at step 5705: 0.03390056639909744
Loss at step 5706: 0.1849382221698761
Loss at step 5707: 0.1576279252767563
Loss at step 5708: 0.10774452984333038
Loss at step 5709: 0.29883718490600586
Loss at step 5710: 0.10674608498811722
Loss at step 5711: 0.15411297976970673
Loss at step 5712: 0.08415565639734268
Loss at step 5713: 0.015175632201135159
Loss at step 5714: 0.28487417101860046
Loss at step 5715: 0.1379585713148117
Loss at step 5716: 0.12114106118679047
Loss at step 5717: 0.090670146048069
Loss at step 5718: 0.22502802312374115
Loss at step 5719: 0.10160232335329056
Loss at step 5720: 0.3494470715522766
Loss at step 5721: 0.15992791950702667
Loss at step 5722: 0.33502960205078125
Loss at step 5723: 0.017061825841665268
Loss at step 5724: 0.2961222231388092
Loss at step 5725: 0.21865925192832947
Loss at step 5726: 0.058231502771377563
Loss at step 5727: 0.12959106266498566
Loss at step 5728: 0.24164679646492004
Loss at step 5729: 0.17689862847328186
Loss at step 5730: 0.21895831823349
Loss at step 5731: 0.15366433560848236
Loss at step 5732: 0.10778462141752243
Loss at step 5733: 0.3227907121181488
Loss at step 5734: 0.07655266672372818
Loss at step 5735: 0.1373099386692047
Loss at step 5736: 0.0846821740269661
Loss at step 5737: 0.030764203518629074
Loss at step 5738: 0.09473693370819092
Loss at step 5739: 0.4674936830997467
Loss at step 5740: 0.08226721733808517
Loss at step 5741: 0.14416080713272095
Loss at step 5742: 0.1546192169189453
Loss at step 5743: 0.012735034339129925
Loss at step 5744: 0.2721029222011566
Loss at step 5745: 0.12047065049409866
Loss at step 5746: 0.15001913905143738
Loss at step 5747: 0.2260550707578659
Loss at step 5748: 0.12063252925872803
Loss at step 5749: 0.25542810559272766
Loss at step 5750: 0.06052565574645996
Loss at step 5751: 0.005912602413445711
Loss at step 5752: 0.021301385015249252
Loss at step 5753: 0.03412651643157005
Loss at step 5754: 0.10641997307538986
Loss at step 5755: 0.027559751644730568
Loss at step 5756: 0.013613699935376644
Loss at step 5757: 0.17173975706100464
Loss at step 5758: 0.06777873635292053
Loss at step 5759: 0.005843158811330795
Loss at step 5760: 0.1454375833272934
Loss at step 5761: 0.17830486595630646
Loss at step 5762: 0.10725139826536179
Loss at step 5763: 0.05175888165831566
Loss at step 5764: 0.08357187360525131
Loss at step 5765: 0.04294475167989731
Loss at step 5766: 0.08939870446920395
Loss at step 5767: 0.07216238230466843
Loss at step 5768: 0.08467722684144974
Loss at step 5769: 0.4952944815158844
Loss at step 5770: 0.2146894484758377
Loss at step 5771: 0.17833729088306427
Loss at step 5772: 0.34210512042045593
Loss at step 5773: 0.07850565761327744
Loss at step 5774: 0.15747761726379395
Loss at step 5775: 0.21075130999088287
Loss at step 5776: 0.1739443987607956
Loss at step 5777: 0.19802112877368927
Loss at step 5778: 0.11632668226957321
Loss at step 5779: 0.20375105738639832
Loss at step 5780: 0.15151244401931763
Loss at step 5781: 0.36731693148612976
Loss at step 5782: 0.1105247214436531
Loss at step 5783: 0.20115479826927185
Loss at step 5784: 0.12997721135616302
Loss at step 5785: 0.36529654264450073
Loss at step 5786: 0.2632186710834503
Loss at step 5787: 0.06774839758872986
Loss at step 5788: 0.2000379115343094
Loss at step 5789: 0.0469360314309597
Loss at step 5790: 0.03142474591732025
Loss at step 5791: 0.1032242551445961
Loss at step 5792: 0.4731413722038269
Loss at step 5793: 0.34038054943084717
Loss at step 5794: 0.022846447303891182
Loss at step 5795: 0.07977936416864395
Loss at step 5796: 0.05717204511165619
Loss at step 5797: 0.5070951581001282
Loss at step 5798: 0.07875953614711761
Loss at step 5799: 0.1792953908443451
Saving training state...
[2025-08-02 22:09:39,782] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 22:09:46,353] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 5800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 5800: 0.054048843681812286
Loss at step 5801: 0.18498381972312927
Loss at step 5802: 0.46067503094673157
Loss at step 5803: 0.007325727492570877
Loss at step 5804: 0.1585387885570526
Loss at step 5805: 0.07965276390314102
Loss at step 5806: 0.17258934676647186
Loss at step 5807: 0.0
Loss at step 5808: 0.07470828294754028
Loss at step 5809: 0.03167975693941116
Loss at step 5810: 0.20340272784233093
Loss at step 5811: 0.12500245869159698
Loss at step 5812: 0.2031448781490326
Loss at step 5813: 0.6327603459358215
Loss at step 5814: 0.146108016371727
Loss at step 5815: 0.09540269523859024
Loss at step 5816: 0.3234543204307556
Loss at step 5817: 0.07517559826374054
Loss at step 5818: 0.21186141669750214
Loss at step 5819: 0.143006831407547
Loss at step 5820: 0.1319335550069809
Loss at step 5821: 0.06940469890832901
Loss at step 5822: 0.4440370500087738
Loss at step 5823: 0.15655478835105896
Loss at step 5824: 0.3106527626514435
Loss at step 5825: 0.5182904601097107
Loss at step 5826: 0.09914463013410568
Loss at step 5827: 0.07508030533790588
Loss at step 5828: 0.10506058484315872
Loss at step 5829: 0.022971466183662415
Loss at step 5830: 0.35303524136543274
Loss at step 5831: 0.09231612086296082
Loss at step 5832: 0.007997410371899605
Loss at step 5833: 0.16850771009922028
Loss at step 5834: 0.4641261100769043
Loss at step 5835: 0.09857313334941864
Loss at step 5836: 0.0804915726184845
Loss at step 5837: 0.055337242782115936
Loss at step 5838: 0.17966319620609283
Loss at step 5839: 0.31222400069236755
Loss at step 5840: 0.16032765805721283
Loss at step 5841: 0.30578622221946716
Loss at step 5842: 0.060238949954509735
Loss at step 5843: 0.10502003878355026
Loss at step 5844: 0.42586541175842285
Loss at step 5845: 0.10123957693576813
Loss at step 5846: 0.4421190321445465
Loss at step 5847: 0.012025886215269566
Loss at step 5848: 0.32120412588119507
Loss at step 5849: 0.1502285599708557
Loss at step 5850: 0.12080240994691849
Loss at step 5851: 0.08673898130655289
Loss at step 5852: 0.11392410844564438
Loss at step 5853: 0.28947335481643677
Loss at step 5854: 0.045300185680389404
Loss at step 5855: 0.10874375700950623
Loss at step 5856: 0.03400058299303055
Loss at step 5857: 0.28191402554512024
Loss at step 5858: 0.08228237926959991
Loss at step 5859: 0.031168553978204727
Loss at step 5860: 0.5139184594154358
Loss at step 5861: 0.007834669202566147
Loss at step 5862: 0.049255672842264175
Loss at step 5863: 0.019038304686546326
Loss at step 5864: 0.07095121592283249
Loss at step 5865: 0.35896772146224976
Loss at step 5866: 0.007118230685591698
Loss at step 5867: 0.05882301926612854
Loss at step 5868: 0.46084392070770264
Loss at step 5869: 0.028900697827339172
Loss at step 5870: 0.5532577037811279
Loss at step 5871: 0.013584915548563004
Loss at step 5872: 0.09435286372900009
Loss at step 5873: 0.12798349559307098
Loss at step 5874: 0.27030739188194275
Loss at step 5875: 0.14615052938461304
Loss at step 5876: 0.056255120784044266
Loss at step 5877: 0.14620181918144226
Loss at step 5878: 0.1572016477584839
Loss at step 5879: 0.029612595215439796
Loss at step 5880: 0.07344501465559006
Loss at step 5881: 0.25000596046447754
Loss at step 5882: 0.04267439246177673
Loss at step 5883: 0.0787397250533104
Loss at step 5884: 0.007365469355136156
Loss at step 5885: 0.05420861393213272
Loss at step 5886: 0.08383221924304962
Loss at step 5887: 0.09415537118911743
Loss at step 5888: 0.059792809188365936
Loss at step 5889: 0.0698910653591156
Loss at step 5890: 0.08660091459751129
Loss at step 5891: 0.05117214843630791
Loss at step 5892: 0.04880563169717789
Loss at step 5893: 0.02381081134080887
Loss at step 5894: 0.14351578056812286
Loss at step 5895: 0.030179649591445923
Loss at step 5896: 0.31350892782211304
Loss at step 5897: 0.11358560621738434
Loss at step 5898: 0.2289671003818512
Loss at step 5899: 0.06814578175544739
Loss at step 5900: 0.0
Loss at step 5901: 0.13113993406295776
Loss at step 5902: 0.06141418218612671
Loss at step 5903: 0.6240323781967163
Loss at step 5904: 0.09084057807922363
Loss at step 5905: 0.1793624311685562
Loss at step 5906: 0.14707839488983154
Loss at step 5907: 0.17066340148448944
Loss at step 5908: 0.14971323311328888
Loss at step 5909: 0.10256756097078323
Loss at step 5910: 0.1781230866909027
Loss at step 5911: 0.07508086413145065
Loss at step 5912: 0.1080927848815918
Loss at step 5913: 0.4175976812839508
Loss at step 5914: 0.4092976748943329
Loss at step 5915: 0.23807895183563232
Loss at step 5916: 0.09854535013437271
Loss at step 5917: 0.19337454438209534
Loss at step 5918: 0.03758757561445236
Loss at step 5919: 0.036877118051052094
Loss at step 5920: 0.5866517424583435
Loss at step 5921: 0.3544211983680725
Loss at step 5922: 0.012811564840376377
Loss at step 5923: 0.01576324738562107
Loss at step 5924: 0.31830984354019165
Loss at step 5925: 0.39726322889328003
Loss at step 5926: 0.1719420850276947
Loss at step 5927: 0.2029074877500534
Loss at step 5928: 0.3017027676105499
Loss at step 5929: 0.07831142097711563
Loss at step 5930: 0.0485730916261673
Loss at step 5931: 0.09708409756422043
Loss at step 5932: 0.5737540125846863
Loss at step 5933: 0.16520939767360687
Loss at step 5934: 0.07948915660381317
Loss at step 5935: 0.01565674878656864
Loss at step 5936: 0.5068937540054321
Loss at step 5937: 0.2091720700263977
Loss at step 5938: 0.0690726712346077
Loss at step 5939: 0.013783100992441177
Loss at step 5940: 0.3391343355178833
Loss at step 5941: 0.24141812324523926
Loss at step 5942: 0.34278160333633423
Loss at step 5943: 0.1970026046037674
Loss at step 5944: 0.055512528866529465
Loss at step 5945: 0.18631117045879364
Loss at step 5946: 0.18506425619125366
Loss at step 5947: 0.10930374264717102
Loss at step 5948: 0.24789302051067352
Loss at step 5949: 0.2586434483528137
Loss at step 5950: 0.1497505009174347
Loss at step 5951: 0.07478947192430496
Loss at step 5952: 0.05269390717148781
Loss at step 5953: 0.014029411599040031
Loss at step 5954: 0.045547883957624435
Loss at step 5955: 0.03525705635547638
Loss at step 5956: 0.17908066511154175
Loss at step 5957: 0.046175871044397354
Loss at step 5958: 0.08761779963970184
Loss at step 5959: 0.028579629957675934
Loss at step 5960: 0.0
Loss at step 5961: 0.022958841174840927
Loss at step 5962: 0.0
Loss at step 5963: 0.09135789424180984
Loss at step 5964: 0.1829698532819748
Loss at step 5965: 0.2895872890949249
Loss at step 5966: 0.25280624628067017
Loss at step 5967: 0.02954052947461605
Loss at step 5968: 0.131156325340271
Loss at step 5969: 0.3655347228050232
Loss at step 5970: 0.16608381271362305
Loss at step 5971: 0.07259833067655563
Loss at step 5972: 0.17417822778224945
Loss at step 5973: 0.08597839623689651
Loss at step 5974: 0.03959411382675171
Loss at step 5975: 0.15913666784763336
Loss at step 5976: 0.11708886921405792
Loss at step 5977: 0.03644884377717972
Loss at step 5978: 0.245112344622612
Loss at step 5979: 0.014996730722486973
Loss at step 5980: 0.024532100185751915
Loss at step 5981: 0.12826474010944366
Loss at step 5982: 0.036343466490507126
Loss at step 5983: 0.09454107284545898
Loss at step 5984: 0.20345550775527954
Loss at step 5985: 0.13677577674388885
Loss at step 5986: 0.032019514590501785
Loss at step 5987: 0.24322278797626495
Loss at step 5988: 0.08209959417581558
Loss at step 5989: 0.08339054882526398
Loss at step 5990: 0.0
Loss at step 5991: 0.06352682411670685
Loss at step 5992: 0.1458193063735962
Loss at step 5993: 0.4787280857563019
Loss at step 5994: 0.08362735062837601
Loss at step 5995: 0.141127809882164
Loss at step 5996: 0.028503837063908577
Loss at step 5997: 0.07091806828975677
Loss at step 5998: 0.1604762226343155
Loss at step 5999: 0.347231924533844
Saving training state...
[2025-08-02 23:20:25,271] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-02 23:20:31,831] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 6000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 6000: 0.049011774361133575
Loss at step 6001: 0.03149953857064247
Loss at step 6002: 0.09251999109983444
Loss at step 6003: 0.5600903630256653
Loss at step 6004: 0.15022002160549164
Loss at step 6005: 0.1636345088481903
Loss at step 6006: 0.2647249400615692
Loss at step 6007: 0.02042204886674881
Loss at step 6008: 0.12483628839254379
Loss at step 6009: 0.06150909140706062
Loss at step 6010: 0.12077534943819046
Loss at step 6011: 0.096354641020298
Loss at step 6012: 0.32537174224853516
Loss at step 6013: 0.03288588672876358
Loss at step 6014: 0.038460489362478256
Loss at step 6015: 0.15998327732086182
Loss at step 6016: 0.09244808554649353
Loss at step 6017: 0.035135697573423386
Loss at step 6018: 0.152312770485878
Loss at step 6019: 0.11644086986780167
Loss at step 6020: 0.050192806869745255
Loss at step 6021: 0.026721389964222908
Loss at step 6022: 0.1983053833246231
Loss at step 6023: 0.11705923825502396
Loss at step 6024: 0.099281445145607
Loss at step 6025: 0.07723702490329742
Loss at step 6026: 0.028140963986516
Loss at step 6027: 0.6711452007293701
Loss at step 6028: 0.15841005742549896
Loss at step 6029: 0.10722704231739044
Loss at step 6030: 0.1340443640947342
Loss at step 6031: 0.10445491224527359
Loss at step 6032: 0.12283424288034439
Loss at step 6033: 0.3444965183734894
Loss at step 6034: 0.06591582298278809
Loss at step 6035: 0.06826699525117874
Loss at step 6036: 0.2719809412956238
Loss at step 6037: 0.3126150667667389
Loss at step 6038: 0.3006323575973511
Loss at step 6039: 0.0704607218503952
Loss at step 6040: 0.4684509336948395
Loss at step 6041: 0.07053349167108536
Loss at step 6042: 0.4230407476425171
Loss at step 6043: 0.1431150585412979
Loss at step 6044: 0.013890369795262814
Loss at step 6045: 0.11921674013137817
Loss at step 6046: 0.08277475833892822
Loss at step 6047: 0.22602291405200958
Loss at step 6048: nan
Loss at step 6049: nan
Loss at step 6050: nan
Loss at step 6051: nan
Loss at step 6052: nan
Loss at step 6053: nan
Loss at step 6054: nan
Loss at step 6055: nan
Loss at step 6056: nan
Loss at step 6057: nan
Loss at step 6058: nan
Loss at step 6059: nan
Loss at step 6060: nan
Loss at step 6061: nan
Loss at step 6062: nan
Loss at step 6063: nan
Loss at step 6064: nan
Loss at step 6065: nan
Loss at step 6066: nan
Loss at step 6067: nan
Loss at step 6068: nan
Loss at step 6069: nan
Loss at step 6070: nan
Loss at step 6071: nan
Loss at step 6072: nan
Loss at step 6073: nan
Loss at step 6074: nan
Loss at step 6075: nan
Loss at step 6076: nan
Loss at step 6077: nan
Loss at step 6078: nan
Loss at step 6079: nan
Loss at step 6080: nan
Loss at step 6081: nan
Loss at step 6082: nan
Loss at step 6083: nan
Loss at step 6084: nan
Loss at step 6085: nan
Loss at step 6086: nan
Loss at step 6087: nan
Loss at step 6088: nan
Loss at step 6089: nan
Loss at step 6090: nan
Loss at step 6091: nan
Loss at step 6092: nan
Loss at step 6093: nan
Loss at step 6094: nan
Loss at step 6095: nan
Loss at step 6096: nan
Loss at step 6097: nan
Loss at step 6098: nan
Loss at step 6099: nan
Loss at step 6100: nan
Loss at step 6101: nan
Loss at step 6102: nan
Loss at step 6103: nan
Loss at step 6104: nan
Loss at step 6105: nan
Loss at step 6106: nan
Loss at step 6107: nan
Loss at step 6108: nan
Loss at step 6109: nan
Loss at step 6110: nan
Loss at step 6111: nan
Loss at step 6112: nan
Loss at step 6113: nan
Loss at step 6114: nan
Loss at step 6115: nan
Loss at step 6116: nan
Loss at step 6117: nan
Loss at step 6118: nan
Loss at step 6119: nan
Loss at step 6120: nan
Loss at step 6121: nan
Loss at step 6122: nan
Loss at step 6123: nan
Loss at step 6124: nan
Loss at step 6125: nan
Loss at step 6126: nan
Loss at step 6127: nan
Loss at step 6128: nan
Loss at step 6129: nan
Loss at step 6130: nan
Loss at step 6131: nan
Loss at step 6132: nan
Loss at step 6133: nan
Loss at step 6134: nan
Loss at step 6135: nan
Loss at step 6136: nan
Loss at step 6137: nan
Loss at step 6138: nan
Loss at step 6139: nan
Loss at step 6140: nan
Loss at step 6141: nan
Loss at step 6142: nan
Loss at step 6143: nan
Loss at step 6144: nan
Loss at step 6145: nan
Loss at step 6146: nan
Loss at step 6147: nan
Loss at step 6148: nan
Loss at step 6149: nan
Loss at step 6150: nan
Loss at step 6151: nan
Loss at step 6152: nan
Loss at step 6153: nan
Loss at step 6154: nan
Loss at step 6155: nan
Loss at step 6156: nan
Loss at step 6157: nan
Loss at step 6158: nan
Loss at step 6159: nan
Loss at step 6160: nan
Loss at step 6161: nan
Loss at step 6162: nan
Loss at step 6163: nan
Loss at step 6164: nan
Loss at step 6165: nan
Loss at step 6166: nan
Loss at step 6167: nan
Loss at step 6168: nan
Loss at step 6169: nan
Loss at step 6170: nan
Loss at step 6171: nan
Loss at step 6172: nan
Loss at step 6173: nan
Loss at step 6174: nan
Loss at step 6175: nan
Loss at step 6176: nan
Loss at step 6177: nan
Loss at step 6178: nan
Loss at step 6179: nan
Loss at step 6180: nan
Loss at step 6181: nan
Loss at step 6182: nan
Loss at step 6183: nan
Loss at step 6184: nan
Loss at step 6185: nan
Loss at step 6186: nan
Loss at step 6187: nan
Loss at step 6188: nan
Loss at step 6189: nan
Loss at step 6190: nan
Loss at step 6191: nan
Loss at step 6192: nan
Loss at step 6193: nan
Loss at step 6194: nan
Loss at step 6195: nan
Loss at step 6196: nan
Loss at step 6197: nan
Loss at step 6198: nan
Loss at step 6199: nan
Saving training state...
[2025-08-03 00:28:32,659] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 00:28:39,185] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 6200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 6200: nan
Loss at step 6201: nan
Loss at step 6202: nan
Loss at step 6203: nan
Loss at step 6204: nan
Loss at step 6205: nan
Loss at step 6206: nan
Loss at step 6207: nan
Loss at step 6208: nan
Loss at step 6209: nan
Loss at step 6210: nan
Loss at step 6211: nan
Loss at step 6212: nan
Loss at step 6213: nan
Loss at step 6214: nan
Loss at step 6215: nan
Loss at step 6216: nan
Loss at step 6217: nan
Loss at step 6218: nan
Loss at step 6219: nan
Loss at step 6220: nan
Loss at step 6221: nan
Loss at step 6222: nan
Loss at step 6223: nan
Loss at step 6224: nan
Loss at step 6225: nan
Loss at step 6226: nan
Loss at step 6227: nan
Loss at step 6228: nan
Loss at step 6229: nan
Loss at step 6230: nan
Loss at step 6231: nan
Loss at step 6232: nan
Loss at step 6233: nan
Loss at step 6234: nan
Loss at step 6235: nan
Loss at step 6236: nan
Loss at step 6237: nan
Loss at step 6238: nan
Loss at step 6239: nan
Loss at step 6240: nan
Loss at step 6241: nan
Loss at step 6242: nan
Loss at step 6243: nan
Loss at step 6244: nan
Loss at step 6245: nan
Loss at step 6246: nan
Loss at step 6247: nan
Loss at step 6248: nan
Loss at step 6249: nan
Loss at step 6250: nan
Loss at step 6251: nan
Loss at step 6252: nan
Loss at step 6253: nan
Loss at step 6254: nan
Loss at step 6255: nan
Loss at step 6256: nan
Loss at step 6257: nan
Loss at step 6258: nan
Loss at step 6259: nan
Loss at step 6260: nan
Loss at step 6261: nan
Loss at step 6262: nan
Loss at step 6263: nan
Loss at step 6264: nan
Loss at step 6265: nan
Loss at step 6266: nan
Loss at step 6267: nan
Loss at step 6268: nan
Loss at step 6269: nan
Loss at step 6270: nan
Loss at step 6271: nan
Loss at step 6272: nan
Loss at step 6273: nan
Loss at step 6274: nan
Loss at step 6275: nan
Loss at step 6276: nan
Loss at step 6277: nan
Loss at step 6278: nan
Loss at step 6279: nan
Loss at step 6280: nan
Loss at step 6281: nan
Loss at step 6282: nan
Loss at step 6283: nan
Loss at step 6284: nan
Loss at step 6285: nan
Loss at step 6286: nan
Loss at step 6287: nan
Loss at step 6288: nan
Loss at step 6289: nan
Loss at step 6290: nan
Loss at step 6291: nan
Loss at step 6292: nan
Loss at step 6293: nan
Loss at step 6294: nan
Loss at step 6295: nan
Loss at step 6296: nan
Loss at step 6297: nan
Loss at step 6298: nan
Loss at step 6299: nan
Loss at step 6300: nan
Loss at step 6301: nan
Loss at step 6302: nan
Loss at step 6303: nan
Loss at step 6304: nan
Loss at step 6305: nan
Loss at step 6306: nan
Loss at step 6307: nan
Loss at step 6308: nan
Loss at step 6309: nan
Loss at step 6310: nan
Loss at step 6311: nan
Loss at step 6312: nan
Loss at step 6313: nan
Loss at step 6314: nan
Loss at step 6315: nan
Loss at step 6316: nan
Loss at step 6317: nan
Loss at step 6318: nan
Loss at step 6319: nan
Loss at step 6320: nan
Loss at step 6321: nan
Loss at step 6322: nan
Loss at step 6323: nan
Loss at step 6324: nan
Loss at step 6325: nan
Loss at step 6326: nan
Loss at step 6327: nan
Loss at step 6328: nan
Loss at step 6329: nan
Loss at step 6330: nan
Loss at step 6331: nan
Loss at step 6332: nan
Loss at step 6333: nan
Loss at step 6334: nan
Loss at step 6335: nan
Loss at step 6336: nan
Loss at step 6337: nan
Loss at step 6338: nan
Loss at step 6339: nan
Loss at step 6340: nan
Loss at step 6341: nan
Loss at step 6342: nan
Loss at step 6343: nan
Loss at step 6344: nan
Loss at step 6345: nan
Loss at step 6346: nan
Loss at step 6347: nan
Loss at step 6348: nan
Loss at step 6349: nan
Loss at step 6350: nan
Loss at step 6351: nan
Loss at step 6352: nan
Loss at step 6353: nan
Loss at step 6354: nan
Loss at step 6355: nan
Loss at step 6356: nan
Loss at step 6357: nan
Loss at step 6358: nan
Loss at step 6359: nan
Loss at step 6360: nan
Loss at step 6361: nan
Loss at step 6362: nan
Loss at step 6363: nan
Loss at step 6364: nan
Loss at step 6365: nan
Loss at step 6366: nan
Loss at step 6367: nan
Loss at step 6368: nan
Loss at step 6369: nan
Loss at step 6370: nan
Loss at step 6371: nan
Loss at step 6372: nan
Loss at step 6373: nan
Loss at step 6374: nan
Loss at step 6375: nan
Loss at step 6376: nan
Loss at step 6377: nan
Loss at step 6378: nan
Loss at step 6379: nan
Loss at step 6380: nan
Loss at step 6381: nan
Loss at step 6382: nan
Loss at step 6383: nan
Loss at step 6384: nan
Loss at step 6385: nan
Loss at step 6386: nan
Loss at step 6387: nan
Loss at step 6388: nan
Loss at step 6389: nan
Loss at step 6390: nan
Loss at step 6391: nan
Loss at step 6392: nan
Loss at step 6393: nan
Loss at step 6394: nan
Loss at step 6395: nan
Loss at step 6396: nan
Loss at step 6397: nan
Loss at step 6398: nan
Loss at step 6399: nan
Saving training state...
[2025-08-03 01:37:12,672] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 01:37:19,151] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 6400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 6400: nan
Loss at step 6401: nan
Loss at step 6402: nan
Loss at step 6403: nan
Loss at step 6404: nan
Loss at step 6405: nan
Loss at step 6406: nan
Loss at step 6407: nan
Loss at step 6408: nan
Loss at step 6409: nan
Loss at step 6410: nan
Loss at step 6411: nan
Loss at step 6412: nan
Loss at step 6413: nan
Loss at step 6414: nan
Loss at step 6415: nan
Loss at step 6416: nan
Loss at step 6417: nan
Loss at step 6418: nan
Loss at step 6419: nan
Loss at step 6420: nan
Loss at step 6421: nan
Loss at step 6422: nan
Loss at step 6423: nan
Loss at step 6424: nan
Loss at step 6425: nan
Loss at step 6426: nan
Loss at step 6427: nan
Loss at step 6428: nan
Loss at step 6429: nan
Loss at step 6430: nan
Loss at step 6431: nan
Loss at step 6432: nan
Loss at step 6433: nan
Loss at step 6434: nan
Loss at step 6435: nan
Loss at step 6436: nan
Loss at step 6437: nan
Loss at step 6438: nan
Loss at step 6439: nan
Loss at step 6440: nan
Loss at step 6441: nan
Loss at step 6442: nan
Loss at step 6443: nan
Loss at step 6444: nan
Loss at step 6445: nan
Loss at step 6446: nan
Loss at step 6447: nan
Loss at step 6448: nan
Loss at step 6449: nan
Loss at step 6450: nan
Loss at step 6451: nan
Loss at step 6452: nan
Loss at step 6453: nan
Loss at step 6454: nan
Loss at step 6455: nan
Loss at step 6456: nan
Loss at step 6457: nan
Loss at step 6458: nan
Loss at step 6459: nan
Loss at step 6460: nan
Loss at step 6461: nan
Loss at step 6462: nan
Loss at step 6463: nan
Loss at step 6464: nan
Loss at step 6465: nan
Loss at step 6466: nan
Loss at step 6467: nan
Loss at step 6468: nan
Loss at step 6469: nan
Loss at step 6470: nan
Loss at step 6471: nan
Loss at step 6472: nan
Loss at step 6473: nan
Loss at step 6474: nan
Loss at step 6475: nan
Loss at step 6476: nan
Loss at step 6477: nan
Loss at step 6478: nan
Loss at step 6479: nan
Loss at step 6480: nan
Loss at step 6481: nan
Loss at step 6482: nan
Loss at step 6483: nan
Loss at step 6484: nan
Loss at step 6485: nan
Loss at step 6486: nan
Loss at step 6487: nan
Loss at step 6488: nan
Loss at step 6489: nan
Loss at step 6490: nan
Loss at step 6491: nan
Loss at step 6492: nan
Loss at step 6493: nan
Loss at step 6494: nan
Loss at step 6495: nan
Loss at step 6496: nan
Loss at step 6497: nan
Loss at step 6498: nan
Loss at step 6499: nan
Loss at step 6500: nan
Loss at step 6501: nan
Loss at step 6502: nan
Loss at step 6503: nan
Loss at step 6504: nan
Loss at step 6505: nan
Loss at step 6506: nan
Loss at step 6507: nan
Loss at step 6508: nan
Loss at step 6509: nan
Loss at step 6510: nan
Loss at step 6511: nan
Loss at step 6512: nan
Loss at step 6513: nan
Loss at step 6514: nan
Loss at step 6515: nan
Loss at step 6516: nan
Loss at step 6517: nan
Loss at step 6518: nan
Loss at step 6519: nan
Loss at step 6520: nan
Loss at step 6521: nan
Loss at step 6522: nan
Loss at step 6523: nan
Loss at step 6524: nan
Loss at step 6525: nan
Loss at step 6526: nan
Loss at step 6527: nan
Loss at step 6528: nan
Loss at step 6529: nan
Loss at step 6530: nan
Loss at step 6531: nan
Loss at step 6532: nan
Loss at step 6533: nan
Loss at step 6534: nan
Loss at step 6535: nan
Loss at step 6536: nan
Loss at step 6537: nan
Loss at step 6538: nan
Loss at step 6539: nan
Loss at step 6540: nan
Loss at step 6541: nan
Loss at step 6542: nan
Loss at step 6543: nan
Loss at step 6544: nan
Loss at step 6545: nan
Loss at step 6546: nan
Loss at step 6547: nan
Loss at step 6548: nan
Loss at step 6549: nan
Loss at step 6550: nan
Loss at step 6551: nan
Loss at step 6552: nan
Loss at step 6553: nan
Loss at step 6554: nan
Loss at step 6555: nan
Loss at step 6556: nan
Loss at step 6557: nan
Loss at step 6558: nan
Loss at step 6559: nan
Loss at step 6560: nan
Loss at step 6561: nan
Loss at step 6562: nan
Loss at step 6563: nan
Loss at step 6564: nan
Loss at step 6565: nan
Loss at step 6566: nan
Loss at step 6567: nan
Loss at step 6568: nan
Loss at step 6569: nan
Loss at step 6570: nan
Loss at step 6571: nan
Loss at step 6572: nan
Loss at step 6573: nan
Loss at step 6574: nan
Loss at step 6575: nan
Loss at step 6576: nan
Loss at step 6577: nan
Loss at step 6578: nan
Loss at step 6579: nan
Loss at step 6580: nan
Loss at step 6581: nan
Loss at step 6582: nan
Loss at step 6583: nan
Loss at step 6584: nan
Loss at step 6585: nan
Loss at step 6586: nan
Loss at step 6587: nan
Loss at step 6588: nan
Loss at step 6589: nan
Loss at step 6590: nan
Loss at step 6591: nan
Loss at step 6592: nan
Loss at step 6593: nan
Loss at step 6594: nan
Loss at step 6595: nan
Loss at step 6596: nan
Loss at step 6597: nan
Loss at step 6598: nan
Loss at step 6599: nan
Saving training state...
[2025-08-03 02:46:10,013] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 02:46:16,423] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 6600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 6600: nan
Loss at step 6601: nan
Loss at step 6602: nan
Loss at step 6603: nan
Loss at step 6604: nan
Loss at step 6605: nan
Loss at step 6606: nan
Loss at step 6607: nan
Loss at step 6608: nan
Loss at step 6609: nan
Loss at step 6610: nan
Loss at step 6611: nan
Loss at step 6612: nan
Loss at step 6613: nan
Loss at step 6614: nan
Loss at step 6615: nan
Loss at step 6616: nan
Loss at step 6617: nan
Loss at step 6618: nan
Loss at step 6619: nan
Loss at step 6620: nan
Loss at step 6621: nan
Loss at step 6622: nan
Loss at step 6623: nan
Loss at step 6624: nan
Loss at step 6625: nan
Loss at step 6626: nan
Loss at step 6627: nan
Loss at step 6628: nan
Loss at step 6629: nan
Loss at step 6630: nan
Loss at step 6631: nan
Loss at step 6632: nan
Loss at step 6633: nan
Loss at step 6634: nan
Loss at step 6635: nan
Loss at step 6636: nan
Loss at step 6637: nan
Loss at step 6638: nan
Loss at step 6639: nan
Loss at step 6640: nan
Loss at step 6641: nan
Loss at step 6642: nan
Loss at step 6643: nan
Loss at step 6644: nan
Loss at step 6645: nan
Loss at step 6646: nan
Loss at step 6647: nan
Loss at step 6648: nan
Loss at step 6649: nan
Loss at step 6650: nan
Loss at step 6651: nan
Loss at step 6652: nan
Loss at step 6653: nan
Loss at step 6654: nan
Loss at step 6655: nan
Loss at step 6656: nan
Loss at step 6657: nan
Loss at step 6658: nan
Loss at step 6659: nan
Loss at step 6660: nan
Loss at step 6661: nan
Loss at step 6662: nan
Loss at step 6663: nan
Loss at step 6664: nan
Loss at step 6665: nan
Loss at step 6666: nan
Loss at step 6667: nan
Loss at step 6668: nan
Loss at step 6669: nan
Loss at step 6670: nan
Loss at step 6671: nan
Loss at step 6672: nan
Loss at step 6673: nan
Loss at step 6674: nan
Loss at step 6675: nan
Loss at step 6676: nan
Loss at step 6677: nan
Loss at step 6678: nan
Loss at step 6679: nan
Loss at step 6680: nan
Loss at step 6681: nan
Loss at step 6682: nan
Loss at step 6683: nan
Loss at step 6684: nan
Loss at step 6685: nan
Loss at step 6686: nan
Loss at step 6687: nan
Loss at step 6688: nan
Loss at step 6689: nan
Loss at step 6690: nan
Loss at step 6691: nan
Loss at step 6692: nan
Loss at step 6693: nan
Loss at step 6694: nan
Loss at step 6695: nan
Loss at step 6696: nan
Loss at step 6697: nan
Loss at step 6698: nan
Loss at step 6699: nan
Loss at step 6700: nan
Loss at step 6701: nan
Loss at step 6702: nan
Loss at step 6703: nan
Loss at step 6704: nan
Loss at step 6705: nan
Loss at step 6706: nan
Loss at step 6707: nan
Loss at step 6708: nan
Loss at step 6709: nan
Loss at step 6710: nan
Loss at step 6711: nan
Loss at step 6712: nan
Loss at step 6713: nan
Loss at step 6714: nan
Loss at step 6715: nan
Loss at step 6716: nan
Loss at step 6717: nan
Loss at step 6718: nan
Loss at step 6719: nan
Loss at step 6720: nan
Loss at step 6721: nan
Loss at step 6722: nan
Loss at step 6723: nan
Loss at step 6724: nan
Loss at step 6725: nan
Loss at step 6726: nan
Loss at step 6727: nan
Loss at step 6728: nan
Loss at step 6729: nan
Loss at step 6730: nan
Loss at step 6731: nan
Loss at step 6732: nan
Loss at step 6733: nan
Loss at step 6734: nan
Loss at step 6735: nan
Loss at step 6736: nan
Loss at step 6737: nan
Loss at step 6738: nan
Loss at step 6739: nan
Loss at step 6740: nan
Loss at step 6741: nan
Loss at step 6742: nan
Loss at step 6743: nan
Loss at step 6744: nan
Loss at step 6745: nan
Loss at step 6746: nan
Loss at step 6747: nan
Loss at step 6748: nan
Loss at step 6749: nan
Loss at step 6750: nan
Loss at step 6751: nan
Loss at step 6752: nan
Loss at step 6753: nan
Loss at step 6754: nan
Loss at step 6755: nan
Loss at step 6756: nan
Loss at step 6757: nan
Loss at step 6758: nan
Loss at step 6759: nan
Loss at step 6760: nan
Loss at step 6761: nan
Loss at step 6762: nan
Loss at step 6763: nan
Loss at step 6764: nan
Loss at step 6765: nan
Loss at step 6766: nan
Loss at step 6767: nan
Loss at step 6768: nan
Loss at step 6769: nan
Loss at step 6770: nan
Loss at step 6771: nan
Loss at step 6772: nan
Loss at step 6773: nan
Loss at step 6774: nan
Loss at step 6775: nan
Loss at step 6776: nan
Loss at step 6777: nan
Loss at step 6778: nan
Loss at step 6779: nan
Loss at step 6780: nan
Loss at step 6781: nan
Loss at step 6782: nan
Loss at step 6783: nan
Loss at step 6784: nan
Loss at step 6785: nan
Loss at step 6786: nan
Loss at step 6787: nan
Loss at step 6788: nan
Loss at step 6789: nan
Loss at step 6790: nan
Loss at step 6791: nan
Loss at step 6792: nan
Loss at step 6793: nan
Loss at step 6794: nan
Loss at step 6795: nan
Loss at step 6796: nan
Loss at step 6797: nan
Loss at step 6798: nan
Loss at step 6799: nan
Saving training state...
[2025-08-03 03:55:14,592] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 03:55:21,023] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 6800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 6800: nan
Loss at step 6801: nan
Loss at step 6802: nan
Loss at step 6803: nan
Loss at step 6804: nan
Loss at step 6805: nan
Loss at step 6806: nan
Loss at step 6807: nan
Loss at step 6808: nan
Loss at step 6809: nan
Loss at step 6810: nan
Loss at step 6811: nan
Loss at step 6812: nan
Loss at step 6813: nan
Loss at step 6814: nan
Loss at step 6815: nan
Loss at step 6816: nan
Loss at step 6817: nan
Loss at step 6818: nan
Loss at step 6819: nan
Loss at step 6820: nan
Loss at step 6821: nan
Loss at step 6822: nan
Loss at step 6823: nan
Loss at step 6824: nan
Loss at step 6825: nan
Loss at step 6826: nan
Loss at step 6827: nan
Loss at step 6828: nan
Loss at step 6829: nan
Loss at step 6830: nan
Loss at step 6831: nan
Loss at step 6832: nan
Loss at step 6833: nan
Loss at step 6834: nan
Loss at step 6835: nan
Loss at step 6836: nan
Loss at step 6837: nan
Loss at step 6838: nan
Loss at step 6839: nan
Loss at step 6840: nan
Loss at step 6841: nan
Loss at step 6842: nan
Loss at step 6843: nan
Loss at step 6844: nan
Loss at step 6845: nan
Loss at step 6846: nan
Loss at step 6847: nan
Loss at step 6848: nan
Loss at step 6849: nan
Loss at step 6850: nan
Loss at step 6851: nan
Loss at step 6852: nan
Loss at step 6853: nan
Loss at step 6854: nan
Loss at step 6855: nan
Loss at step 6856: nan
Loss at step 6857: nan
Loss at step 6858: nan
Loss at step 6859: nan
Loss at step 6860: nan
Loss at step 6861: nan
Loss at step 6862: nan
Loss at step 6863: nan
Loss at step 6864: nan
Loss at step 6865: nan
Loss at step 6866: nan
Loss at step 6867: nan
Loss at step 6868: nan
Loss at step 6869: nan
Loss at step 6870: nan
Loss at step 6871: nan
Loss at step 6872: nan
Loss at step 6873: nan
Loss at step 6874: nan
Loss at step 6875: nan
Loss at step 6876: nan
Loss at step 6877: nan
Loss at step 6878: nan
Loss at step 6879: nan
Loss at step 6880: nan
Loss at step 6881: nan
Loss at step 6882: nan
Loss at step 6883: nan
Loss at step 6884: nan
Loss at step 6885: nan
Loss at step 6886: nan
Loss at step 6887: nan
Loss at step 6888: nan
Loss at step 6889: nan
Loss at step 6890: nan
Loss at step 6891: nan
Loss at step 6892: nan
Loss at step 6893: nan
Loss at step 6894: nan
Loss at step 6895: nan
Loss at step 6896: nan
Loss at step 6897: nan
Loss at step 6898: nan
Loss at step 6899: nan
Loss at step 6900: nan
Loss at step 6901: nan
Loss at step 6902: nan
Loss at step 6903: nan
Loss at step 6904: nan
Loss at step 6905: nan
Loss at step 6906: nan
Loss at step 6907: nan
Loss at step 6908: nan
Loss at step 6909: nan
Loss at step 6910: nan
Loss at step 6911: nan
Loss at step 6912: nan
Loss at step 6913: nan
Loss at step 6914: nan
Loss at step 6915: nan
Loss at step 6916: nan
Loss at step 6917: nan
Loss at step 6918: nan
Loss at step 6919: nan
Loss at step 6920: nan
Loss at step 6921: nan
Loss at step 6922: nan
Loss at step 6923: nan
Loss at step 6924: nan
Loss at step 6925: nan
Loss at step 6926: nan
Loss at step 6927: nan
Loss at step 6928: nan
Loss at step 6929: nan
Loss at step 6930: nan
Loss at step 6931: nan
Loss at step 6932: nan
Loss at step 6933: nan
Loss at step 6934: nan
Loss at step 6935: nan
Loss at step 6936: nan
Loss at step 6937: nan
Loss at step 6938: nan
Loss at step 6939: nan
Loss at step 6940: nan
Loss at step 6941: nan
Loss at step 6942: nan
Loss at step 6943: nan
Loss at step 6944: nan
Loss at step 6945: nan
Loss at step 6946: nan
Loss at step 6947: nan
Loss at step 6948: nan
Loss at step 6949: nan
Loss at step 6950: nan
Loss at step 6951: nan
Loss at step 6952: nan
Loss at step 6953: nan
Loss at step 6954: nan
Loss at step 6955: nan
Loss at step 6956: nan
Loss at step 6957: nan
Loss at step 6958: nan
Loss at step 6959: nan
Loss at step 6960: nan
Loss at step 6961: nan
Loss at step 6962: nan
Loss at step 6963: nan
Loss at step 6964: nan
Loss at step 6965: nan
Loss at step 6966: nan
Loss at step 6967: nan
Loss at step 6968: nan
Loss at step 6969: nan
Loss at step 6970: nan
Loss at step 6971: nan
Loss at step 6972: nan
Loss at step 6973: nan
Loss at step 6974: nan
Loss at step 6975: nan
Loss at step 6976: nan
Loss at step 6977: nan
Loss at step 6978: nan
Loss at step 6979: nan
Loss at step 6980: nan
Loss at step 6981: nan
Loss at step 6982: nan
Loss at step 6983: nan
Loss at step 6984: nan
Loss at step 6985: nan
Loss at step 6986: nan
Loss at step 6987: nan
Loss at step 6988: nan
Loss at step 6989: nan
Loss at step 6990: nan
Loss at step 6991: nan
Loss at step 6992: nan
Loss at step 6993: nan
Loss at step 6994: nan
Loss at step 6995: nan
Loss at step 6996: nan
Loss at step 6997: nan
Loss at step 6998: nan
Loss at step 6999: nan
Saving training state...
[2025-08-03 05:02:55,190] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 05:03:01,571] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 7000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 7000: nan
Loss at step 7001: nan
Loss at step 7002: nan
Loss at step 7003: nan
Loss at step 7004: nan
Loss at step 7005: nan
Loss at step 7006: nan
Loss at step 7007: nan
Loss at step 7008: nan
Loss at step 7009: nan
Loss at step 7010: nan
Loss at step 7011: nan
Loss at step 7012: nan
Loss at step 7013: nan
Loss at step 7014: nan
Loss at step 7015: nan
Loss at step 7016: nan
Loss at step 7017: nan
Loss at step 7018: nan
Loss at step 7019: nan
Loss at step 7020: nan
Loss at step 7021: nan
Loss at step 7022: nan
Loss at step 7023: nan
Loss at step 7024: nan
Loss at step 7025: nan
Loss at step 7026: nan
Loss at step 7027: nan
Loss at step 7028: nan
Loss at step 7029: nan
Loss at step 7030: nan
Loss at step 7031: nan
Loss at step 7032: nan
Loss at step 7033: nan
Loss at step 7034: nan
Loss at step 7035: nan
Loss at step 7036: nan
Loss at step 7037: nan
Loss at step 7038: nan
Loss at step 7039: nan
Loss at step 7040: nan
Loss at step 7041: nan
Loss at step 7042: nan
Loss at step 7043: nan
Loss at step 7044: nan
Loss at step 7045: nan
Loss at step 7046: nan
Loss at step 7047: nan
Loss at step 7048: nan
Loss at step 7049: nan
Loss at step 7050: nan
Loss at step 7051: nan
Loss at step 7052: nan
Loss at step 7053: nan
Loss at step 7054: nan
Loss at step 7055: nan
Loss at step 7056: nan
Loss at step 7057: nan
Loss at step 7058: nan
Loss at step 7059: nan
Loss at step 7060: nan
Loss at step 7061: nan
Loss at step 7062: nan
Loss at step 7063: nan
Loss at step 7064: nan
Loss at step 7065: nan
Loss at step 7066: nan
Loss at step 7067: nan
Loss at step 7068: nan
Loss at step 7069: nan
Loss at step 7070: nan
Loss at step 7071: nan
Loss at step 7072: nan
Loss at step 7073: nan
Loss at step 7074: nan
Loss at step 7075: nan
Loss at step 7076: nan
Loss at step 7077: nan
Loss at step 7078: nan
Loss at step 7079: nan
Loss at step 7080: nan
Loss at step 7081: nan
Loss at step 7082: nan
Loss at step 7083: nan
Loss at step 7084: nan
Loss at step 7085: nan
Loss at step 7086: nan
Loss at step 7087: nan
Loss at step 7088: nan
Loss at step 7089: nan
Loss at step 7090: nan
Loss at step 7091: nan
Loss at step 7092: nan
Loss at step 7093: nan
Loss at step 7094: nan
Loss at step 7095: nan
Loss at step 7096: nan
Loss at step 7097: nan
Loss at step 7098: nan
Loss at step 7099: nan
Loss at step 7100: nan
Loss at step 7101: nan
Loss at step 7102: nan
Loss at step 7103: nan
Loss at step 7104: nan
Loss at step 7105: nan
Loss at step 7106: nan
Loss at step 7107: nan
Loss at step 7108: nan
Loss at step 7109: nan
Loss at step 7110: nan
Loss at step 7111: nan
Loss at step 7112: nan
Loss at step 7113: nan
Loss at step 7114: nan
Loss at step 7115: nan
Loss at step 7116: nan
Loss at step 7117: nan
Loss at step 7118: nan
Loss at step 7119: nan
Loss at step 7120: nan
Loss at step 7121: nan
Loss at step 7122: nan
Loss at step 7123: nan
Loss at step 7124: nan
Loss at step 7125: nan
Loss at step 7126: nan
Loss at step 7127: nan
Loss at step 7128: nan
Loss at step 7129: nan
Loss at step 7130: nan
Loss at step 7131: nan
Loss at step 7132: nan
Loss at step 7133: nan
Loss at step 7134: nan
Loss at step 7135: nan
Loss at step 7136: nan
Loss at step 7137: nan
Loss at step 7138: nan
Loss at step 7139: nan
Loss at step 7140: nan
Loss at step 7141: nan
Loss at step 7142: nan
Loss at step 7143: nan
Loss at step 7144: nan
Loss at step 7145: nan
Loss at step 7146: nan
Loss at step 7147: nan
Loss at step 7148: nan
Loss at step 7149: nan
Loss at step 7150: nan
Loss at step 7151: nan
Loss at step 7152: nan
Loss at step 7153: nan
Loss at step 7154: nan
Loss at step 7155: nan
Loss at step 7156: nan
Loss at step 7157: nan
Loss at step 7158: nan
Loss at step 7159: nan
Loss at step 7160: nan
Loss at step 7161: nan
Loss at step 7162: nan
Loss at step 7163: nan
Loss at step 7164: nan
Loss at step 7165: nan
Loss at step 7166: nan
Loss at step 7167: nan
Loss at step 7168: nan
Loss at step 7169: nan
Loss at step 7170: nan
Loss at step 7171: nan
Loss at step 7172: nan
Loss at step 7173: nan
Loss at step 7174: nan
Loss at step 7175: nan
Loss at step 7176: nan
Loss at step 7177: nan
Loss at step 7178: nan
Loss at step 7179: nan
Loss at step 7180: nan
Loss at step 7181: nan
Loss at step 7182: nan
Loss at step 7183: nan
Loss at step 7184: nan
Loss at step 7185: nan
Loss at step 7186: nan
Loss at step 7187: nan
Loss at step 7188: nan
Loss at step 7189: nan
Loss at step 7190: nan
Loss at step 7191: nan
Loss at step 7192: nan
Loss at step 7193: nan
Loss at step 7194: nan
Loss at step 7195: nan
Loss at step 7196: nan
Loss at step 7197: nan
Loss at step 7198: nan
Loss at step 7199: nan
Saving training state...
[2025-08-03 06:10:57,300] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 06:11:03,604] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 7200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 7200: nan
Loss at step 7201: nan
Loss at step 7202: nan
Loss at step 7203: nan
Loss at step 7204: nan
Loss at step 7205: nan
Loss at step 7206: nan
Loss at step 7207: nan
Loss at step 7208: nan
Loss at step 7209: nan
Loss at step 7210: nan
Loss at step 7211: nan
Loss at step 7212: nan
Loss at step 7213: nan
Loss at step 7214: nan
Loss at step 7215: nan
Loss at step 7216: nan
Loss at step 7217: nan
Loss at step 7218: nan
Loss at step 7219: nan
Loss at step 7220: nan
Loss at step 7221: nan
Loss at step 7222: nan
Loss at step 7223: nan
Loss at step 7224: nan
Loss at step 7225: nan
Loss at step 7226: nan
Loss at step 7227: nan
Loss at step 7228: nan
Loss at step 7229: nan
Loss at step 7230: nan
Loss at step 7231: nan
Loss at step 7232: nan
Loss at step 7233: nan
Loss at step 7234: nan
Loss at step 7235: nan
Loss at step 7236: nan
Loss at step 7237: nan
Loss at step 7238: nan
Loss at step 7239: nan
Loss at step 7240: nan
Loss at step 7241: nan
Loss at step 7242: nan
Loss at step 7243: nan
Loss at step 7244: nan
Loss at step 7245: nan
Loss at step 7246: nan
Loss at step 7247: nan
Loss at step 7248: nan
Loss at step 7249: nan
Loss at step 7250: nan
Loss at step 7251: nan
Loss at step 7252: nan
Loss at step 7253: nan
Loss at step 7254: nan
Loss at step 7255: nan
Loss at step 7256: nan
Loss at step 7257: nan
Loss at step 7258: nan
Loss at step 7259: nan
Loss at step 7260: nan
Loss at step 7261: nan
Loss at step 7262: nan
Loss at step 7263: nan
Loss at step 7264: nan
Loss at step 7265: nan
Loss at step 7266: nan
Loss at step 7267: nan
Loss at step 7268: nan
Loss at step 7269: nan
Loss at step 7270: nan
Loss at step 7271: nan
Loss at step 7272: nan
Loss at step 7273: nan
Loss at step 7274: nan
Loss at step 7275: nan
Loss at step 7276: nan
Loss at step 7277: nan
Loss at step 7278: nan
Loss at step 7279: nan
Loss at step 7280: nan
Loss at step 7281: nan
Loss at step 7282: nan
Loss at step 7283: nan
Loss at step 7284: nan
Loss at step 7285: nan
Loss at step 7286: nan
Loss at step 7287: nan
Loss at step 7288: nan
Loss at step 7289: nan
Loss at step 7290: nan
Loss at step 7291: nan
Loss at step 7292: nan
Loss at step 7293: nan
Loss at step 7294: nan
Loss at step 7295: nan
Loss at step 7296: nan
Loss at step 7297: nan
Loss at step 7298: nan
Loss at step 7299: nan
Loss at step 7300: nan
Loss at step 7301: nan
Loss at step 7302: nan
Loss at step 7303: nan
Loss at step 7304: nan
Loss at step 7305: nan
Loss at step 7306: nan
Loss at step 7307: nan
Loss at step 7308: nan
Loss at step 7309: nan
Loss at step 7310: nan
Loss at step 7311: nan
Loss at step 7312: nan
Loss at step 7313: nan
Loss at step 7314: nan
Loss at step 7315: nan
Loss at step 7316: nan
Loss at step 7317: nan
Loss at step 7318: nan
Loss at step 7319: nan
Loss at step 7320: nan
Loss at step 7321: nan
Loss at step 7322: nan
Loss at step 7323: nan
Loss at step 7324: nan
Loss at step 7325: nan
Loss at step 7326: nan
Loss at step 7327: nan
Loss at step 7328: nan
Loss at step 7329: nan
Loss at step 7330: nan
Loss at step 7331: nan
Loss at step 7332: nan
Loss at step 7333: nan
Loss at step 7334: nan
Loss at step 7335: nan
Loss at step 7336: nan
Loss at step 7337: nan
Loss at step 7338: nan
Loss at step 7339: nan
Loss at step 7340: nan
Loss at step 7341: nan
Loss at step 7342: nan
Loss at step 7343: nan
Loss at step 7344: nan
Loss at step 7345: nan
Loss at step 7346: nan
Loss at step 7347: nan
Loss at step 7348: nan
Loss at step 7349: nan
Loss at step 7350: nan
Loss at step 7351: nan
Loss at step 7352: nan
Loss at step 7353: nan
Loss at step 7354: nan
Loss at step 7355: nan
Loss at step 7356: nan
Loss at step 7357: nan
Loss at step 7358: nan
Loss at step 7359: nan
Loss at step 7360: nan
Loss at step 7361: nan
Loss at step 7362: nan
Loss at step 7363: nan
Loss at step 7364: nan
Loss at step 7365: nan
Loss at step 7366: nan
Loss at step 7367: nan
Loss at step 7368: nan
Loss at step 7369: nan
Loss at step 7370: nan
Loss at step 7371: nan
Loss at step 7372: nan
Loss at step 7373: nan
Loss at step 7374: nan
Loss at step 7375: nan
Loss at step 7376: nan
Loss at step 7377: nan
Loss at step 7378: nan
Loss at step 7379: nan
Loss at step 7380: nan
Loss at step 7381: nan
Loss at step 7382: nan
Loss at step 7383: nan
Loss at step 7384: nan
Loss at step 7385: nan
Loss at step 7386: nan
Loss at step 7387: nan
Loss at step 7388: nan
Loss at step 7389: nan
Loss at step 7390: nan
Loss at step 7391: nan
Loss at step 7392: nan
Loss at step 7393: nan
Loss at step 7394: nan
Loss at step 7395: nan
Loss at step 7396: nan
Loss at step 7397: nan
Loss at step 7398: nan
Loss at step 7399: nan
Saving training state...
[2025-08-03 07:18:51,655] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 07:18:58,118] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 7400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 7400: nan
Loss at step 7401: nan
Loss at step 7402: nan
Loss at step 7403: nan
Loss at step 7404: nan
Loss at step 7405: nan
Loss at step 7406: nan
Loss at step 7407: nan
Loss at step 7408: nan
Loss at step 7409: nan
Loss at step 7410: nan
Loss at step 7411: nan
Loss at step 7412: nan
Loss at step 7413: nan
Loss at step 7414: nan
Loss at step 7415: nan
Loss at step 7416: nan
Loss at step 7417: nan
Loss at step 7418: nan
Loss at step 7419: nan
Loss at step 7420: nan
Loss at step 7421: nan
Loss at step 7422: nan
Loss at step 7423: nan
Loss at step 7424: nan
Loss at step 7425: nan
Loss at step 7426: nan
Loss at step 7427: nan
Loss at step 7428: nan
Loss at step 7429: nan
Loss at step 7430: nan
Loss at step 7431: nan
Loss at step 7432: nan
Loss at step 7433: nan
Loss at step 7434: nan
Loss at step 7435: nan
Loss at step 7436: nan
Loss at step 7437: nan
Loss at step 7438: nan
Loss at step 7439: nan
Loss at step 7440: nan
Loss at step 7441: nan
Loss at step 7442: nan
Loss at step 7443: nan
Loss at step 7444: nan
Loss at step 7445: nan
Loss at step 7446: nan
Loss at step 7447: nan
Loss at step 7448: nan
Loss at step 7449: nan
Loss at step 7450: nan
Loss at step 7451: nan
Loss at step 7452: nan
Loss at step 7453: nan
Loss at step 7454: nan
Loss at step 7455: nan
Loss at step 7456: nan
Loss at step 7457: nan
Loss at step 7458: nan
Loss at step 7459: nan
Loss at step 7460: nan
Loss at step 7461: nan
Loss at step 7462: nan
Loss at step 7463: nan
Loss at step 7464: nan
Loss at step 7465: nan
Loss at step 7466: nan
Loss at step 7467: nan
Loss at step 7468: nan
Loss at step 7469: nan
Loss at step 7470: nan
Loss at step 7471: nan
Loss at step 7472: nan
Loss at step 7473: nan
Loss at step 7474: nan
Loss at step 7475: nan
Loss at step 7476: nan
Loss at step 7477: nan
Loss at step 7478: nan
Loss at step 7479: nan
Loss at step 7480: nan
Loss at step 7481: nan
Loss at step 7482: nan
Loss at step 7483: nan
Loss at step 7484: nan
Loss at step 7485: nan
Loss at step 7486: nan
Loss at step 7487: nan
Loss at step 7488: nan
Loss at step 7489: nan
Loss at step 7490: nan
Loss at step 7491: nan
Loss at step 7492: nan
Loss at step 7493: nan
Loss at step 7494: nan
Loss at step 7495: nan
Loss at step 7496: nan
Loss at step 7497: nan
Loss at step 7498: nan
Loss at step 7499: nan
Loss at step 7500: nan
Loss at step 7501: nan
Loss at step 7502: nan
Loss at step 7503: nan
Loss at step 7504: nan
Loss at step 7505: nan
Loss at step 7506: nan
Loss at step 7507: nan
Loss at step 7508: nan
Loss at step 7509: nan
Loss at step 7510: nan
Loss at step 7511: nan
Loss at step 7512: nan
Loss at step 7513: nan
Loss at step 7514: nan
Loss at step 7515: nan
Loss at step 7516: nan
Loss at step 7517: nan
Loss at step 7518: nan
Loss at step 7519: nan
Loss at step 7520: nan
Loss at step 7521: nan
Loss at step 7522: nan
Loss at step 7523: nan
Loss at step 7524: nan
Loss at step 7525: nan
Loss at step 7526: nan
Loss at step 7527: nan
Loss at step 7528: nan
Loss at step 7529: nan
Loss at step 7530: nan
Loss at step 7531: nan
Loss at step 7532: nan
Loss at step 7533: nan
Loss at step 7534: nan
Loss at step 7535: nan
Loss at step 7536: nan
Loss at step 7537: nan
Loss at step 7538: nan
Loss at step 7539: nan
Loss at step 7540: nan
Loss at step 7541: nan
Loss at step 7542: nan
Loss at step 7543: nan
Loss at step 7544: nan
Loss at step 7545: nan
Loss at step 7546: nan
Loss at step 7547: nan
Loss at step 7548: nan
Loss at step 7549: nan
Loss at step 7550: nan
Loss at step 7551: nan
Loss at step 7552: nan
Loss at step 7553: nan
Loss at step 7554: nan
Loss at step 7555: nan
Loss at step 7556: nan
Loss at step 7557: nan
Loss at step 7558: nan
Loss at step 7559: nan
Loss at step 7560: nan
Loss at step 7561: nan
Loss at step 7562: nan
Loss at step 7563: nan
Loss at step 7564: nan
Loss at step 7565: nan
Loss at step 7566: nan
Loss at step 7567: nan
Loss at step 7568: nan
Loss at step 7569: nan
Loss at step 7570: nan
Loss at step 7571: nan
Loss at step 7572: nan
Loss at step 7573: nan
Loss at step 7574: nan
Loss at step 7575: nan
Loss at step 7576: nan
Loss at step 7577: nan
Loss at step 7578: nan
Loss at step 7579: nan
Loss at step 7580: nan
Loss at step 7581: nan
Loss at step 7582: nan
Loss at step 7583: nan
Loss at step 7584: nan
Loss at step 7585: nan
Loss at step 7586: nan
Loss at step 7587: nan
Loss at step 7588: nan
Loss at step 7589: nan
Loss at step 7590: nan
Loss at step 7591: nan
Loss at step 7592: nan
Loss at step 7593: nan
Loss at step 7594: nan
Loss at step 7595: nan
Loss at step 7596: nan
Loss at step 7597: nan
Loss at step 7598: nan
Loss at step 7599: nan
Saving training state...
[2025-08-03 08:27:44,147] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 08:27:50,606] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 7600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 7600: nan
Loss at step 7601: nan
Loss at step 7602: nan
Loss at step 7603: nan
Loss at step 7604: nan
Loss at step 7605: nan
Loss at step 7606: nan
Loss at step 7607: nan
Loss at step 7608: nan
Loss at step 7609: nan
Loss at step 7610: nan
Loss at step 7611: nan
Loss at step 7612: nan
Loss at step 7613: nan
Loss at step 7614: nan
Loss at step 7615: nan
Loss at step 7616: nan
Loss at step 7617: nan
Loss at step 7618: nan
Loss at step 7619: nan
Loss at step 7620: nan
Loss at step 7621: nan
Loss at step 7622: nan
Loss at step 7623: nan
Loss at step 7624: nan
Loss at step 7625: nan
Loss at step 7626: nan
Loss at step 7627: nan
Loss at step 7628: nan
Loss at step 7629: nan
Loss at step 7630: nan
Loss at step 7631: nan
Loss at step 7632: nan
Loss at step 7633: nan
Loss at step 7634: nan
Loss at step 7635: nan
Loss at step 7636: nan
Loss at step 7637: nan
Loss at step 7638: nan
Loss at step 7639: nan
Loss at step 7640: nan
Loss at step 7641: nan
Loss at step 7642: nan
Loss at step 7643: nan
Loss at step 7644: nan
Loss at step 7645: nan
Loss at step 7646: nan
Loss at step 7647: nan
Loss at step 7648: nan
Loss at step 7649: nan
Loss at step 7650: nan
Loss at step 7651: nan
Loss at step 7652: nan
Loss at step 7653: nan
Loss at step 7654: nan
Loss at step 7655: nan
Loss at step 7656: nan
Loss at step 7657: nan
Loss at step 7658: nan
Loss at step 7659: nan
Loss at step 7660: nan
Loss at step 7661: nan
Loss at step 7662: nan
Loss at step 7663: nan
Loss at step 7664: nan
Loss at step 7665: nan
Loss at step 7666: nan
Loss at step 7667: nan
Loss at step 7668: nan
Loss at step 7669: nan
Loss at step 7670: nan
Loss at step 7671: nan
Loss at step 7672: nan
Loss at step 7673: nan
Loss at step 7674: nan
Loss at step 7675: nan
Loss at step 7676: nan
Loss at step 7677: nan
Loss at step 7678: nan
Loss at step 7679: nan
Loss at step 7680: nan
Loss at step 7681: nan
Loss at step 7682: nan
Loss at step 7683: nan
Loss at step 7684: nan
Loss at step 7685: nan
Loss at step 7686: nan
Loss at step 7687: nan
Loss at step 7688: nan
Loss at step 7689: nan
Loss at step 7690: nan
Loss at step 7691: nan
Loss at step 7692: nan
Loss at step 7693: nan
Loss at step 7694: nan
Loss at step 7695: nan
Loss at step 7696: nan
Loss at step 7697: nan
Loss at step 7698: nan
Loss at step 7699: nan
Loss at step 7700: nan
Loss at step 7701: nan
Loss at step 7702: nan
Loss at step 7703: nan
Loss at step 7704: nan
Loss at step 7705: nan
Loss at step 7706: nan
Loss at step 7707: nan
Loss at step 7708: nan
Loss at step 7709: nan
Loss at step 7710: nan
Loss at step 7711: nan
Loss at step 7712: nan
Loss at step 7713: nan
Loss at step 7714: nan
Loss at step 7715: nan
Loss at step 7716: nan
Loss at step 7717: nan
Loss at step 7718: nan
Loss at step 7719: nan
Loss at step 7720: nan
Loss at step 7721: nan
Loss at step 7722: nan
Loss at step 7723: nan
Loss at step 7724: nan
Loss at step 7725: nan
Loss at step 7726: nan
Loss at step 7727: nan
Loss at step 7728: nan
Loss at step 7729: nan
Loss at step 7730: nan
Loss at step 7731: nan
Loss at step 7732: nan
Loss at step 7733: nan
Loss at step 7734: nan
Loss at step 7735: nan
Loss at step 7736: nan
Loss at step 7737: nan
Loss at step 7738: nan
Loss at step 7739: nan
Loss at step 7740: nan
Loss at step 7741: nan
Loss at step 7742: nan
Loss at step 7743: nan
Loss at step 7744: nan
Loss at step 7745: nan
Loss at step 7746: nan
Loss at step 7747: nan
Loss at step 7748: nan
Loss at step 7749: nan
Loss at step 7750: nan
Loss at step 7751: nan
Loss at step 7752: nan
Loss at step 7753: nan
Loss at step 7754: nan
Loss at step 7755: nan
Loss at step 7756: nan
Loss at step 7757: nan
Loss at step 7758: nan
Loss at step 7759: nan
Loss at step 7760: nan
Loss at step 7761: nan
Loss at step 7762: nan
Loss at step 7763: nan
Loss at step 7764: nan
Loss at step 7765: nan
Loss at step 7766: nan
Loss at step 7767: nan
Loss at step 7768: nan
Loss at step 7769: nan
Loss at step 7770: nan
Loss at step 7771: nan
Loss at step 7772: nan
Loss at step 7773: nan
Loss at step 7774: nan
Loss at step 7775: nan
Loss at step 7776: nan
Loss at step 7777: nan
Loss at step 7778: nan
Loss at step 7779: nan
Loss at step 7780: nan
Loss at step 7781: nan
Loss at step 7782: nan
Loss at step 7783: nan
Loss at step 7784: nan
Loss at step 7785: nan
Loss at step 7786: nan
Loss at step 7787: nan
Loss at step 7788: nan
Loss at step 7789: nan
Loss at step 7790: nan
Loss at step 7791: nan
Loss at step 7792: nan
Loss at step 7793: nan
Loss at step 7794: nan
Loss at step 7795: nan
Loss at step 7796: nan
Loss at step 7797: nan
Loss at step 7798: nan
Loss at step 7799: nan
Saving training state...
[2025-08-03 09:36:15,780] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 09:36:22,349] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 7800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 7800: nan
Loss at step 7801: nan
Loss at step 7802: nan
Loss at step 7803: nan
Loss at step 7804: nan
Loss at step 7805: nan
Loss at step 7806: nan
Loss at step 7807: nan
Loss at step 7808: nan
Loss at step 7809: nan
Loss at step 7810: nan
Loss at step 7811: nan
Loss at step 7812: nan
Loss at step 7813: nan
Loss at step 7814: nan
Loss at step 7815: nan
Loss at step 7816: nan
Loss at step 7817: nan
Loss at step 7818: nan
Loss at step 7819: nan
Loss at step 7820: nan
Loss at step 7821: nan
Loss at step 7822: nan
Loss at step 7823: nan
Loss at step 7824: nan
Loss at step 7825: nan
Loss at step 7826: nan
Loss at step 7827: nan
Loss at step 7828: nan
Loss at step 7829: nan
Loss at step 7830: nan
Loss at step 7831: nan
Loss at step 7832: nan
Loss at step 7833: nan
Loss at step 7834: nan
Loss at step 7835: nan
Loss at step 7836: nan
Loss at step 7837: nan
Loss at step 7838: nan
Loss at step 7839: nan
Loss at step 7840: nan
Loss at step 7841: nan
Loss at step 7842: nan
Loss at step 7843: nan
Loss at step 7844: nan
Loss at step 7845: nan
Loss at step 7846: nan
Loss at step 7847: nan
Loss at step 7848: nan
Loss at step 7849: nan
Loss at step 7850: nan
Loss at step 7851: nan
Loss at step 7852: nan
Loss at step 7853: nan
Loss at step 7854: nan
Loss at step 7855: nan
Loss at step 7856: nan
Loss at step 7857: nan
Loss at step 7858: nan
Loss at step 7859: nan
Loss at step 7860: nan
Loss at step 7861: nan
Loss at step 7862: nan
Loss at step 7863: nan
Loss at step 7864: nan
Loss at step 7865: nan
Loss at step 7866: nan
Loss at step 7867: nan
Loss at step 7868: nan
Loss at step 7869: nan
Loss at step 7870: nan
Loss at step 7871: nan
Loss at step 7872: nan
Loss at step 7873: nan
Loss at step 7874: nan
Loss at step 7875: nan
Loss at step 7876: nan
Loss at step 7877: nan
Loss at step 7878: nan
Loss at step 7879: nan
Loss at step 7880: nan
Loss at step 7881: nan
Loss at step 7882: nan
Loss at step 7883: nan
Loss at step 7884: nan
Loss at step 7885: nan
Loss at step 7886: nan
Loss at step 7887: nan
Loss at step 7888: nan
Loss at step 7889: nan
Loss at step 7890: nan
Loss at step 7891: nan
Loss at step 7892: nan
Loss at step 7893: nan
Loss at step 7894: nan
Loss at step 7895: nan
Loss at step 7896: nan
Loss at step 7897: nan
Loss at step 7898: nan
Loss at step 7899: nan
Loss at step 7900: nan
Loss at step 7901: nan
Loss at step 7902: nan
Loss at step 7903: nan
Loss at step 7904: nan
Loss at step 7905: nan
Loss at step 7906: nan
Loss at step 7907: nan
Loss at step 7908: nan
Loss at step 7909: nan
Loss at step 7910: nan
Loss at step 7911: nan
Loss at step 7912: nan
Loss at step 7913: nan
Loss at step 7914: nan
Loss at step 7915: nan
Loss at step 7916: nan
Loss at step 7917: nan
Loss at step 7918: nan
Loss at step 7919: nan
Loss at step 7920: nan
Loss at step 7921: nan
Loss at step 7922: nan
Loss at step 7923: nan
Loss at step 7924: nan
Loss at step 7925: nan
Loss at step 7926: nan
Loss at step 7927: nan
Loss at step 7928: nan
Loss at step 7929: nan
Loss at step 7930: nan
Loss at step 7931: nan
Loss at step 7932: nan
Loss at step 7933: nan
Loss at step 7934: nan
Loss at step 7935: nan
Loss at step 7936: nan
Loss at step 7937: nan
Loss at step 7938: nan
Loss at step 7939: nan
Loss at step 7940: nan
Loss at step 7941: nan
Loss at step 7942: nan
Loss at step 7943: nan
Loss at step 7944: nan
Loss at step 7945: nan
Loss at step 7946: nan
Loss at step 7947: nan
Loss at step 7948: nan
Loss at step 7949: nan
Loss at step 7950: nan
Loss at step 7951: nan
Loss at step 7952: nan
Loss at step 7953: nan
Loss at step 7954: nan
Loss at step 7955: nan
Loss at step 7956: nan
Loss at step 7957: nan
Loss at step 7958: nan
Loss at step 7959: nan
Loss at step 7960: nan
Loss at step 7961: nan
Loss at step 7962: nan
Loss at step 7963: nan
Loss at step 7964: nan
Loss at step 7965: nan
Loss at step 7966: nan
Loss at step 7967: nan
Loss at step 7968: nan
Loss at step 7969: nan
Loss at step 7970: nan
Loss at step 7971: nan
Loss at step 7972: nan
Loss at step 7973: nan
Loss at step 7974: nan
Loss at step 7975: nan
Loss at step 7976: nan
Loss at step 7977: nan
Loss at step 7978: nan
Loss at step 7979: nan
Loss at step 7980: nan
Loss at step 7981: nan
Loss at step 7982: nan
Loss at step 7983: nan
Loss at step 7984: nan
Loss at step 7985: nan
Loss at step 7986: nan
Loss at step 7987: nan
Loss at step 7988: nan
Loss at step 7989: nan
Loss at step 7990: nan
Loss at step 7991: nan
Loss at step 7992: nan
Loss at step 7993: nan
Loss at step 7994: nan
Loss at step 7995: nan
Loss at step 7996: nan
Loss at step 7997: nan
Loss at step 7998: nan
Loss at step 7999: nan
Saving training state...
[2025-08-03 10:44:48,846] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 10:44:55,433] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 8000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 8000: nan
Loss at step 8001: nan
Loss at step 8002: nan
Loss at step 8003: nan
Loss at step 8004: nan
Loss at step 8005: nan
Loss at step 8006: nan
Loss at step 8007: nan
Loss at step 8008: nan
Loss at step 8009: nan
Loss at step 8010: nan
Loss at step 8011: nan
Loss at step 8012: nan
Loss at step 8013: nan
Loss at step 8014: nan
Loss at step 8015: nan
Loss at step 8016: nan
Loss at step 8017: nan
Loss at step 8018: nan
Loss at step 8019: nan
Loss at step 8020: nan
Loss at step 8021: nan
Loss at step 8022: nan
Loss at step 8023: nan
Loss at step 8024: nan
Loss at step 8025: nan
Loss at step 8026: nan
Loss at step 8027: nan
Loss at step 8028: nan
Loss at step 8029: nan
Loss at step 8030: nan
Loss at step 8031: nan
Loss at step 8032: nan
Loss at step 8033: nan
Loss at step 8034: nan
Loss at step 8035: nan
Loss at step 8036: nan
Loss at step 8037: nan
Loss at step 8038: nan
Loss at step 8039: nan
Loss at step 8040: nan
Loss at step 8041: nan
Loss at step 8042: nan
Loss at step 8043: nan
Loss at step 8044: nan
Loss at step 8045: nan
Loss at step 8046: nan
Loss at step 8047: nan
Loss at step 8048: nan
Loss at step 8049: nan
Loss at step 8050: nan
Loss at step 8051: nan
Loss at step 8052: nan
Loss at step 8053: nan
Loss at step 8054: nan
Loss at step 8055: nan
Loss at step 8056: nan
Loss at step 8057: nan
Loss at step 8058: nan
Loss at step 8059: nan
Loss at step 8060: nan
Loss at step 8061: nan
Loss at step 8062: nan
Loss at step 8063: nan
Loss at step 8064: nan
Loss at step 8065: nan
Loss at step 8066: nan
Loss at step 8067: nan
Loss at step 8068: nan
Loss at step 8069: nan
Loss at step 8070: nan
Loss at step 8071: nan
Loss at step 8072: nan
Loss at step 8073: nan
Loss at step 8074: nan
Loss at step 8075: nan
Loss at step 8076: nan
Loss at step 8077: nan
Loss at step 8078: nan
Loss at step 8079: nan
Loss at step 8080: nan
Loss at step 8081: nan
Loss at step 8082: nan
Loss at step 8083: nan
Loss at step 8084: nan
Loss at step 8085: nan
Loss at step 8086: nan
Loss at step 8087: nan
Loss at step 8088: nan
Loss at step 8089: nan
Loss at step 8090: nan
Loss at step 8091: nan
Loss at step 8092: nan
Loss at step 8093: nan
Loss at step 8094: nan
Loss at step 8095: nan
Loss at step 8096: nan
Loss at step 8097: nan
Loss at step 8098: nan
Loss at step 8099: nan
Loss at step 8100: nan
Loss at step 8101: nan
Loss at step 8102: nan
Loss at step 8103: nan
Loss at step 8104: nan
Loss at step 8105: nan
Loss at step 8106: nan
Loss at step 8107: nan
Loss at step 8108: nan
Loss at step 8109: nan
Loss at step 8110: nan
Loss at step 8111: nan
Loss at step 8112: nan
Loss at step 8113: nan
Loss at step 8114: nan
Loss at step 8115: nan
Loss at step 8116: nan
Loss at step 8117: nan
Loss at step 8118: nan
Loss at step 8119: nan
Loss at step 8120: nan
Loss at step 8121: nan
Loss at step 8122: nan
Loss at step 8123: nan
Loss at step 8124: nan
Loss at step 8125: nan
Loss at step 8126: nan
Loss at step 8127: nan
Loss at step 8128: nan
Loss at step 8129: nan
Loss at step 8130: nan
Loss at step 8131: nan
Loss at step 8132: nan
Loss at step 8133: nan
Loss at step 8134: nan
Loss at step 8135: nan
Loss at step 8136: nan
Loss at step 8137: nan
Loss at step 8138: nan
Loss at step 8139: nan
Loss at step 8140: nan
Loss at step 8141: nan
Loss at step 8142: nan
Loss at step 8143: nan
Loss at step 8144: nan
Loss at step 8145: nan
Loss at step 8146: nan
Loss at step 8147: nan
Loss at step 8148: nan
Loss at step 8149: nan
Loss at step 8150: nan
Loss at step 8151: nan
Loss at step 8152: nan
Loss at step 8153: nan
Loss at step 8154: nan
Loss at step 8155: nan
Loss at step 8156: nan
Loss at step 8157: nan
Loss at step 8158: nan
Loss at step 8159: nan
Loss at step 8160: nan
Loss at step 8161: nan
Loss at step 8162: nan
Loss at step 8163: nan
Loss at step 8164: nan
Loss at step 8165: nan
Loss at step 8166: nan
Loss at step 8167: nan
Loss at step 8168: nan
Loss at step 8169: nan
Loss at step 8170: nan
Loss at step 8171: nan
Loss at step 8172: nan
Loss at step 8173: nan
Loss at step 8174: nan
Loss at step 8175: nan
Loss at step 8176: nan
Loss at step 8177: nan
Loss at step 8178: nan
Loss at step 8179: nan
Loss at step 8180: nan
Loss at step 8181: nan
Loss at step 8182: nan
Loss at step 8183: nan
Loss at step 8184: nan
Loss at step 8185: nan
Loss at step 8186: nan
Loss at step 8187: nan
Loss at step 8188: nan
Loss at step 8189: nan
Loss at step 8190: nan
Loss at step 8191: nan
Loss at step 8192: nan
Loss at step 8193: nan
Loss at step 8194: nan
Loss at step 8195: nan
Loss at step 8196: nan
Loss at step 8197: nan
Loss at step 8198: nan
Loss at step 8199: nan
Saving training state...
[2025-08-03 11:54:16,772] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 11:54:23,150] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 8200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 8200: nan
Loss at step 8201: nan
Loss at step 8202: nan
Loss at step 8203: nan
Loss at step 8204: nan
Loss at step 8205: nan
Loss at step 8206: nan
Loss at step 8207: nan
Loss at step 8208: nan
Loss at step 8209: nan
Loss at step 8210: nan
Loss at step 8211: nan
Loss at step 8212: nan
Loss at step 8213: nan
Loss at step 8214: nan
Loss at step 8215: nan
Loss at step 8216: nan
Loss at step 8217: nan
Loss at step 8218: nan
Loss at step 8219: nan
Loss at step 8220: nan
Loss at step 8221: nan
Loss at step 8222: nan
Loss at step 8223: nan
Loss at step 8224: nan
Loss at step 8225: nan
Loss at step 8226: nan
Loss at step 8227: nan
Loss at step 8228: nan
Loss at step 8229: nan
Loss at step 8230: nan
Loss at step 8231: nan
Loss at step 8232: nan
Loss at step 8233: nan
Loss at step 8234: nan
Loss at step 8235: nan
Loss at step 8236: nan
Loss at step 8237: nan
Loss at step 8238: nan
Loss at step 8239: nan
Loss at step 8240: nan
Loss at step 8241: nan
Loss at step 8242: nan
Loss at step 8243: nan
Loss at step 8244: nan
Loss at step 8245: nan
Loss at step 8246: nan
Loss at step 8247: nan
Loss at step 8248: nan
Loss at step 8249: nan
Loss at step 8250: nan
Loss at step 8251: nan
Loss at step 8252: nan
Loss at step 8253: nan
Loss at step 8254: nan
Loss at step 8255: nan
Loss at step 8256: nan
Loss at step 8257: nan
Loss at step 8258: nan
Loss at step 8259: nan
Loss at step 8260: nan
Loss at step 8261: nan
Loss at step 8262: nan
Loss at step 8263: nan
Loss at step 8264: nan
Loss at step 8265: nan
Loss at step 8266: nan
Loss at step 8267: nan
Loss at step 8268: nan
Loss at step 8269: nan
Loss at step 8270: nan
Loss at step 8271: nan
Loss at step 8272: nan
Loss at step 8273: nan
Loss at step 8274: nan
Loss at step 8275: nan
Loss at step 8276: nan
Loss at step 8277: nan
Loss at step 8278: nan
Loss at step 8279: nan
Loss at step 8280: nan
Loss at step 8281: nan
Loss at step 8282: nan
Loss at step 8283: nan
Loss at step 8284: nan
Loss at step 8285: nan
Loss at step 8286: nan
Loss at step 8287: nan
Loss at step 8288: nan
Loss at step 8289: nan
Loss at step 8290: nan
Loss at step 8291: nan
Loss at step 8292: nan
Loss at step 8293: nan
Loss at step 8294: nan
Loss at step 8295: nan
Loss at step 8296: nan
Loss at step 8297: nan
Loss at step 8298: nan
Loss at step 8299: nan
Loss at step 8300: nan
Loss at step 8301: nan
Loss at step 8302: nan
Loss at step 8303: nan
Loss at step 8304: nan
Loss at step 8305: nan
Loss at step 8306: nan
Loss at step 8307: nan
Loss at step 8308: nan
Loss at step 8309: nan
Loss at step 8310: nan
Loss at step 8311: nan
Loss at step 8312: nan
Loss at step 8313: nan
Loss at step 8314: nan
Loss at step 8315: nan
Loss at step 8316: nan
Loss at step 8317: nan
Loss at step 8318: nan
Loss at step 8319: nan
Loss at step 8320: nan
Loss at step 8321: nan
Loss at step 8322: nan
Loss at step 8323: nan
Loss at step 8324: nan
Loss at step 8325: nan
Loss at step 8326: nan
Loss at step 8327: nan
Loss at step 8328: nan
Loss at step 8329: nan
Loss at step 8330: nan
Loss at step 8331: nan
Loss at step 8332: nan
Loss at step 8333: nan
Loss at step 8334: nan
Loss at step 8335: nan
Loss at step 8336: nan
Loss at step 8337: nan
Loss at step 8338: nan
Loss at step 8339: nan
Loss at step 8340: nan
Loss at step 8341: nan
Loss at step 8342: nan
Loss at step 8343: nan
Loss at step 8344: nan
Loss at step 8345: nan
Loss at step 8346: nan
Loss at step 8347: nan
Loss at step 8348: nan
Loss at step 8349: nan
Loss at step 8350: nan
Loss at step 8351: nan
Loss at step 8352: nan
Loss at step 8353: nan
Loss at step 8354: nan
Loss at step 8355: nan
Loss at step 8356: nan
Loss at step 8357: nan
Loss at step 8358: nan
Loss at step 8359: nan
Loss at step 8360: nan
Loss at step 8361: nan
Loss at step 8362: nan
Loss at step 8363: nan
Loss at step 8364: nan
Loss at step 8365: nan
Loss at step 8366: nan
Loss at step 8367: nan
Loss at step 8368: nan
Loss at step 8369: nan
Loss at step 8370: nan
Loss at step 8371: nan
Loss at step 8372: nan
Loss at step 8373: nan
Loss at step 8374: nan
Loss at step 8375: nan
Loss at step 8376: nan
Loss at step 8377: nan
Loss at step 8378: nan
Loss at step 8379: nan
Loss at step 8380: nan
Loss at step 8381: nan
Loss at step 8382: nan
Loss at step 8383: nan
Loss at step 8384: nan
Loss at step 8385: nan
Loss at step 8386: nan
Loss at step 8387: nan
Loss at step 8388: nan
Loss at step 8389: nan
Loss at step 8390: nan
Loss at step 8391: nan
Loss at step 8392: nan
Loss at step 8393: nan
Loss at step 8394: nan
Loss at step 8395: nan
Loss at step 8396: nan
Loss at step 8397: nan
Loss at step 8398: nan
Loss at step 8399: nan
Saving training state...
[2025-08-03 13:02:12,847] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 13:02:19,309] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 8400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 8400: nan
Loss at step 8401: nan
Loss at step 8402: nan
Loss at step 8403: nan
Loss at step 8404: nan
Loss at step 8405: nan
Loss at step 8406: nan
Loss at step 8407: nan
Loss at step 8408: nan
Loss at step 8409: nan
Loss at step 8410: nan
Loss at step 8411: nan
Loss at step 8412: nan
Loss at step 8413: nan
Loss at step 8414: nan
Loss at step 8415: nan
Loss at step 8416: nan
Loss at step 8417: nan
Loss at step 8418: nan
Loss at step 8419: nan
Loss at step 8420: nan
Loss at step 8421: nan
Loss at step 8422: nan
Loss at step 8423: nan
Loss at step 8424: nan
Loss at step 8425: nan
Loss at step 8426: nan
Loss at step 8427: nan
Loss at step 8428: nan
Loss at step 8429: nan
Loss at step 8430: nan
Loss at step 8431: nan
Loss at step 8432: nan
Loss at step 8433: nan
Loss at step 8434: nan
Loss at step 8435: nan
Loss at step 8436: nan
Loss at step 8437: nan
Loss at step 8438: nan
Loss at step 8439: nan
Loss at step 8440: nan
Loss at step 8441: nan
Loss at step 8442: nan
Loss at step 8443: nan
Loss at step 8444: nan
Loss at step 8445: nan
Loss at step 8446: nan
Loss at step 8447: nan
Loss at step 8448: nan
Loss at step 8449: nan
Loss at step 8450: nan
Loss at step 8451: nan
Loss at step 8452: nan
Loss at step 8453: nan
Loss at step 8454: nan
Loss at step 8455: nan
Loss at step 8456: nan
Loss at step 8457: nan
Loss at step 8458: nan
Loss at step 8459: nan
Loss at step 8460: nan
Loss at step 8461: nan
Loss at step 8462: nan
Loss at step 8463: nan
Loss at step 8464: nan
Loss at step 8465: nan
Loss at step 8466: nan
Loss at step 8467: nan
Loss at step 8468: nan
Loss at step 8469: nan
Loss at step 8470: nan
Loss at step 8471: nan
Loss at step 8472: nan
Loss at step 8473: nan
Loss at step 8474: nan
Loss at step 8475: nan
Loss at step 8476: nan
Loss at step 8477: nan
Loss at step 8478: nan
Loss at step 8479: nan
Loss at step 8480: nan
Loss at step 8481: nan
Loss at step 8482: nan
Loss at step 8483: nan
Loss at step 8484: nan
Loss at step 8485: nan
Loss at step 8486: nan
Loss at step 8487: nan
Loss at step 8488: nan
Loss at step 8489: nan
Loss at step 8490: nan
Loss at step 8491: nan
Loss at step 8492: nan
Loss at step 8493: nan
Loss at step 8494: nan
Loss at step 8495: nan
Loss at step 8496: nan
Loss at step 8497: nan
Loss at step 8498: nan
Loss at step 8499: nan
Loss at step 8500: nan
Loss at step 8501: nan
Loss at step 8502: nan
Loss at step 8503: nan
Loss at step 8504: nan
Loss at step 8505: nan
Loss at step 8506: nan
Loss at step 8507: nan
Loss at step 8508: nan
Loss at step 8509: nan
Loss at step 8510: nan
Loss at step 8511: nan
Loss at step 8512: nan
Loss at step 8513: nan
Loss at step 8514: nan
Loss at step 8515: nan
Loss at step 8516: nan
Loss at step 8517: nan
Loss at step 8518: nan
Loss at step 8519: nan
Loss at step 8520: nan
Loss at step 8521: nan
Loss at step 8522: nan
Loss at step 8523: nan
Loss at step 8524: nan
Loss at step 8525: nan
Loss at step 8526: nan
Loss at step 8527: nan
Loss at step 8528: nan
Loss at step 8529: nan
Loss at step 8530: nan
Loss at step 8531: nan
Loss at step 8532: nan
Loss at step 8533: nan
Loss at step 8534: nan
Loss at step 8535: nan
Loss at step 8536: nan
Loss at step 8537: nan
Loss at step 8538: nan
Loss at step 8539: nan
Loss at step 8540: nan
Loss at step 8541: nan
Loss at step 8542: nan
Loss at step 8543: nan
Loss at step 8544: nan
Loss at step 8545: nan
Loss at step 8546: nan
Loss at step 8547: nan
Loss at step 8548: nan
Loss at step 8549: nan
Loss at step 8550: nan
Loss at step 8551: nan
Loss at step 8552: nan
Loss at step 8553: nan
Loss at step 8554: nan
Loss at step 8555: nan
Loss at step 8556: nan
Loss at step 8557: nan
Loss at step 8558: nan
Loss at step 8559: nan
Loss at step 8560: nan
Loss at step 8561: nan
Loss at step 8562: nan
Loss at step 8563: nan
Loss at step 8564: nan
Loss at step 8565: nan
Loss at step 8566: nan
Loss at step 8567: nan
Loss at step 8568: nan
Loss at step 8569: nan
Loss at step 8570: nan
Loss at step 8571: nan
Loss at step 8572: nan
Loss at step 8573: nan
Loss at step 8574: nan
Loss at step 8575: nan
Loss at step 8576: nan
Loss at step 8577: nan
Loss at step 8578: nan
Loss at step 8579: nan
Loss at step 8580: nan
Loss at step 8581: nan
Loss at step 8582: nan
Loss at step 8583: nan
Loss at step 8584: nan
Loss at step 8585: nan
Loss at step 8586: nan
Loss at step 8587: nan
Loss at step 8588: nan
Loss at step 8589: nan
Loss at step 8590: nan
Loss at step 8591: nan
Loss at step 8592: nan
Loss at step 8593: nan
Loss at step 8594: nan
Loss at step 8595: nan
Loss at step 8596: nan
Loss at step 8597: nan
Loss at step 8598: nan
Loss at step 8599: nan
Saving training state...
[2025-08-03 14:10:46,933] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 14:10:53,499] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 8600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 8600: nan
Loss at step 8601: nan
Loss at step 8602: nan
Loss at step 8603: nan
Loss at step 8604: nan
Loss at step 8605: nan
Loss at step 8606: nan
Loss at step 8607: nan
Loss at step 8608: nan
Loss at step 8609: nan
Loss at step 8610: nan
Loss at step 8611: nan
Loss at step 8612: nan
Loss at step 8613: nan
Loss at step 8614: nan
Loss at step 8615: nan
Loss at step 8616: nan
Loss at step 8617: nan
Loss at step 8618: nan
Loss at step 8619: nan
Loss at step 8620: nan
Loss at step 8621: nan
Loss at step 8622: nan
Loss at step 8623: nan
Loss at step 8624: nan
Loss at step 8625: nan
Loss at step 8626: nan
Loss at step 8627: nan
Loss at step 8628: nan
Loss at step 8629: nan
Loss at step 8630: nan
Loss at step 8631: nan
Loss at step 8632: nan
Loss at step 8633: nan
Loss at step 8634: nan
Loss at step 8635: nan
Loss at step 8636: nan
Loss at step 8637: nan
Loss at step 8638: nan
Loss at step 8639: nan
Loss at step 8640: nan
Loss at step 8641: nan
Loss at step 8642: nan
Loss at step 8643: nan
Loss at step 8644: nan
Loss at step 8645: nan
Loss at step 8646: nan
Loss at step 8647: nan
Loss at step 8648: nan
Loss at step 8649: nan
Loss at step 8650: nan
Loss at step 8651: nan
Loss at step 8652: nan
Loss at step 8653: nan
Loss at step 8654: nan
Loss at step 8655: nan
Loss at step 8656: nan
Loss at step 8657: nan
Loss at step 8658: nan
Loss at step 8659: nan
Loss at step 8660: nan
Loss at step 8661: nan
Loss at step 8662: nan
Loss at step 8663: nan
Loss at step 8664: nan
Loss at step 8665: nan
Loss at step 8666: nan
Loss at step 8667: nan
Loss at step 8668: nan
Loss at step 8669: nan
Loss at step 8670: nan
Loss at step 8671: nan
Loss at step 8672: nan
Loss at step 8673: nan
Loss at step 8674: nan
Loss at step 8675: nan
Loss at step 8676: nan
Loss at step 8677: nan
Loss at step 8678: nan
Loss at step 8679: nan
Loss at step 8680: nan
Loss at step 8681: nan
Loss at step 8682: nan
Loss at step 8683: nan
Loss at step 8684: nan
Loss at step 8685: nan
Loss at step 8686: nan
Loss at step 8687: nan
Loss at step 8688: nan
Loss at step 8689: nan
Loss at step 8690: nan
Loss at step 8691: nan
Loss at step 8692: nan
Loss at step 8693: nan
Loss at step 8694: nan
Loss at step 8695: nan
Loss at step 8696: nan
Loss at step 8697: nan
Loss at step 8698: nan
Loss at step 8699: nan
Loss at step 8700: nan
Loss at step 8701: nan
Loss at step 8702: nan
Loss at step 8703: nan
Loss at step 8704: nan
Loss at step 8705: nan
Loss at step 8706: nan
Loss at step 8707: nan
Loss at step 8708: nan
Loss at step 8709: nan
Loss at step 8710: nan
Loss at step 8711: nan
Loss at step 8712: nan
Loss at step 8713: nan
Loss at step 8714: nan
Loss at step 8715: nan
Loss at step 8716: nan
Loss at step 8717: nan
Loss at step 8718: nan
Loss at step 8719: nan
Loss at step 8720: nan
Loss at step 8721: nan
Loss at step 8722: nan
Loss at step 8723: nan
Loss at step 8724: nan
Loss at step 8725: nan
Loss at step 8726: nan
Loss at step 8727: nan
Loss at step 8728: nan
Loss at step 8729: nan
Loss at step 8730: nan
Loss at step 8731: nan
Loss at step 8732: nan
Loss at step 8733: nan
Loss at step 8734: nan
Loss at step 8735: nan
Loss at step 8736: nan
Loss at step 8737: nan
Loss at step 8738: nan
Loss at step 8739: nan
Loss at step 8740: nan
Loss at step 8741: nan
Loss at step 8742: nan
Loss at step 8743: nan
Loss at step 8744: nan
Loss at step 8745: nan
Loss at step 8746: nan
Loss at step 8747: nan
Loss at step 8748: nan
Loss at step 8749: nan
Loss at step 8750: nan
Loss at step 8751: nan
Loss at step 8752: nan
Loss at step 8753: nan
Loss at step 8754: nan
Loss at step 8755: nan
Loss at step 8756: nan
Loss at step 8757: nan
Loss at step 8758: nan
Loss at step 8759: nan
Loss at step 8760: nan
Loss at step 8761: nan
Loss at step 8762: nan
Loss at step 8763: nan
Loss at step 8764: nan
Loss at step 8765: nan
Loss at step 8766: nan
Loss at step 8767: nan
Loss at step 8768: nan
Loss at step 8769: nan
Loss at step 8770: nan
Loss at step 8771: nan
Loss at step 8772: nan
Loss at step 8773: nan
Loss at step 8774: nan
Loss at step 8775: nan
Loss at step 8776: nan
Loss at step 8777: nan
Loss at step 8778: nan
Loss at step 8779: nan
Loss at step 8780: nan
Loss at step 8781: nan
Loss at step 8782: nan
Loss at step 8783: nan
Loss at step 8784: nan
Loss at step 8785: nan
Loss at step 8786: nan
Loss at step 8787: nan
Loss at step 8788: nan
Loss at step 8789: nan
Loss at step 8790: nan
Loss at step 8791: nan
Loss at step 8792: nan
Loss at step 8793: nan
Loss at step 8794: nan
Loss at step 8795: nan
Loss at step 8796: nan
Loss at step 8797: nan
Loss at step 8798: nan
Loss at step 8799: nan
Saving training state...
[2025-08-03 15:19:03,448] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 15:19:10,040] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 8800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 8800: nan
Loss at step 8801: nan
Loss at step 8802: nan
Loss at step 8803: nan
Loss at step 8804: nan
Loss at step 8805: nan
Loss at step 8806: nan
Loss at step 8807: nan
Loss at step 8808: nan
Loss at step 8809: nan
Loss at step 8810: nan
Loss at step 8811: nan
Loss at step 8812: nan
Loss at step 8813: nan
Loss at step 8814: nan
Loss at step 8815: nan
Loss at step 8816: nan
Loss at step 8817: nan
Loss at step 8818: nan
Loss at step 8819: nan
Loss at step 8820: nan
Loss at step 8821: nan
Loss at step 8822: nan
Loss at step 8823: nan
Loss at step 8824: nan
Loss at step 8825: nan
Loss at step 8826: nan
Loss at step 8827: nan
Loss at step 8828: nan
Loss at step 8829: nan
Loss at step 8830: nan
Loss at step 8831: nan
Loss at step 8832: nan
Loss at step 8833: nan
Loss at step 8834: nan
Loss at step 8835: nan
Loss at step 8836: nan
Loss at step 8837: nan
Loss at step 8838: nan
Loss at step 8839: nan
Loss at step 8840: nan
Loss at step 8841: nan
Loss at step 8842: nan
Loss at step 8843: nan
Loss at step 8844: nan
Loss at step 8845: nan
Loss at step 8846: nan
Loss at step 8847: nan
Loss at step 8848: nan
Loss at step 8849: nan
Loss at step 8850: nan
Loss at step 8851: nan
Loss at step 8852: nan
Loss at step 8853: nan
Loss at step 8854: nan
Loss at step 8855: nan
Loss at step 8856: nan
Loss at step 8857: nan
Loss at step 8858: nan
Loss at step 8859: nan
Loss at step 8860: nan
Loss at step 8861: nan
Loss at step 8862: nan
Loss at step 8863: nan
Loss at step 8864: nan
Loss at step 8865: nan
Loss at step 8866: nan
Loss at step 8867: nan
Loss at step 8868: nan
Loss at step 8869: nan
Loss at step 8870: nan
Loss at step 8871: nan
Loss at step 8872: nan
Loss at step 8873: nan
Loss at step 8874: nan
Loss at step 8875: nan
Loss at step 8876: nan
Loss at step 8877: nan
Loss at step 8878: nan
Loss at step 8879: nan
Loss at step 8880: nan
Loss at step 8881: nan
Loss at step 8882: nan
Loss at step 8883: nan
Loss at step 8884: nan
Loss at step 8885: nan
Loss at step 8886: nan
Loss at step 8887: nan
Loss at step 8888: nan
Loss at step 8889: nan
Loss at step 8890: nan
Loss at step 8891: nan
Loss at step 8892: nan
Loss at step 8893: nan
Loss at step 8894: nan
Loss at step 8895: nan
Loss at step 8896: nan
Loss at step 8897: nan
Loss at step 8898: nan
Loss at step 8899: nan
Loss at step 8900: nan
Loss at step 8901: nan
Loss at step 8902: nan
Loss at step 8903: nan
Loss at step 8904: nan
Loss at step 8905: nan
Loss at step 8906: nan
Loss at step 8907: nan
Loss at step 8908: nan
Loss at step 8909: nan
Loss at step 8910: nan
Loss at step 8911: nan
Loss at step 8912: nan
Loss at step 8913: nan
Loss at step 8914: nan
Loss at step 8915: nan
Loss at step 8916: nan
Loss at step 8917: nan
Loss at step 8918: nan
Loss at step 8919: nan
Loss at step 8920: nan
Loss at step 8921: nan
Loss at step 8922: nan
Loss at step 8923: nan
Loss at step 8924: nan
Loss at step 8925: nan
Loss at step 8926: nan
Loss at step 8927: nan
Loss at step 8928: nan
Loss at step 8929: nan
Loss at step 8930: nan
Loss at step 8931: nan
Loss at step 8932: nan
Loss at step 8933: nan
Loss at step 8934: nan
Loss at step 8935: nan
Loss at step 8936: nan
Loss at step 8937: nan
Loss at step 8938: nan
Loss at step 8939: nan
Loss at step 8940: nan
Loss at step 8941: nan
Loss at step 8942: nan
Loss at step 8943: nan
Loss at step 8944: nan
Loss at step 8945: nan
Loss at step 8946: nan
Loss at step 8947: nan
Loss at step 8948: nan
Loss at step 8949: nan
Loss at step 8950: nan
Loss at step 8951: nan
Loss at step 8952: nan
Loss at step 8953: nan
Loss at step 8954: nan
Loss at step 8955: nan
Loss at step 8956: nan
Loss at step 8957: nan
Loss at step 8958: nan
Loss at step 8959: nan
Loss at step 8960: nan
Loss at step 8961: nan
Loss at step 8962: nan
Loss at step 8963: nan
Loss at step 8964: nan
Loss at step 8965: nan
Loss at step 8966: nan
Loss at step 8967: nan
Loss at step 8968: nan
Loss at step 8969: nan
Loss at step 8970: nan
Loss at step 8971: nan
Loss at step 8972: nan
Loss at step 8973: nan
Loss at step 8974: nan
Loss at step 8975: nan
Loss at step 8976: nan
Loss at step 8977: nan
Loss at step 8978: nan
Loss at step 8979: nan
Loss at step 8980: nan
Loss at step 8981: nan
Loss at step 8982: nan
Loss at step 8983: nan
Loss at step 8984: nan
Loss at step 8985: nan
Loss at step 8986: nan
Loss at step 8987: nan
Loss at step 8988: nan
Loss at step 8989: nan
Loss at step 8990: nan
Loss at step 8991: nan
Loss at step 8992: nan
Loss at step 8993: nan
Loss at step 8994: nan
Loss at step 8995: nan
Loss at step 8996: nan
Loss at step 8997: nan
Loss at step 8998: nan
Loss at step 8999: nan
Saving training state...
[2025-08-03 16:26:37,490] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 16:26:44,977] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 9000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 9000: nan
Loss at step 9001: nan
Loss at step 9002: nan
Loss at step 9003: nan
Loss at step 9004: nan
Loss at step 9005: nan
Loss at step 9006: nan
Loss at step 9007: nan
Loss at step 9008: nan
Loss at step 9009: nan
Loss at step 9010: nan
Loss at step 9011: nan
Loss at step 9012: nan
Loss at step 9013: nan
Loss at step 9014: nan
Loss at step 9015: nan
Loss at step 9016: nan
Loss at step 9017: nan
Loss at step 9018: nan
Loss at step 9019: nan
Loss at step 9020: nan
Loss at step 9021: nan
Loss at step 9022: nan
Loss at step 9023: nan
Loss at step 9024: nan
Loss at step 9025: nan
Loss at step 9026: nan
Loss at step 9027: nan
Loss at step 9028: nan
Loss at step 9029: nan
Loss at step 9030: nan
Loss at step 9031: nan
Loss at step 9032: nan
Loss at step 9033: nan
Loss at step 9034: nan
Loss at step 9035: nan
Loss at step 9036: nan
Loss at step 9037: nan
Loss at step 9038: nan
Loss at step 9039: nan
Loss at step 9040: nan
Loss at step 9041: nan
Loss at step 9042: nan
Loss at step 9043: nan
Loss at step 9044: nan
Loss at step 9045: nan
Loss at step 9046: nan
Loss at step 9047: nan
Loss at step 9048: nan
Loss at step 9049: nan
Loss at step 9050: nan
Loss at step 9051: nan
Loss at step 9052: nan
Loss at step 9053: nan
Loss at step 9054: nan
Loss at step 9055: nan
Loss at step 9056: nan
Loss at step 9057: nan
Loss at step 9058: nan
Loss at step 9059: nan
Loss at step 9060: nan
Loss at step 9061: nan
Loss at step 9062: nan
Loss at step 9063: nan
Loss at step 9064: nan
Loss at step 9065: nan
Loss at step 9066: nan
Loss at step 9067: nan
Loss at step 9068: nan
Loss at step 9069: nan
Loss at step 9070: nan
Loss at step 9071: nan
Loss at step 9072: nan
Loss at step 9073: nan
Loss at step 9074: nan
Loss at step 9075: nan
Loss at step 9076: nan
Loss at step 9077: nan
Loss at step 9078: nan
Loss at step 9079: nan
Loss at step 9080: nan
Loss at step 9081: nan
Loss at step 9082: nan
Loss at step 9083: nan
Loss at step 9084: nan
Loss at step 9085: nan
Loss at step 9086: nan
Loss at step 9087: nan
Loss at step 9088: nan
Loss at step 9089: nan
Loss at step 9090: nan
Loss at step 9091: nan
Loss at step 9092: nan
Loss at step 9093: nan
Loss at step 9094: nan
Loss at step 9095: nan
Loss at step 9096: nan
Loss at step 9097: nan
Loss at step 9098: nan
Loss at step 9099: nan
Loss at step 9100: nan
Loss at step 9101: nan
Loss at step 9102: nan
Loss at step 9103: nan
Loss at step 9104: nan
Loss at step 9105: nan
Loss at step 9106: nan
Loss at step 9107: nan
Loss at step 9108: nan
Loss at step 9109: nan
Loss at step 9110: nan
Loss at step 9111: nan
Loss at step 9112: nan
Loss at step 9113: nan
Loss at step 9114: nan
Loss at step 9115: nan
Loss at step 9116: nan
Loss at step 9117: nan
Loss at step 9118: nan
Loss at step 9119: nan
Loss at step 9120: nan
Loss at step 9121: nan
Loss at step 9122: nan
Loss at step 9123: nan
Loss at step 9124: nan
Loss at step 9125: nan
Loss at step 9126: nan
Loss at step 9127: nan
Loss at step 9128: nan
Loss at step 9129: nan
Loss at step 9130: nan
Loss at step 9131: nan
Loss at step 9132: nan
Loss at step 9133: nan
Loss at step 9134: nan
Loss at step 9135: nan
Loss at step 9136: nan
Loss at step 9137: nan
Loss at step 9138: nan
Loss at step 9139: nan
Loss at step 9140: nan
Loss at step 9141: nan
Loss at step 9142: nan
Loss at step 9143: nan
Loss at step 9144: nan
Loss at step 9145: nan
Loss at step 9146: nan
Loss at step 9147: nan
Loss at step 9148: nan
Loss at step 9149: nan
Loss at step 9150: nan
Loss at step 9151: nan
Loss at step 9152: nan
Loss at step 9153: nan
Loss at step 9154: nan
Loss at step 9155: nan
Loss at step 9156: nan
Loss at step 9157: nan
Loss at step 9158: nan
Loss at step 9159: nan
Loss at step 9160: nan
Loss at step 9161: nan
Loss at step 9162: nan
Loss at step 9163: nan
Loss at step 9164: nan
Loss at step 9165: nan
Loss at step 9166: nan
Loss at step 9167: nan
Loss at step 9168: nan
Loss at step 9169: nan
Loss at step 9170: nan
Loss at step 9171: nan
Loss at step 9172: nan
Loss at step 9173: nan
Loss at step 9174: nan
Loss at step 9175: nan
Loss at step 9176: nan
Loss at step 9177: nan
Loss at step 9178: nan
Loss at step 9179: nan
Loss at step 9180: nan
Loss at step 9181: nan
Loss at step 9182: nan
Loss at step 9183: nan
Loss at step 9184: nan
Loss at step 9185: nan
Loss at step 9186: nan
Loss at step 9187: nan
Loss at step 9188: nan
Loss at step 9189: nan
Loss at step 9190: nan
Loss at step 9191: nan
Loss at step 9192: nan
Loss at step 9193: nan
Loss at step 9194: nan
Loss at step 9195: nan
Loss at step 9196: nan
Loss at step 9197: nan
Loss at step 9198: nan
Loss at step 9199: nan
Saving training state...
[2025-08-03 17:34:14,519] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 17:34:20,939] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 9200
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 9200: nan
Loss at step 9201: nan
Loss at step 9202: nan
Loss at step 9203: nan
Loss at step 9204: nan
Loss at step 9205: nan
Loss at step 9206: nan
Loss at step 9207: nan
Loss at step 9208: nan
Loss at step 9209: nan
Loss at step 9210: nan
Loss at step 9211: nan
Loss at step 9212: nan
Loss at step 9213: nan
Loss at step 9214: nan
Loss at step 9215: nan
Loss at step 9216: nan
Loss at step 9217: nan
Loss at step 9218: nan
Loss at step 9219: nan
Loss at step 9220: nan
Loss at step 9221: nan
Loss at step 9222: nan
Loss at step 9223: nan
Loss at step 9224: nan
Loss at step 9225: nan
Loss at step 9226: nan
Loss at step 9227: nan
Loss at step 9228: nan
Loss at step 9229: nan
Loss at step 9230: nan
Loss at step 9231: nan
Loss at step 9232: nan
Loss at step 9233: nan
Loss at step 9234: nan
Loss at step 9235: nan
Loss at step 9236: nan
Loss at step 9237: nan
Loss at step 9238: nan
Loss at step 9239: nan
Loss at step 9240: nan
Loss at step 9241: nan
Loss at step 9242: nan
Loss at step 9243: nan
Loss at step 9244: nan
Loss at step 9245: nan
Loss at step 9246: nan
Loss at step 9247: nan
Loss at step 9248: nan
Loss at step 9249: nan
Loss at step 9250: nan
Loss at step 9251: nan
Loss at step 9252: nan
Loss at step 9253: nan
Loss at step 9254: nan
Loss at step 9255: nan
Loss at step 9256: nan
Loss at step 9257: nan
Loss at step 9258: nan
Loss at step 9259: nan
Loss at step 9260: nan
Loss at step 9261: nan
Loss at step 9262: nan
Loss at step 9263: nan
Loss at step 9264: nan
Loss at step 9265: nan
Loss at step 9266: nan
Loss at step 9267: nan
Loss at step 9268: nan
Loss at step 9269: nan
Loss at step 9270: nan
Loss at step 9271: nan
Loss at step 9272: nan
Loss at step 9273: nan
Loss at step 9274: nan
Loss at step 9275: nan
Loss at step 9276: nan
Loss at step 9277: nan
Loss at step 9278: nan
Loss at step 9279: nan
Loss at step 9280: nan
Loss at step 9281: nan
Loss at step 9282: nan
Loss at step 9283: nan
Loss at step 9284: nan
Loss at step 9285: nan
Loss at step 9286: nan
Loss at step 9287: nan
Loss at step 9288: nan
Loss at step 9289: nan
Loss at step 9290: nan
Loss at step 9291: nan
Loss at step 9292: nan
Loss at step 9293: nan
Loss at step 9294: nan
Loss at step 9295: nan
Loss at step 9296: nan
Loss at step 9297: nan
Loss at step 9298: nan
Loss at step 9299: nan
Loss at step 9300: nan
Loss at step 9301: nan
Loss at step 9302: nan
Loss at step 9303: nan
Loss at step 9304: nan
Loss at step 9305: nan
Loss at step 9306: nan
Loss at step 9307: nan
Loss at step 9308: nan
Loss at step 9309: nan
Loss at step 9310: nan
Loss at step 9311: nan
Loss at step 9312: nan
Loss at step 9313: nan
Loss at step 9314: nan
Loss at step 9315: nan
Loss at step 9316: nan
Loss at step 9317: nan
Loss at step 9318: nan
Loss at step 9319: nan
Loss at step 9320: nan
Loss at step 9321: nan
Loss at step 9322: nan
Loss at step 9323: nan
Loss at step 9324: nan
Loss at step 9325: nan
Loss at step 9326: nan
Loss at step 9327: nan
Loss at step 9328: nan
Loss at step 9329: nan
Loss at step 9330: nan
Loss at step 9331: nan
Loss at step 9332: nan
Loss at step 9333: nan
Loss at step 9334: nan
Loss at step 9335: nan
Loss at step 9336: nan
Loss at step 9337: nan
Loss at step 9338: nan
Loss at step 9339: nan
Loss at step 9340: nan
Loss at step 9341: nan
Loss at step 9342: nan
Loss at step 9343: nan
Loss at step 9344: nan
Loss at step 9345: nan
Loss at step 9346: nan
Loss at step 9347: nan
Loss at step 9348: nan
Loss at step 9349: nan
Loss at step 9350: nan
Loss at step 9351: nan
Loss at step 9352: nan
Loss at step 9353: nan
Loss at step 9354: nan
Loss at step 9355: nan
Loss at step 9356: nan
Loss at step 9357: nan
Loss at step 9358: nan
Loss at step 9359: nan
Loss at step 9360: nan
Loss at step 9361: nan
Loss at step 9362: nan
Loss at step 9363: nan
Loss at step 9364: nan
Loss at step 9365: nan
Loss at step 9366: nan
Loss at step 9367: nan
Loss at step 9368: nan
Loss at step 9369: nan
Loss at step 9370: nan
Loss at step 9371: nan
Loss at step 9372: nan
Loss at step 9373: nan
Loss at step 9374: nan
Loss at step 9375: nan
Loss at step 9376: nan
Loss at step 9377: nan
Loss at step 9378: nan
Loss at step 9379: nan
Loss at step 9380: nan
Loss at step 9381: nan
Loss at step 9382: nan
Loss at step 9383: nan
Loss at step 9384: nan
Loss at step 9385: nan
Loss at step 9386: nan
Loss at step 9387: nan
Loss at step 9388: nan
Loss at step 9389: nan
Loss at step 9390: nan
Loss at step 9391: nan
Loss at step 9392: nan
Loss at step 9393: nan
Loss at step 9394: nan
Loss at step 9395: nan
Loss at step 9396: nan
Loss at step 9397: nan
Loss at step 9398: nan
Loss at step 9399: nan
Saving training state...
[2025-08-03 18:41:59,785] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 18:42:06,325] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 9400
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 9400: nan
Loss at step 9401: nan
Loss at step 9402: nan
Loss at step 9403: nan
Loss at step 9404: nan
Loss at step 9405: nan
Loss at step 9406: nan
Loss at step 9407: nan
Loss at step 9408: nan
Loss at step 9409: nan
Loss at step 9410: nan
Loss at step 9411: nan
Loss at step 9412: nan
Loss at step 9413: nan
Loss at step 9414: nan
Loss at step 9415: nan
Loss at step 9416: nan
Loss at step 9417: nan
Loss at step 9418: nan
Loss at step 9419: nan
Loss at step 9420: nan
Loss at step 9421: nan
Loss at step 9422: nan
Loss at step 9423: nan
Loss at step 9424: nan
Loss at step 9425: nan
Loss at step 9426: nan
Loss at step 9427: nan
Loss at step 9428: nan
Loss at step 9429: nan
Loss at step 9430: nan
Loss at step 9431: nan
Loss at step 9432: nan
Loss at step 9433: nan
Loss at step 9434: nan
Loss at step 9435: nan
Loss at step 9436: nan
Loss at step 9437: nan
Loss at step 9438: nan
Loss at step 9439: nan
Loss at step 9440: nan
Loss at step 9441: nan
Loss at step 9442: nan
Loss at step 9443: nan
Loss at step 9444: nan
Loss at step 9445: nan
Loss at step 9446: nan
Loss at step 9447: nan
Loss at step 9448: nan
Loss at step 9449: nan
Loss at step 9450: nan
Loss at step 9451: nan
Loss at step 9452: nan
Loss at step 9453: nan
Loss at step 9454: nan
Loss at step 9455: nan
Loss at step 9456: nan
Loss at step 9457: nan
Loss at step 9458: nan
Loss at step 9459: nan
Loss at step 9460: nan
Loss at step 9461: nan
Loss at step 9462: nan
Loss at step 9463: nan
Loss at step 9464: nan
Loss at step 9465: nan
Loss at step 9466: nan
Loss at step 9467: nan
Loss at step 9468: nan
Loss at step 9469: nan
Loss at step 9470: nan
Loss at step 9471: nan
Loss at step 9472: nan
Loss at step 9473: nan
Loss at step 9474: nan
Loss at step 9475: nan
Loss at step 9476: nan
Loss at step 9477: nan
Loss at step 9478: nan
Loss at step 9479: nan
Loss at step 9480: nan
Loss at step 9481: nan
Loss at step 9482: nan
Loss at step 9483: nan
Loss at step 9484: nan
Loss at step 9485: nan
Loss at step 9486: nan
Loss at step 9487: nan
Loss at step 9488: nan
Loss at step 9489: nan
Loss at step 9490: nan
Loss at step 9491: nan
Loss at step 9492: nan
Loss at step 9493: nan
Loss at step 9494: nan
Loss at step 9495: nan
Loss at step 9496: nan
Loss at step 9497: nan
Loss at step 9498: nan
Loss at step 9499: nan
Loss at step 9500: nan
Loss at step 9501: nan
Loss at step 9502: nan
Loss at step 9503: nan
Loss at step 9504: nan
Loss at step 9505: nan
Loss at step 9506: nan
Loss at step 9507: nan
Loss at step 9508: nan
Loss at step 9509: nan
Loss at step 9510: nan
Loss at step 9511: nan
Loss at step 9512: nan
Loss at step 9513: nan
Loss at step 9514: nan
Loss at step 9515: nan
Loss at step 9516: nan
Loss at step 9517: nan
Loss at step 9518: nan
Loss at step 9519: nan
Loss at step 9520: nan
Loss at step 9521: nan
Loss at step 9522: nan
Loss at step 9523: nan
Loss at step 9524: nan
Loss at step 9525: nan
Loss at step 9526: nan
Loss at step 9527: nan
Loss at step 9528: nan
Loss at step 9529: nan
Loss at step 9530: nan
Loss at step 9531: nan
Loss at step 9532: nan
Loss at step 9533: nan
Loss at step 9534: nan
Loss at step 9535: nan
Loss at step 9536: nan
Loss at step 9537: nan
Loss at step 9538: nan
Loss at step 9539: nan
Loss at step 9540: nan
Loss at step 9541: nan
Loss at step 9542: nan
Loss at step 9543: nan
Loss at step 9544: nan
Loss at step 9545: nan
Loss at step 9546: nan
Loss at step 9547: nan
Loss at step 9548: nan
Loss at step 9549: nan
Loss at step 9550: nan
Loss at step 9551: nan
Loss at step 9552: nan
Loss at step 9553: nan
Loss at step 9554: nan
Loss at step 9555: nan
Loss at step 9556: nan
Loss at step 9557: nan
Loss at step 9558: nan
Loss at step 9559: nan
Loss at step 9560: nan
Loss at step 9561: nan
Loss at step 9562: nan
Loss at step 9563: nan
Loss at step 9564: nan
Loss at step 9565: nan
Loss at step 9566: nan
Loss at step 9567: nan
Loss at step 9568: nan
Loss at step 9569: nan
Loss at step 9570: nan
Loss at step 9571: nan
Loss at step 9572: nan
Loss at step 9573: nan
Loss at step 9574: nan
Loss at step 9575: nan
Loss at step 9576: nan
Loss at step 9577: nan
Loss at step 9578: nan
Loss at step 9579: nan
Loss at step 9580: nan
Loss at step 9581: nan
Loss at step 9582: nan
Loss at step 9583: nan
Loss at step 9584: nan
Loss at step 9585: nan
Loss at step 9586: nan
Loss at step 9587: nan
Loss at step 9588: nan
Loss at step 9589: nan
Loss at step 9590: nan
Loss at step 9591: nan
Loss at step 9592: nan
Loss at step 9593: nan
Loss at step 9594: nan
Loss at step 9595: nan
Loss at step 9596: nan
Loss at step 9597: nan
Loss at step 9598: nan
Loss at step 9599: nan
Saving training state...
[2025-08-03 19:49:34,156] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 19:49:40,594] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 9600
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 9600: nan
Loss at step 9601: nan
Loss at step 9602: nan
Loss at step 9603: nan
Loss at step 9604: nan
Loss at step 9605: nan
Loss at step 9606: nan
Loss at step 9607: nan
Loss at step 9608: nan
Loss at step 9609: nan
Loss at step 9610: nan
Loss at step 9611: nan
Loss at step 9612: nan
Loss at step 9613: nan
Loss at step 9614: nan
Loss at step 9615: nan
Loss at step 9616: nan
Loss at step 9617: nan
Loss at step 9618: nan
Loss at step 9619: nan
Loss at step 9620: nan
Loss at step 9621: nan
Loss at step 9622: nan
Loss at step 9623: nan
Loss at step 9624: nan
Loss at step 9625: nan
Loss at step 9626: nan
Loss at step 9627: nan
Loss at step 9628: nan
Loss at step 9629: nan
Loss at step 9630: nan
Loss at step 9631: nan
Loss at step 9632: nan
Loss at step 9633: nan
Loss at step 9634: nan
Loss at step 9635: nan
Loss at step 9636: nan
Loss at step 9637: nan
Loss at step 9638: nan
Loss at step 9639: nan
Loss at step 9640: nan
Loss at step 9641: nan
Loss at step 9642: nan
Loss at step 9643: nan
Loss at step 9644: nan
Loss at step 9645: nan
Loss at step 9646: nan
Loss at step 9647: nan
Loss at step 9648: nan
Loss at step 9649: nan
Loss at step 9650: nan
Loss at step 9651: nan
Loss at step 9652: nan
Loss at step 9653: nan
Loss at step 9654: nan
Loss at step 9655: nan
Loss at step 9656: nan
Loss at step 9657: nan
Loss at step 9658: nan
Loss at step 9659: nan
Loss at step 9660: nan
Loss at step 9661: nan
Loss at step 9662: nan
Loss at step 9663: nan
Loss at step 9664: nan
Loss at step 9665: nan
Loss at step 9666: nan
Loss at step 9667: nan
Loss at step 9668: nan
Loss at step 9669: nan
Loss at step 9670: nan
Loss at step 9671: nan
Loss at step 9672: nan
Loss at step 9673: nan
Loss at step 9674: nan
Loss at step 9675: nan
Loss at step 9676: nan
Loss at step 9677: nan
Loss at step 9678: nan
Loss at step 9679: nan
Loss at step 9680: nan
Loss at step 9681: nan
Loss at step 9682: nan
Loss at step 9683: nan
Loss at step 9684: nan
Loss at step 9685: nan
Loss at step 9686: nan
Loss at step 9687: nan
Loss at step 9688: nan
Loss at step 9689: nan
Loss at step 9690: nan
Loss at step 9691: nan
Loss at step 9692: nan
Loss at step 9693: nan
Loss at step 9694: nan
Loss at step 9695: nan
Loss at step 9696: nan
Loss at step 9697: nan
Loss at step 9698: nan
Loss at step 9699: nan
Loss at step 9700: nan
Loss at step 9701: nan
Loss at step 9702: nan
Loss at step 9703: nan
Loss at step 9704: nan
Loss at step 9705: nan
Loss at step 9706: nan
Loss at step 9707: nan
Loss at step 9708: nan
Loss at step 9709: nan
Loss at step 9710: nan
Loss at step 9711: nan
Loss at step 9712: nan
Loss at step 9713: nan
Loss at step 9714: nan
Loss at step 9715: nan
Loss at step 9716: nan
Loss at step 9717: nan
Loss at step 9718: nan
Loss at step 9719: nan
Loss at step 9720: nan
Loss at step 9721: nan
Loss at step 9722: nan
Loss at step 9723: nan
Loss at step 9724: nan
Loss at step 9725: nan
Loss at step 9726: nan
Loss at step 9727: nan
Loss at step 9728: nan
Loss at step 9729: nan
Loss at step 9730: nan
Loss at step 9731: nan
Loss at step 9732: nan
Loss at step 9733: nan
Loss at step 9734: nan
Loss at step 9735: nan
Loss at step 9736: nan
Loss at step 9737: nan
Loss at step 9738: nan
Loss at step 9739: nan
Loss at step 9740: nan
Loss at step 9741: nan
Loss at step 9742: nan
Loss at step 9743: nan
Loss at step 9744: nan
Loss at step 9745: nan
Loss at step 9746: nan
Loss at step 9747: nan
Loss at step 9748: nan
Loss at step 9749: nan
Loss at step 9750: nan
Loss at step 9751: nan
Loss at step 9752: nan
Loss at step 9753: nan
Loss at step 9754: nan
Loss at step 9755: nan
Loss at step 9756: nan
Loss at step 9757: nan
Loss at step 9758: nan
Loss at step 9759: nan
Loss at step 9760: nan
Loss at step 9761: nan
Loss at step 9762: nan
Loss at step 9763: nan
Loss at step 9764: nan
Loss at step 9765: nan
Loss at step 9766: nan
Loss at step 9767: nan
Loss at step 9768: nan
Loss at step 9769: nan
Loss at step 9770: nan
Loss at step 9771: nan
Loss at step 9772: nan
Loss at step 9773: nan
Loss at step 9774: nan
Loss at step 9775: nan
Loss at step 9776: nan
Loss at step 9777: nan
Loss at step 9778: nan
Loss at step 9779: nan
Loss at step 9780: nan
Loss at step 9781: nan
Loss at step 9782: nan
Loss at step 9783: nan
Loss at step 9784: nan
Loss at step 9785: nan
Loss at step 9786: nan
Loss at step 9787: nan
Loss at step 9788: nan
Loss at step 9789: nan
Loss at step 9790: nan
Loss at step 9791: nan
Loss at step 9792: nan
Loss at step 9793: nan
Loss at step 9794: nan
Loss at step 9795: nan
Loss at step 9796: nan
Loss at step 9797: nan
Loss at step 9798: nan
Loss at step 9799: nan
Saving training state...
[2025-08-03 20:57:52,308] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 20:57:58,675] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 9800
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 9800: nan
Loss at step 9801: nan
Loss at step 9802: nan
Loss at step 9803: nan
Loss at step 9804: nan
Loss at step 9805: nan
Loss at step 9806: nan
Loss at step 9807: nan
Loss at step 9808: nan
Loss at step 9809: nan
Loss at step 9810: nan
Loss at step 9811: nan
Loss at step 9812: nan
Loss at step 9813: nan
Loss at step 9814: nan
Loss at step 9815: nan
Loss at step 9816: nan
Loss at step 9817: nan
Loss at step 9818: nan
Loss at step 9819: nan
Loss at step 9820: nan
Loss at step 9821: nan
Loss at step 9822: nan
Loss at step 9823: nan
Loss at step 9824: nan
Loss at step 9825: nan
Loss at step 9826: nan
Loss at step 9827: nan
Loss at step 9828: nan
Loss at step 9829: nan
Loss at step 9830: nan
Loss at step 9831: nan
Loss at step 9832: nan
Loss at step 9833: nan
Loss at step 9834: nan
Loss at step 9835: nan
Loss at step 9836: nan
Loss at step 9837: nan
Loss at step 9838: nan
Loss at step 9839: nan
Loss at step 9840: nan
Loss at step 9841: nan
Loss at step 9842: nan
Loss at step 9843: nan
Loss at step 9844: nan
Loss at step 9845: nan
Loss at step 9846: nan
Loss at step 9847: nan
Loss at step 9848: nan
Loss at step 9849: nan
Loss at step 9850: nan
Loss at step 9851: nan
Loss at step 9852: nan
Loss at step 9853: nan
Loss at step 9854: nan
Loss at step 9855: nan
Loss at step 9856: nan
Loss at step 9857: nan
Loss at step 9858: nan
Loss at step 9859: nan
Loss at step 9860: nan
Loss at step 9861: nan
Loss at step 9862: nan
Loss at step 9863: nan
Loss at step 9864: nan
Loss at step 9865: nan
Loss at step 9866: nan
Loss at step 9867: nan
Loss at step 9868: nan
Loss at step 9869: nan
Loss at step 9870: nan
Loss at step 9871: nan
Loss at step 9872: nan
Loss at step 9873: nan
Loss at step 9874: nan
Loss at step 9875: nan
Loss at step 9876: nan
Loss at step 9877: nan
Loss at step 9878: nan
Loss at step 9879: nan
Loss at step 9880: nan
Loss at step 9881: nan
Loss at step 9882: nan
Loss at step 9883: nan
Loss at step 9884: nan
Loss at step 9885: nan
Loss at step 9886: nan
Loss at step 9887: nan
Loss at step 9888: nan
Loss at step 9889: nan
Loss at step 9890: nan
Loss at step 9891: nan
Loss at step 9892: nan
Loss at step 9893: nan
Loss at step 9894: nan
Loss at step 9895: nan
Loss at step 9896: nan
Loss at step 9897: nan
Loss at step 9898: nan
Loss at step 9899: nan
Loss at step 9900: nan
Loss at step 9901: nan
Loss at step 9902: nan
Loss at step 9903: nan
Loss at step 9904: nan
Loss at step 9905: nan
Loss at step 9906: nan
Loss at step 9907: nan
Loss at step 9908: nan
Loss at step 9909: nan
Loss at step 9910: nan
Loss at step 9911: nan
Loss at step 9912: nan
Loss at step 9913: nan
Loss at step 9914: nan
Loss at step 9915: nan
Loss at step 9916: nan
Loss at step 9917: nan
Loss at step 9918: nan
Loss at step 9919: nan
Loss at step 9920: nan
Loss at step 9921: nan
Loss at step 9922: nan
Loss at step 9923: nan
Loss at step 9924: nan
Loss at step 9925: nan
Loss at step 9926: nan
Loss at step 9927: nan
Loss at step 9928: nan
Loss at step 9929: nan
Loss at step 9930: nan
Loss at step 9931: nan
Loss at step 9932: nan
Loss at step 9933: nan
Loss at step 9934: nan
Loss at step 9935: nan
Loss at step 9936: nan
Loss at step 9937: nan
Loss at step 9938: nan
Loss at step 9939: nan
Loss at step 9940: nan
Loss at step 9941: nan
Loss at step 9942: nan
Loss at step 9943: nan
Loss at step 9944: nan
Loss at step 9945: nan
Loss at step 9946: nan
Loss at step 9947: nan
Loss at step 9948: nan
Loss at step 9949: nan
Loss at step 9950: nan
Loss at step 9951: nan
Loss at step 9952: nan
Loss at step 9953: nan
Loss at step 9954: nan
Loss at step 9955: nan
Loss at step 9956: nan
Loss at step 9957: nan
Loss at step 9958: nan
Loss at step 9959: nan
Loss at step 9960: nan
Loss at step 9961: nan
Loss at step 9962: nan
Loss at step 9963: nan
Loss at step 9964: nan
Loss at step 9965: nan
Loss at step 9966: nan
Loss at step 9967: nan
Loss at step 9968: nan
Loss at step 9969: nan
Loss at step 9970: nan
Loss at step 9971: nan
Loss at step 9972: nan
Loss at step 9973: nan
Loss at step 9974: nan
Loss at step 9975: nan
Loss at step 9976: nan
Loss at step 9977: nan
Loss at step 9978: nan
Loss at step 9979: nan
Loss at step 9980: nan
Loss at step 9981: nan
Loss at step 9982: nan
Loss at step 9983: nan
Loss at step 9984: nan
Loss at step 9985: nan
Loss at step 9986: nan
Loss at step 9987: nan
Loss at step 9988: nan
Loss at step 9989: nan
Loss at step 9990: nan
Loss at step 9991: nan
Loss at step 9992: nan
Loss at step 9993: nan
Loss at step 9994: nan
Loss at step 9995: nan
Loss at step 9996: nan
Loss at step 9997: nan
Loss at step 9998: nan
Loss at step 9999: nan
Saving training state...
[2025-08-03 22:06:08,449] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
[2025-08-03 22:06:14,804] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /hpc2hdd/home/hongfeizhang/experiments/Wan/2025-08-01-11-17-3e-5/pytorch_model/mp_rank_00_model_states.pt
Checkpoint saved at step 10000
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
extra_image_frame_index: None
Loss at step 10000: nan
Loss at step 10001: nan
Loss at step 10002: nan
Loss at step 10003: nan
Loss at step 10004: nan
Loss at step 10005: nan
Loss at step 10006: nan
Loss at step 10007: nan
Loss at step 10008: nan
Loss at step 10009: nan
Loss at step 10010: nan
Loss at step 10011: nan
Loss at step 10012: nan
Loss at step 10013: nan
Loss at step 10014: nan
Loss at step 10015: nan
Loss at step 10016: nan
Loss at step 10017: nan
Loss at step 10018: nan
Loss at step 10019: nan
Loss at step 10020: nan
Loss at step 10021: nan
Loss at step 10022: nan
Loss at step 10023: nan
Loss at step 10024: nan
Loss at step 10025: nan
Loss at step 10026: nan
Loss at step 10027: nan
Loss at step 10028: nan
Loss at step 10029: nan
Loss at step 10030: nan
Loss at step 10031: nan
Loss at step 10032: nan
Loss at step 10033: nan
Loss at step 10034: nan
Loss at step 10035: nan
Loss at step 10036: nan
Loss at step 10037: nan
Loss at step 10038: nan
Loss at step 10039: nan
Loss at step 10040: nan
Loss at step 10041: nan
Loss at step 10042: nan
Loss at step 10043: nan
Loss at step 10044: nan
Loss at step 10045: nan
Loss at step 10046: nan
Loss at step 10047: nan
Loss at step 10048: nan
Loss at step 10049: nan
Loss at step 10050: nan
Loss at step 10051: nan
Loss at step 10052: nan
Loss at step 10053: nan
Loss at step 10054: nan
Loss at step 10055: nan
Loss at step 10056: nan
Loss at step 10057: nan
Loss at step 10058: nan
Loss at step 10059: nan
Loss at step 10060: nan
Loss at step 10061: nan
Loss at step 10062: nan
Loss at step 10063: nan
Loss at step 10064: nan
Loss at step 10065: nan
Loss at step 10066: nan
Loss at step 10067: nan
Loss at step 10068: nan
Loss at step 10069: nan
Loss at step 10070: nan
Loss at step 10071: nan
Loss at step 10072: nan
Loss at step 10073: nan
Loss at step 10074: nan
Loss at step 10075: nan
Loss at step 10076: nan
Loss at step 10077: nan
Loss at step 10078: nan
Loss at step 10079: nan
Loss at step 10080: nan
Loss at step 10081: nan
Loss at step 10082: nan
Loss at step 10083: nan
Loss at step 10084: nan
Loss at step 10085: nan
Loss at step 10086: nan
Loss at step 10087: nan
Loss at step 10088: nan
Loss at step 10089: nan
Loss at step 10090: nan
Loss at step 10091: nan
Loss at step 10092: nan
Loss at step 10093: nan
Loss at step 10094: nan
Loss at step 10095: nan
Loss at step 10096: nan
Loss at step 10097: nan
Loss at step 10098: nan
Loss at step 10099: nan
Loss at step 10100: nan
Loss at step 10101: nan
Loss at step 10102: nan
Loss at step 10103: nan
Loss at step 10104: nan
Loss at step 10105: nan
Loss at step 10106: nan
Loss at step 10107: nan
Loss at step 10108: nan
Loss at step 10109: nan
Loss at step 10110: nan
Loss at step 10111: nan
Loss at step 10112: nan
Loss at step 10113: nan
Loss at step 10114: nan
Loss at step 10115: nan
Loss at step 10116: nan
Loss at step 10117: nan
Loss at step 10118: nan
Loss at step 10119: nan
Loss at step 10120: nan
Loss at step 10121: nan
Loss at step 10122: nan
Loss at step 10123: nan
Loss at step 10124: nan
Loss at step 10125: nan
Loss at step 10126: nan
Loss at step 10127: nan
Loss at step 10128: nan
Loss at step 10129: nan
Loss at step 10130: nan
Loss at step 10131: nan
Loss at step 10132: nan
Loss at step 10133: nan
Loss at step 10134: nan
Loss at step 10135: nan
Loss at step 10136: nan
Loss at step 10137: nan
Loss at step 10138: nan
Loss at step 10139: nan
Loss at step 10140: nan
Loss at step 10141: nan
Loss at step 10142: nan
Loss at step 10143: nan
Loss at step 10144: nan
Loss at step 10145: nan
Loss at step 10146: nan
Loss at step 10147: nan
Loss at step 10148: nan
Loss at step 10149: nan
Loss at step 10150: nan
Loss at step 10151: nan
Loss at step 10152: nan
Loss at step 10153: nan
Loss at step 10154: nan
Loss at step 10155: nan
Loss at step 10156: nan
Loss at step 10157: nan
Loss at step 10158: nan
Loss at step 10159: nan
Loss at step 10160: nan
Loss at step 10161: nan
Loss at step 10162: nan
Loss at step 10163: nan
Loss at step 10164: nan
Loss at step 10165: nan
Loss at step 10166: nan
Loss at step 10167: nan
Loss at step 10168: nan
Loss at step 10169: nan
Loss at step 10170: nan
Loss at step 10171: nan
Loss at step 10172: nan
Loss at step 10173: nan
Loss at step 10174: nan
Loss at step 10175: nan
Loss at step 10176: nan
Loss at step 10177: nan
Loss at step 10178: nan
Loss at step 10179: nan
Loss at step 10180: nan
Loss at step 10181: nan
Loss at step 10182: nan
Loss at step 10183: nan
Loss at step 10184: nan
Loss at step 10185: nan
Loss at step 10186: nan
Loss at step 10187: nan
Loss at step 10188: nan
Loss at step 10189: nan
Loss at step 10190: nan
Loss at step 10191: nan
Loss at step 10192: nan
Loss at step 10193: nan
Loss at step 10194: nan
Loss at step 10195: nan
Loss at step 10196: nan
Loss at step 10197: nan
Loss at step 10198: nan
Loss at step 10199: nan
Saving training state...
[2025-08-03 23:14:11,718] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is begin to save!
